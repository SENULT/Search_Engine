{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef45224",
   "metadata": {},
   "source": [
    "# Topic 8: SEO & Link Analysis\n",
    "\n",
    "## Objectives:\n",
    "- Implement PageRank algorithm\n",
    "- Implement HITS (Hyperlink-Induced Topic Search)\n",
    "- Link analysis for Vietnamese football articles\n",
    "- Visualize authority and hub scores\n",
    "\n",
    "**Dataset**: 1,830 Vietnamese football articles from VnExpress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53495e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a88d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c1c6f",
   "metadata": {},
   "source": [
    "## 2. Load Vietnamese Football Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data files\n",
    "data_dir = Path('../data/raw')\n",
    "all_documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    file_path = data_dir / f'vnexpressT_bongda_part{i}.json'\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        docs = json.load(f)\n",
    "        all_documents.extend(docs)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(all_documents)} articles\")\n",
    "print(f\"Sample document keys: {list(all_documents[0].keys())}\")\n",
    "\n",
    "# Display first article\n",
    "print(\"\\nðŸ“° Sample Article:\")\n",
    "print(f\"Title: {all_documents[0].get('title', 'N/A')[:100]}...\")\n",
    "print(f\"URL: {all_documents[0].get('url', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b01e",
   "metadata": {},
   "source": [
    "## 3. Extract Links & Build Link Graph\n",
    "\n",
    "Create a graph where:\n",
    "- Nodes = Articles\n",
    "- Edges = Links between articles (based on content similarity or actual hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8699208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities (teams, players) to create semantic links\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract Vietnamese football entities\"\"\"\n",
    "    if not text:\n",
    "        return set()\n",
    "    \n",
    "    text = text.lower()\n",
    "    entities = set()\n",
    "    \n",
    "    # Teams\n",
    "    teams = ['manchester united', 'barcelona', 'real madrid', 'liverpool', \n",
    "             'chelsea', 'arsenal', 'bayern', 'psg', 'juventus',\n",
    "             'viá»‡t nam', 'hÃ  ná»™i', 'tp.hcm', 'sÃ i gÃ²n', 'viettel']\n",
    "    \n",
    "    # Players\n",
    "    players = ['ronaldo', 'messi', 'neymar', 'mbappÃ©', 'haaland',\n",
    "               'quang háº£i', 'cÃ´ng phÆ°á»£ng', 'vÄƒn quyáº¿t', 'xuÃ¢n trÆ°á»ng']\n",
    "    \n",
    "    # Competitions\n",
    "    competitions = ['world cup', 'champions league', 'premier league', \n",
    "                   'la liga', 'v-league', 'sea games', 'aff cup']\n",
    "    \n",
    "    all_entities = teams + players + competitions\n",
    "    \n",
    "    for entity in all_entities:\n",
    "        if entity in text:\n",
    "            entities.add(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Build link graph\n",
    "print(\"Building link graph...\")\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all documents as nodes\n",
    "for i, doc in enumerate(tqdm(all_documents, desc=\"Adding nodes\")):\n",
    "    title = doc.get('title', f'Doc_{i}')\n",
    "    content = doc.get('content', '') or doc.get('text', '')\n",
    "    entities = extract_entities(title + ' ' + content)\n",
    "    \n",
    "    G.add_node(i, \n",
    "               title=title[:50],  # Truncate for display\n",
    "               entities=entities,\n",
    "               url=doc.get('url', ''))\n",
    "\n",
    "# Create edges based on shared entities\n",
    "print(\"\\nCreating edges based on shared entities...\")\n",
    "edge_count = 0\n",
    "for i in tqdm(range(len(all_documents)), desc=\"Building edges\"):\n",
    "    entities_i = G.nodes[i]['entities']\n",
    "    if not entities_i:\n",
    "        continue\n",
    "    \n",
    "    for j in range(i + 1, min(i + 50, len(all_documents))):  # Limit for performance\n",
    "        entities_j = G.nodes[j]['entities']\n",
    "        \n",
    "        # If they share 2+ entities, create a link\n",
    "        shared = entities_i & entities_j\n",
    "        if len(shared) >= 2:\n",
    "            G.add_edge(i, j, weight=len(shared))\n",
    "            G.add_edge(j, i, weight=len(shared))  # Bidirectional\n",
    "            edge_count += 2\n",
    "\n",
    "print(f\"\\nâœ“ Graph created:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd2d8d",
   "metadata": {},
   "source": [
    "## 4. PageRank Algorithm\n",
    "\n",
    "**Formula**: \n",
    "$$PR(A) = (1-d) + d \\sum_{i=1}^n \\frac{PR(T_i)}{C(T_i)}$$\n",
    "\n",
    "Where:\n",
    "- $d$ = damping factor (0.85)\n",
    "- $T_i$ = pages linking to page A\n",
    "- $C(T_i)$ = number of outbound links from $T_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, damping=0.85, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Implement PageRank algorithm from scratch\n",
    "    \"\"\"\n",
    "    N = G.number_of_nodes()\n",
    "    if N == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize PageRank scores\n",
    "    pr = {node: 1.0 / N for node in G.nodes()}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        pr_new = {}\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            # Sum of PR from incoming links\n",
    "            incoming_sum = 0\n",
    "            for predecessor in G.predecessors(node):\n",
    "                out_degree = G.out_degree(predecessor)\n",
    "                if out_degree > 0:\n",
    "                    incoming_sum += pr[predecessor] / out_degree\n",
    "            \n",
    "            # PageRank formula\n",
    "            pr_new[node] = (1 - damping) / N + damping * incoming_sum\n",
    "        \n",
    "        # Check convergence\n",
    "        diff = sum(abs(pr_new[node] - pr[node]) for node in G.nodes())\n",
    "        pr = pr_new\n",
    "        \n",
    "        if diff < tol:\n",
    "            print(f\"âœ“ Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return pr\n",
    "\n",
    "# Calculate PageRank\n",
    "print(\"Calculating PageRank...\")\n",
    "pr_scores = pagerank(G, damping=0.85)\n",
    "\n",
    "# Compare with NetworkX implementation\n",
    "pr_nx = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 Articles by PageRank:\")\n",
    "top_pr = sorted(pr_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for rank, (node_id, score) in enumerate(top_pr, 1):\n",
    "    title = G.nodes[node_id]['title']\n",
    "    print(f\"{rank}. {title[:60]}... (PR={score:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56c7c1",
   "metadata": {},
   "source": [
    "## 5. HITS Algorithm (Hubs & Authorities)\n",
    "\n",
    "**Formulas**:\n",
    "- Authority: $a(p) = \\sum_{q \\in B(p)} h(q)$\n",
    "- Hub: $h(p) = \\sum_{q \\in F(p)} a(q)$\n",
    "\n",
    "Where:\n",
    "- $B(p)$ = pages pointing to $p$\n",
    "- $F(p)$ = pages that $p$ points to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits(G, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Implement HITS algorithm from scratch\n",
    "    \"\"\"\n",
    "    N = G.number_of_nodes()\n",
    "    if N == 0:\n",
    "        return {}, {}\n",
    "    \n",
    "    # Initialize hub and authority scores\n",
    "    hubs = {node: 1.0 for node in G.nodes()}\n",
    "    authorities = {node: 1.0 for node in G.nodes()}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Update authority scores\n",
    "        auth_new = {}\n",
    "        for node in G.nodes():\n",
    "            auth_new[node] = sum(hubs[predecessor] \n",
    "                                for predecessor in G.predecessors(node))\n",
    "        \n",
    "        # Update hub scores\n",
    "        hubs_new = {}\n",
    "        for node in G.nodes():\n",
    "            hubs_new[node] = sum(auth_new[successor] \n",
    "                                for successor in G.successors(node))\n",
    "        \n",
    "        # Normalize\n",
    "        auth_norm = np.sqrt(sum(v**2 for v in auth_new.values()))\n",
    "        hub_norm = np.sqrt(sum(v**2 for v in hubs_new.values()))\n",
    "        \n",
    "        if auth_norm > 0:\n",
    "            auth_new = {k: v / auth_norm for k, v in auth_new.items()}\n",
    "        if hub_norm > 0:\n",
    "            hubs_new = {k: v / hub_norm for k, v in hubs_new.items()}\n",
    "        \n",
    "        # Check convergence\n",
    "        diff_auth = sum(abs(auth_new[node] - authorities[node]) for node in G.nodes())\n",
    "        diff_hub = sum(abs(hubs_new[node] - hubs[node]) for node in G.nodes())\n",
    "        \n",
    "        authorities = auth_new\n",
    "        hubs = hubs_new\n",
    "        \n",
    "        if diff_auth < tol and diff_hub < tol:\n",
    "            print(f\"âœ“ HITS converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return hubs, authorities\n",
    "\n",
    "# Calculate HITS\n",
    "print(\"Calculating HITS...\")\n",
    "hub_scores, auth_scores = hits(G)\n",
    "\n",
    "# Compare with NetworkX\n",
    "hits_nx = nx.hits(G, max_iter=100)\n",
    "hubs_nx, auth_nx = hits_nx\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 Authority Articles (Most Referenced):\")\n",
    "top_auth = sorted(auth_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for rank, (node_id, score) in enumerate(top_auth, 1):\n",
    "    title = G.nodes[node_id]['title']\n",
    "    print(f\"{rank}. {title[:60]}... (Auth={score:.6f})\")\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 Hub Articles (Most Outgoing Links):\")\n",
    "top_hubs = sorted(hub_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for rank, (node_id, score) in enumerate(top_hubs, 1):\n",
    "    title = G.nodes[node_id]['title']\n",
    "    print(f\"{rank}. {title[:60]}... (Hub={score:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116e44e",
   "metadata": {},
   "source": [
    "## 6. Visualization & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339da3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'node_id': list(G.nodes()),\n",
    "    'title': [G.nodes[i]['title'] for i in G.nodes()],\n",
    "    'pagerank': [pr_scores.get(i, 0) for i in G.nodes()],\n",
    "    'authority': [auth_scores.get(i, 0) for i in G.nodes()],\n",
    "    'hub': [hub_scores.get(i, 0) for i in G.nodes()],\n",
    "    'in_degree': [G.in_degree(i) for i in G.nodes()],\n",
    "    'out_degree': [G.out_degree(i) for i in G.nodes()]\n",
    "})\n",
    "\n",
    "# Sort by PageRank\n",
    "comparison_df = comparison_df.sort_values('pagerank', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 20 Articles - All Metrics:\")\n",
    "print(comparison_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = Path('../outputs/seo_analysis')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "comparison_df.to_csv(output_dir / 'link_analysis_results.csv', index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ“ Results saved to {output_dir / 'link_analysis_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Score Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# PageRank distribution\n",
    "axes[0, 0].hist(comparison_df['pagerank'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('PageRank Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('PageRank Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Authority distribution\n",
    "axes[0, 1].hist(comparison_df['authority'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Authority Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Authority Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Hub distribution\n",
    "axes[1, 0].hist(comparison_df['hub'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Hub Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Hub Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Degree distribution\n",
    "axes[1, 1].scatter(comparison_df['in_degree'], comparison_df['out_degree'], alpha=0.5)\n",
    "axes[1, 1].set_title('In-Degree vs Out-Degree', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('In-Degree')\n",
    "axes[1, 1].set_ylabel('Out-Degree')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'score_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Score distributions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Correlation between metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# PageRank vs Authority\n",
    "axes[0].scatter(comparison_df['pagerank'], comparison_df['authority'], alpha=0.5)\n",
    "axes[0].set_title('PageRank vs Authority', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('PageRank')\n",
    "axes[0].set_ylabel('Authority')\n",
    "axes[0].grid(alpha=0.3)\n",
    "corr1 = np.corrcoef(comparison_df['pagerank'], comparison_df['authority'])[0, 1]\n",
    "axes[0].text(0.05, 0.95, f'Correlation: {corr1:.3f}', \n",
    "            transform=axes[0].transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# PageRank vs Hub\n",
    "axes[1].scatter(comparison_df['pagerank'], comparison_df['hub'], alpha=0.5, color='orange')\n",
    "axes[1].set_title('PageRank vs Hub', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('PageRank')\n",
    "axes[1].set_ylabel('Hub')\n",
    "axes[1].grid(alpha=0.3)\n",
    "corr2 = np.corrcoef(comparison_df['pagerank'], comparison_df['hub'])[0, 1]\n",
    "axes[1].text(0.05, 0.95, f'Correlation: {corr2:.3f}', \n",
    "            transform=axes[1].transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Authority vs Hub\n",
    "axes[2].scatter(comparison_df['authority'], comparison_df['hub'], alpha=0.5, color='green')\n",
    "axes[2].set_title('Authority vs Hub', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Authority')\n",
    "axes[2].set_ylabel('Hub')\n",
    "axes[2].grid(alpha=0.3)\n",
    "corr3 = np.corrcoef(comparison_df['authority'], comparison_df['hub'])[0, 1]\n",
    "axes[2].text(0.05, 0.95, f'Correlation: {corr3:.3f}', \n",
    "            transform=axes[2].transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'metric_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Correlation plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Visualize small subgraph\n",
    "# Get top 30 nodes by PageRank\n",
    "top_30_nodes = list(comparison_df.head(30)['node_id'])\n",
    "subgraph = G.subgraph(top_30_nodes)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "pos = nx.spring_layout(subgraph, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Node sizes based on PageRank\n",
    "node_sizes = [pr_scores[node] * 50000 for node in subgraph.nodes()]\n",
    "\n",
    "# Node colors based on Authority\n",
    "node_colors = [auth_scores[node] for node in subgraph.nodes()]\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_nodes(subgraph, pos, \n",
    "                       node_size=node_sizes,\n",
    "                       node_color=node_colors,\n",
    "                       cmap='YlOrRd',\n",
    "                       alpha=0.7)\n",
    "\n",
    "nx.draw_networkx_edges(subgraph, pos, \n",
    "                       alpha=0.2,\n",
    "                       edge_color='gray',\n",
    "                       arrows=True,\n",
    "                       arrowsize=10)\n",
    "\n",
    "# Add labels for top 10 nodes\n",
    "top_10_nodes = list(comparison_df.head(10)['node_id'])\n",
    "labels = {node: G.nodes[node]['title'][:20] + '...' \n",
    "          for node in subgraph.nodes() if node in top_10_nodes}\n",
    "nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)\n",
    "\n",
    "plt.title('Top 30 Articles Link Network\\n(Size=PageRank, Color=Authority)', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'link_network_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Network visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe27817",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š LINK ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Graph Statistics:\")\n",
    "print(f\"  Total nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Total edges: {G.number_of_edges()}\")\n",
    "print(f\"  Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "print(f\"  Graph density: {nx.density(G):.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š PageRank Statistics:\")\n",
    "print(f\"  Mean: {comparison_df['pagerank'].mean():.6f}\")\n",
    "print(f\"  Std:  {comparison_df['pagerank'].std():.6f}\")\n",
    "print(f\"  Max:  {comparison_df['pagerank'].max():.6f}\")\n",
    "print(f\"  Min:  {comparison_df['pagerank'].min():.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Authority Statistics:\")\n",
    "print(f\"  Mean: {comparison_df['authority'].mean():.6f}\")\n",
    "print(f\"  Std:  {comparison_df['authority'].std():.6f}\")\n",
    "print(f\"  Max:  {comparison_df['authority'].max():.6f}\")\n",
    "print(f\"  Min:  {comparison_df['authority'].min():.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Hub Statistics:\")\n",
    "print(f\"  Mean: {comparison_df['hub'].mean():.6f}\")\n",
    "print(f\"  Std:  {comparison_df['hub'].std():.6f}\")\n",
    "print(f\"  Max:  {comparison_df['hub'].max():.6f}\")\n",
    "print(f\"  Min:  {comparison_df['hub'].min():.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Correlations:\")\n",
    "print(f\"  PageRank vs Authority: {np.corrcoef(comparison_df['pagerank'], comparison_df['authority'])[0,1]:.3f}\")\n",
    "print(f\"  PageRank vs Hub:       {np.corrcoef(comparison_df['pagerank'], comparison_df['hub'])[0,1]:.3f}\")\n",
    "print(f\"  Authority vs Hub:      {np.corrcoef(comparison_df['authority'], comparison_df['hub'])[0,1]:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Topic 8 Complete: SEO & Link Analysis\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
