{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Vietnamese model chưa có. Cài đặt bằng: python -m spacy download vi_core_news_sm\n",
      "BẮT ĐẦU XỬ LÝ TEXT TỪ MONGODB\n",
      "==================================================\n",
      "Kết nối database thành công!\n",
      "Tổng số documents: 1832\n",
      "Bắt đầu xử lý 5 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 5/5 [00:00<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàn thành! Xử lý thành công: 5, Lỗi: 0\n",
      "\n",
      "PHÂN TÍCH KẾT QUẢ:\n",
      "Tổng số documents đã xử lý: 5\n",
      "\n",
      "Thống kê Tokens:\n",
      "  - Trung bình tokens/document: 727.4\n",
      "  - Trung bình filtered tokens/document: 541.4\n",
      "  - Trung bình entities/document: 19.2\n",
      "\n",
      "Top 10 Đội bóng được nhắc đến nhiều nhất:\n",
      "  - nam định: 2 lần\n",
      "  - tuyển việt nam: 2 lần\n",
      "  - hà nội fc: 1 lần\n",
      "  - đà nẵng: 1 lần\n",
      "  - đội tuyển việt nam: 1 lần\n",
      "\n",
      "Top 10 Cầu thủ được nhắc đến nhiều nhất:\n",
      "  - Kim Sang: 3 lần\n",
      "  - Park Hang: 2 lần\n",
      "  - trẻ cần được thi đấu thường xu: 1 lần\n",
      "  - nhập tịch vì sử dụng giấy tờ g: 1 lần\n",
      "  - vào sân: 1 lần\n",
      "  - bị phạt: 1 lần\n",
      "  - của Quy định này có đoạn: 1 lần\n",
      "  - phải được đăng ký đúng quy địn: 1 lần\n",
      "  - được coi là đủ tư cách thi đấu: 1 lần\n",
      "  - hợp lệ: 1 lần\n",
      "\n",
      "Top 10 Giải đấu được nhắc đến nhiều nhất:\n",
      "  - u23: 4 lần\n",
      "  - world cup: 3 lần\n",
      "  - asian cup: 3 lần\n",
      "  - v-league: 2 lần\n",
      "  - champions league: 1 lần\n",
      "  - aff cup: 1 lần\n",
      "Đã lưu kết quả vào d:\\data\\Search_Engine\\processed_vnexpress_20251004_180550.json\n",
      "    Word  Freq  r  Pr(%)  r*Pr\n",
      "     đội   252  1   1.17 0.012\n",
      " cầu_thủ   225  2   1.05 0.021\n",
      "     nam   219  3   1.02 0.031\n",
      "    trận   210  4   0.98 0.039\n",
      "    việt   196  5   0.91 0.046\n",
      "     năm   188  6   0.88 0.053\n",
      "     hai   185  7   0.86 0.060\n",
      "     hlv   182  8   0.85 0.068\n",
      "     đến   182  9   0.85 0.076\n",
      "việt_nam   140 10   0.65 0.065\n",
      "     tôi   140 11   0.65 0.072\n",
      "   trước   138 12   0.64 0.077\n",
      "  league   138 13   0.64 0.084\n",
      "    phải   133 14   0.62 0.087\n",
      "    bóng   126 15   0.59 0.088\n",
      "     ông   124 16   0.58 0.092\n",
      "    nhất   120 17   0.56 0.095\n",
      " bóng_đá   117 18   0.55 0.098\n",
      "     sân   116 19   0.54 0.103\n",
      "   nhiều   115 20   0.54 0.107\n",
      "    giải   115 21   0.54 0.113\n",
      "   thắng   114 22   0.53 0.117\n",
      "     clb   107 23   0.50 0.115\n",
      " thi_đấu    99 24   0.46 0.111\n",
      "  hà_nội    99 25   0.46 0.115\n",
      "     anh    93 26   0.43 0.113\n",
      "      họ    87 27   0.41 0.109\n",
      "      ba    82 28   0.38 0.107\n",
      "     mùa    81 29   0.38 0.109\n",
      "    cùng    78 30   0.36 0.109\n",
      "     cao    77 31   0.36 0.111\n",
      "     hơn    77 32   0.36 0.115\n",
      "     cup    76 33   0.35 0.117\n",
      " vô_địch    76 34   0.35 0.120\n",
      "    ngày    74 35   0.34 0.121\n",
      "  có_thể    74 36   0.34 0.124\n",
      "    đang    74 37   0.34 0.128\n",
      "    chơi    73 38   0.34 0.129\n",
      "     bàn    72 39   0.34 0.131\n",
      "      sự    71 40   0.33 0.132\n",
      "      đá    71 41   0.33 0.136\n",
      "    phút    70 42   0.33 0.137\n",
      "    từng    67 43   0.31 0.134\n",
      "     thứ    67 44   0.31 0.137\n",
      "     làm    66 45   0.31 0.138\n",
      "  nguyễn    64 46   0.30 0.137\n",
      "    park    62 47   0.29 0.136\n",
      "     mới    62 48   0.29 0.139\n",
      "    vòng    62 49   0.29 0.142\n",
      "    điểm    62 50   0.29 0.144\n",
      "\n",
      "Đã lưu kết quả 50 documents (gộp) vào: wordfreq_50docs_total_20251004_180551.csv\n",
      "\n",
      "Đã đóng kết nối database\n",
      "HOÀN THÀNH!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import certifi\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "# Text processing libraries\n",
    "try:\n",
    "    from pyvi import ViTokenizer, ViPosTagger\n",
    "    PYVI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyVi chưa cài đặt. Cài đặt bằng: pip install pyvi\")\n",
    "    PYVI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"vi_core_news_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "except:\n",
    "    print(\"spaCy Vietnamese model chưa có. Cài đặt bằng: python -m spacy download vi_core_news_sm\")\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "# MongoDB connection\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "DB_NAME = \"vnexpress_db\"\n",
    "COLLECTION_NAME = \"vnexpress_bongda\"\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self):\n",
    "        # Vietnamese stopwords\n",
    "        self.stop_words = set([\n",
    "            'và', 'của', 'trong', 'với', 'là', 'có', 'được', 'cho', 'từ', 'một', 'các',\n",
    "            'để', 'không', 'sẽ', 'đã', 'về', 'hay', 'theo', 'như', 'cũng', 'này', 'đó',\n",
    "            'khi', 'những', 'tại', 'sau', 'bị', 'giữa', 'trên', 'dưới', 'ngoài',\n",
    "            'thì', 'nhưng', 'mà', 'hoặc', 'nếu', 'vì', 'do', 'nên', 'rồi', 'còn', 'đều',\n",
    "            'chỉ', 'việc', 'người', 'lại', 'đây', 'đấy', 'ở', 'ra', 'vào', 'lên', 'xuống'\n",
    "        ])\n",
    "        \n",
    "        # Football-specific entities\n",
    "        self.football_teams = [\n",
    "            'hà nội fc', 'hoàng anh gia lai', 'sài gòn fc', 'than quảng ninh', 'viettel fc',\n",
    "            'becamex bình dương', 'slna', 'đà nẵng', 'nam định', 'hải phòng fc', 'hcm city',\n",
    "            'song lam nghệ an', 'quảng nam', 'khánh hòa', 'đội tuyển việt nam', 'tuyển việt nam'\n",
    "        ]\n",
    "        \n",
    "        self.player_patterns = [\n",
    "            r'cầu thủ\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'tiền đạo\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'thủ môn\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'hậu vệ\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'tiền vệ\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'HLV\\s+([A-Za-zÀ-ỹ\\s]{3,30})',\n",
    "            r'huấn luyện viên\\s+([A-Za-zÀ-ỹ\\s]{3,30})'\n",
    "        ]\n",
    "        \n",
    "        self.competitions = [\n",
    "            'v-league', 'v.league', 'v league', 'cup quốc gia', 'aff cup', 'sea games',\n",
    "            'world cup', 'asian cup', 'champions league', 'afc cup', 'u23', 'u22', 'u19'\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Làm sạch và chuẩn hóa text tiếng Việt\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\sàáảãạăắằẳẵặâấầẩẫậèéẻẽẹêếềểễệìíỉĩịòóỏõọôốồổỗộơớờởỡợùúủũụưứừửữựỳýỷỹỵđĐ]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize_vietnamese(self, text):\n",
    "        \"\"\"Tách từ tiếng Việt\"\"\"\n",
    "        if PYVI_AVAILABLE:\n",
    "            try:\n",
    "                return ViTokenizer.tokenize(text).split()\n",
    "            except:\n",
    "                pass\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        entities = {'teams': [], 'players': [], 'competitions': []}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for team in self.football_teams:\n",
    "            if team in text_lower:\n",
    "                entities['teams'].append(team)\n",
    "        \n",
    "        for pattern in self.player_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                player_name = match.group(1).strip()\n",
    "                if len(player_name) > 3:\n",
    "                    entities['players'].append(player_name)\n",
    "        \n",
    "        for comp in self.competitions:\n",
    "            if comp in text_lower:\n",
    "                entities['competitions'].append(comp)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def process_document(self, doc):\n",
    "        title = doc.get('title', '')\n",
    "        content = doc.get('content', '')\n",
    "        full_text = f\"{title} {content}\"\n",
    "        \n",
    "        cleaned_text = self.clean_text(full_text)\n",
    "        tokens = self.tokenize_vietnamese(cleaned_text)\n",
    "        filtered_tokens = self.remove_stopwords(tokens)\n",
    "        entities = self.extract_entities(full_text)\n",
    "        \n",
    "        return {\n",
    "            'doc_id': str(doc.get('_id', '')),\n",
    "            'title': title,\n",
    "            'url': doc.get('url', ''),\n",
    "            'date': doc.get('date', ''),\n",
    "            'author': doc.get('author', ''),\n",
    "            'original_length': len(full_text),\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'tokens': tokens,\n",
    "            'filtered_tokens': filtered_tokens,\n",
    "            'token_count': len(tokens),\n",
    "            'filtered_count': len(filtered_tokens),\n",
    "            'entities': entities,\n",
    "            'entity_count': sum(len(entities[key]) for key in entities)\n",
    "        }\n",
    "\n",
    "def connect_to_database():\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())\n",
    "        db = client[DB_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        doc_count = collection.count_documents({})\n",
    "        print(f\"Kết nối database thành công!\")\n",
    "        print(f\"Tổng số documents: {doc_count}\")\n",
    "        return client, db, collection\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi kết nối database: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def process_all_documents(collection, limit=None):\n",
    "    processor = VietnameseTextProcessor()\n",
    "    if limit:\n",
    "        documents = collection.find().limit(limit)\n",
    "        total_docs = limit\n",
    "    else:\n",
    "        documents = collection.find()\n",
    "        total_docs = collection.count_documents({})\n",
    "    \n",
    "    print(f\"Bắt đầu xử lý {total_docs} documents...\")\n",
    "    processed_results, error_count = [], 0\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Processing documents\", total=total_docs):\n",
    "        try:\n",
    "            result = processor.process_document(doc)\n",
    "            processed_results.append(result)\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Lỗi xử lý document {doc.get('_id', 'unknown')}: {e}\")\n",
    "    \n",
    "    print(f\"Hoàn thành! Xử lý thành công: {len(processed_results)}, Lỗi: {error_count}\")\n",
    "    return processed_results\n",
    "\n",
    "def analyze_results(results):\n",
    "    print(\"\\nPHÂN TÍCH KẾT QUẢ:\")\n",
    "    print(f\"Tổng số documents đã xử lý: {len(results)}\")\n",
    "    \n",
    "    token_counts = [r['token_count'] for r in results]\n",
    "    filtered_counts = [r['filtered_count'] for r in results]\n",
    "    entity_counts = [r['entity_count'] for r in results]\n",
    "    \n",
    "    print(f\"\\nThống kê Tokens:\")\n",
    "    print(f\"  - Trung bình tokens/document: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  - Trung bình filtered tokens/document: {sum(filtered_counts)/len(filtered_counts):.1f}\")\n",
    "    print(f\"  - Trung bình entities/document: {sum(entity_counts)/len(entity_counts):.1f}\")\n",
    "    \n",
    "    all_teams, all_players, all_competitions = [], [], []\n",
    "    for r in results:\n",
    "        all_teams.extend(r['entities']['teams'])\n",
    "        all_players.extend(r['entities']['players'])\n",
    "        all_competitions.extend(r['entities']['competitions'])\n",
    "    \n",
    "    print(f\"\\nTop 10 Đội bóng được nhắc đến nhiều nhất:\")\n",
    "    for team, count in Counter(all_teams).most_common(10):\n",
    "        print(f\"  - {team}: {count} lần\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Cầu thủ được nhắc đến nhiều nhất:\")\n",
    "    for player, count in Counter(all_players).most_common(10):\n",
    "        print(f\"  - {player}: {count} lần\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Giải đấu được nhắc đến nhiều nhất:\")\n",
    "    for comp, count in Counter(all_competitions).most_common(10):\n",
    "        print(f\"  - {comp}: {count} lần\")\n",
    "\n",
    "def save_results(results, filename=\"processed_data.json\"):\n",
    "    filepath = os.path.join(\"d:\\\\data\\\\Search_Engine\", filename)\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"Đã lưu kết quả vào {filepath}\")\n",
    "\n",
    "def analyze_multiple_documents(collection, processor, n=50, top_n=50, filename=None):\n",
    "    \"\"\"Lấy ngẫu nhiên n documents, gộp tokens và tính tần suất chung\"\"\"\n",
    "    docs = collection.aggregate([{\"$sample\": {\"size\": n}}])\n",
    "    all_tokens = []\n",
    "    \n",
    "    for idx, doc in enumerate(docs, 1):\n",
    "        result = processor.process_document(doc)\n",
    "        tokens = result[\"filtered_tokens\"]\n",
    "        all_tokens.extend(tokens)   # gộp tất cả tokens lại\n",
    "    \n",
    "    # Tính tần suất từ toàn bộ tokens\n",
    "    counter = Counter(all_tokens)\n",
    "    total = sum(counter.values())\n",
    "    \n",
    "    data = []\n",
    "    for i, (word, freq) in enumerate(counter.most_common(top_n), 1):\n",
    "        Pr = freq / total * 100\n",
    "        data.append({\n",
    "            \"Word\": word,\n",
    "            \"Freq\": freq,\n",
    "            \"r\": i,\n",
    "            \"Pr(%)\": round(Pr, 2),\n",
    "            \"r*Pr\": round(i * Pr / 100, 3)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Xuất file CSV 1 lần\n",
    "    if not filename:\n",
    "        filename = f\"wordfreq_{n}docs_total_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nĐã lưu kết quả {n} documents (gộp) vào: {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Sinh n-grams từ list tokens\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def analyze_and_save_ngrams(collection, processor, n_docs=1800, max_n=5, top_k=50, filename=None):\n",
    "    \"\"\"\n",
    "    Phân tích n-grams (1 -> max_n), lưu tất cả vào 1 file CSV duy nhất\n",
    "    \"\"\"\n",
    "    docs = collection.aggregate([{\"$sample\": {\"size\": n_docs}}])\n",
    "\n",
    "    all_tokens_list = []\n",
    "    for doc in docs:\n",
    "        result = processor.process_document(doc)\n",
    "        all_tokens_list.append(result[\"filtered_tokens\"])\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        ngrams_all = []\n",
    "        for tokens in all_tokens_list:\n",
    "            ngrams_all.extend(generate_ngrams(tokens, n))\n",
    "        \n",
    "        counter = Counter(ngrams_all)\n",
    "        for phrase, freq in counter.most_common(top_k):\n",
    "            all_data.append({\n",
    "                \"n\": n,\n",
    "                \"Frequency\": freq,\n",
    "                \"Phrase\": phrase\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    if not filename:\n",
    "        filename = f\"ngrams_{n_docs}docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Đã lưu kết quả n-grams (1-{max_n}) vào: {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"BẮT ĐẦU XỬ LÝ TEXT TỪ MONGODB\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    client, db, collection = connect_to_database()\n",
    "    if collection is None:\n",
    "        print(\"Không thể kết nối database!\")\n",
    "        return\n",
    "    \n",
    "    results = process_all_documents(collection, limit=5)  \n",
    "    if results:\n",
    "        analyze_results(results)\n",
    "        save_results(results, f\"processed_vnexpress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    \n",
    "    # Thử lấy 1 document ngẫu nhiên và phân tích từ vựng\n",
    "    processor = VietnameseTextProcessor()\n",
    "    analyze_multiple_documents(collection, processor, n=50, top_n=50)\n",
    "    analyze_and_save_ngrams(collection, processor, n_docs=1830, max_n=5, top_k=100)\n",
    "\n",
    "    client.close()\n",
    "    print(\"\\nĐã đóng kết nối database\")\n",
    "    print(\"HOÀN THÀNH!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
