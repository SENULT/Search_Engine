"""
QUERY PROCESSING & ACCENT RESTORATION

X·ª≠ l√Ω query t·ª´ user v·ªõi c√°c t√≠nh nƒÉng:
1. Vietnamese accent restoration (bong da ‚Üí b√≥ng ƒë√°)
2. Spell checking and correction
3. Query normalization
4. Synonym expansion
5. Query suggestion

Features:
- Automatic accent restoration using vocabulary
- Fuzzy matching for typos
- Smart query preprocessing
"""

import os
import json
import re
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Set
from difflib import SequenceMatcher
import unicodedata

# Import t·ª´ modules
import sys
sys.path.append(os.path.dirname(__file__))


class VietnameseAccentRestorer:
    """
    T·ª± ƒë·ªông restore d·∫•u ti·∫øng Vi·ªát
    
    V√≠ d·ª•:
    - "bong da" ‚Üí "b√≥ng ƒë√°"
    - "viet nam" ‚Üí "vi·ªát nam"
    - "hlv" ‚Üí "HLV" (gi·ªØ nguy√™n abbreviation)
    """
    
    def __init__(self, vocab_file: str = None):
        """
        Args:
            vocab_file: File ch·ª©a vocabulary (words c√≥ d·∫•u)
        """
        self.vocab_file = vocab_file or "data/vocab/vocab.txt"
        self.vocabulary = set()  # Words c√≥ d·∫•u
        self.accent_map = defaultdict(list)  # {word_no_accent: [word_with_accent1, word_with_accent2, ...]}
        self.word_freq = Counter()  # Frequency c·ªßa m·ªói word
        
        self.load_vocabulary()
        self.build_accent_map()
    
    def remove_accents(self, text: str) -> str:
        """
        Remove Vietnamese accents
        
        v√≠ d·ª•: "b√≥ng ƒë√°" ‚Üí "bong da"
        """
        # Normalize Unicode
        text = unicodedata.normalize('NFD', text)
        # Remove combining characters
        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
        # Normalize l·∫°i
        text = unicodedata.normalize('NFC', text)
        
        # Replace ƒë/ƒê
        text = text.replace('ƒë', 'd').replace('ƒê', 'D')
        
        return text
    
    def load_vocabulary(self):
        """Load vocabulary t·ª´ file"""
        if not os.path.exists(self.vocab_file):
            print(f"‚ö†Ô∏è Vocabulary file not found: {self.vocab_file}")
            print("   Using fallback vocabulary...")
            # Fallback vocabulary cho football
            self.vocabulary = {
                'b√≥ng_ƒë√°', 'vi·ªát_nam', 'th√°i_lan', 'hu·∫•n_luy·ªán_vi√™n',
                'c·∫ßu_th·ªß', 'tr·∫≠n_ƒë·∫•u', 'gi·∫£i_ƒë·∫•u', 'v√¥_ƒë·ªãch',
                'th·∫Øng', 'thua', 'h√≤a', 'b√†n_th·∫Øng', 'penalty',
                'th·ªß_m√¥n', 'h·∫≠u_v·ªá', 'ti·ªÅn_ƒë·∫°o', 'ti·ªÅn_v·ªá'
            }
            return
        
        try:
            with open(self.vocab_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word:
                        self.vocabulary.add(word)
                        self.word_freq[word] += 1
            
            print(f"‚úì Loaded vocabulary: {len(self.vocabulary):,} words")
        except Exception as e:
            print(f"‚úó Error loading vocabulary: {e}")
    
    def build_accent_map(self):
        """
        X√¢y d·ª±ng map: word kh√¥ng d·∫•u ‚Üí list words c√≥ d·∫•u
        
        V√≠ d·ª•:
        {
            'bong_da': ['b√≥ng_ƒë√°', 'b√¥ng_da'],
            'viet_nam': ['vi·ªát_nam', 'vi·∫øt_nam']
        }
        """
        for word in self.vocabulary:
            word_no_accent = self.remove_accents(word.lower())
            self.accent_map[word_no_accent].append(word)
        
        print(f"‚úì Built accent map: {len(self.accent_map):,} entries")
    
    def restore_word(self, word: str, context_words: List[str] = None) -> str:
        """
        Restore d·∫•u cho m·ªôt word
        
        Args:
            word: Word kh√¥ng d·∫•u (v√≠ d·ª•: "bong")
            context_words: Context words ƒë·ªÉ ch·ªçn candidate t·ªët nh·∫•t
            
        Returns:
            Word c√≥ d·∫•u (v√≠ d·ª•: "b√≥ng")
        """
        word_lower = word.lower()
        word_no_accent = self.remove_accents(word_lower)
        
        # N·∫øu word ƒë√£ c√≥ trong vocabulary ‚Üí return nguy√™n
        if word_lower in self.vocabulary:
            return word_lower
        
        # T√¨m candidates trong accent_map
        candidates = self.accent_map.get(word_no_accent, [])
        
        if not candidates:
            # Kh√¥ng t√¨m th·∫•y ‚Üí return nguy√™n
            return word_lower
        
        if len(candidates) == 1:
            # Ch·ªâ c√≥ 1 candidate ‚Üí return lu√¥n
            return candidates[0]
        
        # Nhi·ªÅu candidates ‚Üí ch·ªçn theo frequency ho·∫∑c context
        if context_words:
            # TODO: Implement context-based selection
            pass
        
        # Ch·ªçn candidate ph·ªï bi·∫øn nh·∫•t
        best_candidate = max(candidates, key=lambda w: self.word_freq.get(w, 0))
        return best_candidate
    
    def restore_text(self, text: str) -> str:
        """
        Restore d·∫•u cho to√†n b·ªô text
        
        Args:
            text: Text kh√¥ng d·∫•u (v√≠ d·ª•: "bong da viet nam")
            
        Returns:
            Text c√≥ d·∫•u (v√≠ d·ª•: "b√≥ng ƒë√° vi·ªát nam")
        """
        # Tokenize
        words = text.lower().split()
        
        # Restore t·ª´ng word
        restored_words = []
        for i, word in enumerate(words):
            # L·∫•y context (2 words tr∆∞·ªõc + 2 words sau)
            context = words[max(0, i-2):i] + words[i+1:min(len(words), i+3)]
            restored = self.restore_word(word, context)
            restored_words.append(restored)
        
        return ' '.join(restored_words)
    
    def get_suggestions(self, word: str, max_suggestions: int = 5) -> List[Tuple[str, float]]:
        """
        G·ª£i √Ω c√°c t·ª´ t∆∞∆°ng t·ª± (cho autocomplete)
        
        Returns:
            List[(word, similarity_score)]
        """
        word_no_accent = self.remove_accents(word.lower())
        
        # T√¨m t·∫•t c·∫£ words c√≥ prefix gi·ªëng
        suggestions = []
        
        for vocab_word in self.vocabulary:
            vocab_no_accent = self.remove_accents(vocab_word)
            
            # Check prefix
            if vocab_no_accent.startswith(word_no_accent):
                similarity = 1.0
                suggestions.append((vocab_word, similarity))
            else:
                # Check fuzzy match
                similarity = SequenceMatcher(None, word_no_accent, vocab_no_accent).ratio()
                if similarity > 0.7:
                    suggestions.append((vocab_word, similarity))
        
        # Sort by similarity + frequency
        suggestions.sort(key=lambda x: (x[1], self.word_freq.get(x[0], 0)), reverse=True)
        
        return suggestions[:max_suggestions]


class QueryProcessor:
    """
    Query Processing v·ªõi accent restoration
    """
    
    def __init__(self, vocab_file: str = None):
        self.accent_restorer = VietnameseAccentRestorer(vocab_file)
        
        # Vietnamese stopwords (optional, ƒë·ªÉ filter query)
        self.stopwords = {
            'c·ªßa', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'trong', '·ªü', 't·∫°i',
            'v·ªõi', 'ƒë·ªÉ', 'cho', 't·ª´', 'v·ªÅ', 'theo', 'nh∆∞'
        }
        
        # Common Vietnamese abbreviations
        self.abbreviations = {
            'hlv': 'hu·∫•n_luy·ªán_vi√™n',
            'slna': 's√¥ng_lam_ngh·ªá_an',
            'hagl': 'ho√†ng_anh_gia_lai',
            'vff': 'li√™n_ƒëo√†n_b√≥ng_ƒë√°_vi·ªát_nam',
            'aff': 'asean_football_federation',
            'vl': 'v√≤ng_lo·∫°i'
        }
    
    def normalize_query(self, query: str) -> str:
        """
        Normalize query:
        1. Lowercase
        2. Remove extra spaces
        3. Remove special chars
        """
        # Lowercase
        query = query.lower().strip()
        
        # Remove special characters (keep spaces and underscore)
        query = re.sub(r'[^\w\s_]', ' ', query)
        
        # Remove extra spaces
        query = re.sub(r'\s+', ' ', query)
        
        return query
    
    def expand_abbreviations(self, query: str) -> str:
        """
        Expand abbreviations
        
        V√≠ d·ª•: "hlv park" ‚Üí "hu·∫•n luy·ªán vi√™n park"
        """
        words = query.split()
        expanded_words = []
        
        for word in words:
            if word in self.abbreviations:
                expanded_words.append(self.abbreviations[word])
            else:
                expanded_words.append(word)
        
        return ' '.join(expanded_words)
    
    def process_query(self, 
                     query: str, 
                     restore_accents: bool = True,
                     expand_abbr: bool = True,
                     remove_stopwords: bool = False) -> Dict:
        """
        Process query t·ª´ user
        
        Args:
            query: Raw query t·ª´ user
            restore_accents: C√≥ restore d·∫•u kh√¥ng
            expand_abbr: C√≥ expand abbreviations kh√¥ng
            remove_stopwords: C√≥ remove stopwords kh√¥ng
            
        Returns:
            Dict {
                'original': query g·ªëc,
                'normalized': query ƒë√£ normalize,
                'restored': query ƒë√£ restore d·∫•u,
                'expanded': query ƒë√£ expand,
                'tokens': list tokens,
                'suggestions': list g·ª£i √Ω
            }
        """
        result = {
            'original': query,
            'normalized': None,
            'restored': None,
            'expanded': None,
            'tokens': [],
            'suggestions': []
        }
        
        # Step 1: Normalize
        normalized = self.normalize_query(query)
        result['normalized'] = normalized
        
        # Step 2: Expand abbreviations
        if expand_abbr:
            expanded = self.expand_abbreviations(normalized)
        else:
            expanded = normalized
        result['expanded'] = expanded
        
        # Step 3: Restore accents
        if restore_accents:
            restored = self.accent_restorer.restore_text(expanded)
        else:
            restored = expanded
        result['restored'] = restored
        
        # Step 4: Tokenize
        tokens = restored.split()
        
        # Step 5: Remove stopwords (optional)
        if remove_stopwords:
            tokens = [t for t in tokens if t not in self.stopwords]
        
        result['tokens'] = tokens
        
        # Step 6: Get suggestions cho t·ª´ng token
        for token in tokens:
            suggestions = self.accent_restorer.get_suggestions(token, max_suggestions=3)
            if suggestions:
                result['suggestions'].append({
                    'token': token,
                    'alternatives': [s[0] for s in suggestions]
                })
        
        return result


class QueryIndexInterface:
    """
    Interface ƒë·ªÉ query v√†o inverted index v·ªõi accent restoration
    """
    
    def __init__(self, index_file: str = None, vocab_file: str = None):
        """
        Args:
            index_file: Path to inverted index pickle file
            vocab_file: Path to vocabulary file
        """
        self.query_processor = QueryProcessor(vocab_file)
        self.index = None
        self.ranker = None
        
        if index_file:
            self.load_index(index_file)
    
    def load_index(self, index_file: str):
        """Load inverted index"""
        try:
            from src.indexing.inverted_index import IndexBuilder
            from src.ranking.rankers import CombinedRanker
            
            self.index = IndexBuilder.load_index_from_pickle(index_file)
            self.ranker = CombinedRanker(self.index)
            
            print(f"‚úì Loaded index: {self.index.total_docs:,} documents")
        except Exception as e:
            print(f"‚úó Error loading index: {e}")
    
    def search(self, 
               query: str, 
               top_k: int = 10,
               method: str = 'bm25',
               restore_accents: bool = True,
               verbose: bool = True) -> List[Tuple[str, float]]:
        """
        Search v·ªõi query processing t·ª± ƒë·ªông
        
        Args:
            query: Raw query t·ª´ user (c√≥ th·ªÉ kh√¥ng d·∫•u)
            top_k: S·ªë k·∫øt qu·∫£
            method: Ranking method ('bm25', 'tfidf', 'combined')
            restore_accents: T·ª± ƒë·ªông restore d·∫•u
            verbose: In chi ti·∫øt query processing
            
        Returns:
            List[(doc_id, score)]
        """
        if not self.ranker:
            print("‚ö†Ô∏è Index ch∆∞a ƒë∆∞·ª£c load. Call load_index() tr∆∞·ªõc.")
            return []
        
        # Process query
        processed = self.query_processor.process_query(
            query,
            restore_accents=restore_accents,
            expand_abbr=True,
            remove_stopwords=False
        )
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"QUERY PROCESSING")
            print(f"{'='*80}")
            print(f"Original:   {processed['original']}")
            print(f"Normalized: {processed['normalized']}")
            print(f"Expanded:   {processed['expanded']}")
            print(f"Restored:   {processed['restored']}")
            print(f"Tokens:     {processed['tokens']}")
            
            if processed['suggestions']:
                print(f"\nSuggestions:")
                for sugg in processed['suggestions']:
                    print(f"  '{sugg['token']}' ‚Üí {sugg['alternatives']}")
            print(f"{'='*80}\n")
        
        # Search v·ªõi restored tokens
        query_terms = processed['tokens']
        results = self.ranker.search(query_terms, top_k=top_k, method=method)
        
        return results
    
    def interactive_search(self):
        """Interactive search interface"""
        print(f"\n{'='*80}")
        print(f"VIETNAMESE SEARCH ENGINE - INTERACTIVE MODE")
        print(f"{'='*80}")
        print(f"Features:")
        print(f"  ‚úì Automatic accent restoration (bong da ‚Üí b√≥ng ƒë√°)")
        print(f"  ‚úì Abbreviation expansion (hlv ‚Üí hu·∫•n luy·ªán vi√™n)")
        print(f"  ‚úì Smart query processing")
        print(f"\nCommands:")
        print(f"  - Type your query to search")
        print(f"  - Type 'exit' or 'quit' to exit")
        print(f"  - Type 'help' for more commands")
        print(f"{'='*80}\n")
        
        while True:
            try:
                query = input("üîç Search: ").strip()
                
                if not query:
                    continue
                
                if query.lower() in ['exit', 'quit', 'q']:
                    print("Goodbye! üëã")
                    break
                
                if query.lower() == 'help':
                    print("\nAvailable commands:")
                    print("  search <query>  - Search with accent restoration")
                    print("  raw <query>     - Search without accent restoration")
                    print("  suggest <word>  - Get suggestions for a word")
                    print("  exit/quit       - Exit")
                    continue
                
                if query.startswith('raw '):
                    # Search without accent restoration
                    raw_query = query[4:].strip()
                    results = self.search(raw_query, restore_accents=False, verbose=True)
                elif query.startswith('suggest '):
                    # Get suggestions
                    word = query[8:].strip()
                    suggestions = self.query_processor.accent_restorer.get_suggestions(word)
                    print(f"\nSuggestions for '{word}':")
                    for i, (sugg, score) in enumerate(suggestions, 1):
                        print(f"  {i}. {sugg} (score: {score:.2f})")
                    continue
                else:
                    # Normal search with accent restoration
                    results = self.search(query, restore_accents=True, verbose=True)
                
                # Display results
                if results:
                    print(f"\nüìä RESULTS (Top {len(results)}):")
                    print(f"{'='*80}")
                    for rank, (doc_id, score) in enumerate(results, 1):
                        print(f"{rank:2d}. Doc: {doc_id[:60]:<60} | Score: {score:8.4f}")
                    print(f"{'='*80}\n")
                else:
                    print("‚ùå No results found.\n")
                
            except KeyboardInterrupt:
                print("\n\nGoodbye! üëã")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}\n")


def demo():
    """Demo query processing"""
    print("="*80)
    print("QUERY PROCESSING & ACCENT RESTORATION - DEMO")
    print("="*80)
    
    # Initialize
    processor = QueryProcessor()
    
    # Test queries
    test_queries = [
        "bong da viet nam",
        "hlv park hang seo",
        "doi tuyen thai lan",
        "tran chung ket aff cup",
        "cau thu xuat sac nhat"
    ]
    
    print("\nüìù TEST QUERIES:\n")
    
    for query in test_queries:
        result = processor.process_query(query)
        
        print(f"Original:  '{result['original']}'")
        print(f"Restored:  '{result['restored']}'")
        print(f"Tokens:    {result['tokens']}")
        
        if result['suggestions']:
            print(f"Suggestions:")
            for sugg in result['suggestions'][:2]:  # Top 2
                print(f"  - {sugg['token']} ‚Üí {sugg['alternatives'][:2]}")
        
        print("-" * 80)
    
    print("\n‚úì DEMO COMPLETED!")


if __name__ == "__main__":
    demo()
