{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db375ee9",
   "metadata": {},
   "source": [
    "# Text Processing for Vietnamese Football News\n",
    "\n",
    "Notebook n√†y x·ª≠ l√Ω d·ªØ li·ªáu b√≥ng ƒë√° t·ª´ MongoDB, bao g·ªìm:\n",
    "- K·∫øt n·ªëi MongoDB\n",
    "- L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát\n",
    "- T√°ch t·ª´ v√† lo·∫°i b·ªè stopwords\n",
    "- Tr√≠ch xu·∫•t entities (ƒë·ªôi b√≥ng, c·∫ßu th·ªß, gi·∫£i ƒë·∫•u)\n",
    "- Ph√¢n t√≠ch t·∫ßn su·∫•t t·ª´ v√† n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f4ff2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72ae9755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import certifi\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bf7a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyVi ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n"
     ]
    }
   ],
   "source": [
    "# Text processing libraries - PyVi\n",
    "try:\n",
    "    from pyvi import ViTokenizer, ViPosTagger\n",
    "    PYVI_AVAILABLE = True\n",
    "    print(\"‚úì PyVi ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyVi ch∆∞a c√†i ƒë·∫∑t. C√†i ƒë·∫∑t b·∫±ng: pip install pyvi\")\n",
    "    PYVI_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1be0ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó spaCy Vietnamese model ch∆∞a c√≥. C√†i ƒë·∫∑t b·∫±ng: python -m spacy download vi_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "# Text processing libraries - spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"vi_core_news_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"‚úì spaCy Vietnamese model ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "except:\n",
    "    print(\"‚úó spaCy Vietnamese model ch∆∞a c√≥. C√†i ƒë·∫∑t b·∫±ng: python -m spacy download vi_core_news_sm\")\n",
    "    SPACY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e3182",
   "metadata": {},
   "source": [
    "## 2. MongoDB Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b9a6c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: vnexpress_db\n",
      "Collection: vnexpress_bongda\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection\n",
    "MONGO_URI = load_dotenv(\"MONGO_URI\") or os.getenv(\"MONGO_URI\")\n",
    "DB_NAME = \"vnexpress_db\"\n",
    "COLLECTION_NAME = \"vnexpress_bongda\"\n",
    "\n",
    "print(f\"Database: {DB_NAME}\")\n",
    "print(f\"Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab0d59",
   "metadata": {},
   "source": [
    "## 3. Vietnamese Text Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93e304bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VietnameseTextProcessor class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "class VietnameseTextProcessor:\n",
    "    def __init__(self):\n",
    "        # Vietnamese stopwords\n",
    "        self.stop_words = set([\n",
    "            'v√†', 'c·ªßa', 'trong', 'v·ªõi', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'cho', 't·ª´', 'm·ªôt', 'c√°c',\n",
    "            'ƒë·ªÉ', 'kh√¥ng', 's·∫Ω', 'ƒë√£', 'v·ªÅ', 'hay', 'theo', 'nh∆∞', 'c≈©ng', 'n√†y', 'ƒë√≥',\n",
    "            'khi', 'nh·ªØng', 't·∫°i', 'sau', 'b·ªã', 'gi·ªØa', 'tr√™n', 'd∆∞·ªõi', 'ngo√†i',\n",
    "            'th√¨', 'nh∆∞ng', 'm√†', 'ho·∫∑c', 'n·∫øu', 'v√¨', 'do', 'n√™n', 'r·ªìi', 'c√≤n', 'ƒë·ªÅu',\n",
    "            'ch·ªâ', 'vi·ªác', 'ng∆∞·ªùi', 'l·∫°i', 'ƒë√¢y', 'ƒë·∫•y', '·ªü', 'ra', 'v√†o', 'l√™n', 'xu·ªëng'\n",
    "        ])\n",
    "        \n",
    "        # Football-specific entities\n",
    "        self.football_teams = [\n",
    "            'h√† n·ªôi fc', 'ho√†ng anh gia lai', 's√†i g√≤n fc', 'than qu·∫£ng ninh', 'viettel fc',\n",
    "            'becamex b√¨nh d∆∞∆°ng', 'slna', 'ƒë√† n·∫µng', 'nam ƒë·ªãnh', 'h·∫£i ph√≤ng fc', 'hcm city',\n",
    "            'song lam ngh·ªá an', 'qu·∫£ng nam', 'kh√°nh h√≤a', 'ƒë·ªôi tuy·ªÉn vi·ªát nam', 'tuy·ªÉn vi·ªát nam'\n",
    "        ]\n",
    "        \n",
    "        self.player_patterns = [\n",
    "            r'c·∫ßu th·ªß\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'ti·ªÅn ƒë·∫°o\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'th·ªß m√¥n\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'h·∫≠u v·ªá\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'ti·ªÅn v·ªá\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'HLV\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})',\n",
    "            r'hu·∫•n luy·ªán vi√™n\\s+([A-Za-z√Ä-·ªπ\\s]{3,30})'\n",
    "        ]\n",
    "        \n",
    "        self.competitions = [\n",
    "            'v-league', 'v.league', 'v league', 'cup qu·ªëc gia', 'aff cup', 'sea games',\n",
    "            'world cup', 'asian cup', 'champions league', 'afc cup', 'u23', 'u22', 'u19'\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"L√†m s·∫°ch v√† chu·∫©n h√≥a text ti·∫øng Vi·ªát\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize_vietnamese(self, text):\n",
    "        \"\"\"T√°ch t·ª´ ti·∫øng Vi·ªát\"\"\"\n",
    "        if PYVI_AVAILABLE:\n",
    "            try:\n",
    "                return ViTokenizer.tokenize(text).split()\n",
    "            except:\n",
    "                pass\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        entities = {'teams': [], 'players': [], 'competitions': []}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for team in self.football_teams:\n",
    "            if team in text_lower:\n",
    "                entities['teams'].append(team)\n",
    "        \n",
    "        for pattern in self.player_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                player_name = match.group(1).strip()\n",
    "                if len(player_name) > 3:\n",
    "                    entities['players'].append(player_name)\n",
    "        \n",
    "        for comp in self.competitions:\n",
    "            if comp in text_lower:\n",
    "                entities['competitions'].append(comp)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def process_document(self, doc):\n",
    "        title = doc.get('title', '')\n",
    "        content = doc.get('content', '')\n",
    "        full_text = f\"{title} {content}\"\n",
    "        \n",
    "        cleaned_text = self.clean_text(full_text)\n",
    "        tokens = self.tokenize_vietnamese(cleaned_text)\n",
    "        filtered_tokens = self.remove_stopwords(tokens)\n",
    "        entities = self.extract_entities(full_text)\n",
    "        \n",
    "        return {\n",
    "            'doc_id': str(doc.get('_id', '')),\n",
    "            'title': title,\n",
    "            'url': doc.get('url', ''),\n",
    "            'date': doc.get('date', ''),\n",
    "            'author': doc.get('author', ''),\n",
    "            'original_length': len(full_text),\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'tokens': tokens,\n",
    "            'filtered_tokens': filtered_tokens,\n",
    "            'token_count': len(tokens),\n",
    "            'filtered_count': len(filtered_tokens),\n",
    "            'entities': entities,\n",
    "            'entity_count': sum(len(entities[key]) for key in entities)\n",
    "        }\n",
    "\n",
    "print(\"‚úì VietnameseTextProcessor class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e53bdf",
   "metadata": {},
   "source": [
    "## 4. Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d41a68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database functions ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def connect_to_database():\n",
    "    \"\"\"K·∫øt n·ªëi t·ªõi MongoDB\"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())\n",
    "        db = client[DB_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        doc_count = collection.count_documents({})\n",
    "        print(f\"‚úì K·∫øt n·ªëi database th√†nh c√¥ng!\")\n",
    "        print(f\"  T·ªïng s·ªë documents: {doc_count}\")\n",
    "        return client, db, collection\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó L·ªói k·∫øt n·ªëi database: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"‚úì Database functions ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ce963",
   "metadata": {},
   "source": [
    "## 5. Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f30518b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì process_all_documents function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def process_all_documents(collection, limit=None):\n",
    "    \"\"\"X·ª≠ l√Ω t·∫•t c·∫£ documents t·ª´ collection\"\"\"\n",
    "    processor = VietnameseTextProcessor()\n",
    "    if limit:\n",
    "        documents = collection.find().limit(limit)\n",
    "        total_docs = limit\n",
    "    else:\n",
    "        documents = collection.find()\n",
    "        total_docs = collection.count_documents({})\n",
    "    \n",
    "    print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {total_docs} documents...\")\n",
    "    processed_results, error_count = [], 0\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Processing documents\", total=total_docs):\n",
    "        try:\n",
    "            result = processor.process_document(doc)\n",
    "            processed_results.append(result)\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"L·ªói x·ª≠ l√Ω document {doc.get('_id', 'unknown')}: {e}\")\n",
    "    \n",
    "    print(f\"‚úì Ho√†n th√†nh! X·ª≠ l√Ω th√†nh c√¥ng: {len(processed_results)}, L·ªói: {error_count}\")\n",
    "    return processed_results\n",
    "\n",
    "print(\"‚úì process_all_documents function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819ee91",
   "metadata": {},
   "source": [
    "## 6. Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b015ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_results function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Ph√¢n t√≠ch k·∫øt qu·∫£ x·ª≠ l√Ω\"\"\"\n",
    "    print(\"\\nPH√ÇN T√çCH K·∫æT QU·∫¢:\")\n",
    "    print(f\"T·ªïng s·ªë documents ƒë√£ x·ª≠ l√Ω: {len(results)}\")\n",
    "    \n",
    "    token_counts = [r['token_count'] for r in results]\n",
    "    filtered_counts = [r['filtered_count'] for r in results]\n",
    "    entity_counts = [r['entity_count'] for r in results]\n",
    "    \n",
    "    print(f\"\\nTh·ªëng k√™ Tokens:\")\n",
    "    print(f\"  - Trung b√¨nh tokens/document: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  - Trung b√¨nh filtered tokens/document: {sum(filtered_counts)/len(filtered_counts):.1f}\")\n",
    "    print(f\"  - Trung b√¨nh entities/document: {sum(entity_counts)/len(entity_counts):.1f}\")\n",
    "    \n",
    "    all_teams, all_players, all_competitions = [], [], []\n",
    "    for r in results:\n",
    "        all_teams.extend(r['entities']['teams'])\n",
    "        all_players.extend(r['entities']['players'])\n",
    "        all_competitions.extend(r['entities']['competitions'])\n",
    "    \n",
    "    print(f\"\\nTop 10 ƒê·ªôi b√≥ng ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\")\n",
    "    for team, count in Counter(all_teams).most_common(10):\n",
    "        print(f\"  - {team}: {count} l·∫ßn\")\n",
    "    \n",
    "    print(f\"\\nTop 10 C·∫ßu th·ªß ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\")\n",
    "    for player, count in Counter(all_players).most_common(10):\n",
    "        print(f\"  - {player}: {count} l·∫ßn\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Gi·∫£i ƒë·∫•u ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\")\n",
    "    for comp, count in Counter(all_competitions).most_common(10):\n",
    "        print(f\"  - {comp}: {count} l·∫ßn\")\n",
    "\n",
    "print(\"‚úì analyze_results function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb60f384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì save_results function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def save_results(results, filename=\"processed_data.json\"):\n",
    "    \"\"\"L∆∞u k·∫øt qu·∫£ ra file JSON\"\"\"\n",
    "    # T·∫°o folder outputs n·∫øu ch∆∞a c√≥\n",
    "    output_dir = os.path.join(\"d:\\\\data\\\\Search_Engine\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {filepath}\")\n",
    "\n",
    "print(\"‚úì save_results function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc16fce",
   "metadata": {},
   "source": [
    "## 7. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297fc94",
   "metadata": {},
   "source": [
    "## 6.5. Stemming v√† POS Tagging\n",
    "\n",
    "### Stemming - Chu·∫©n h√≥a t·ª´ v·ªÅ d·∫°ng g·ªëc\n",
    "### POS Tagging - G·∫Øn nh√£n t·ª´ lo·∫°i (s·ª≠ d·ª•ng HMM trong PyVi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a6985a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì apply_stemming_and_pos function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def apply_stemming_and_pos(text):\n",
    "    \"\"\"\n",
    "    √Åp d·ª•ng POS Tagging (s·ª≠ d·ª•ng HMM model trong PyVi)\n",
    "    v√† tr√≠ch xu·∫•t t·ª´ g·ªëc t·ª´ POS tags\n",
    "    \"\"\"\n",
    "    if not PYVI_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è PyVi ch∆∞a c√†i ƒë·∫∑t, kh√¥ng th·ªÉ th·ª±c hi·ªán POS tagging\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # POS Tagging v·ªõi PyVi (s·ª≠ d·ª•ng HMM model)\n",
    "        pos_result = ViPosTagger.postagging(ViTokenizer.tokenize(text))\n",
    "        \n",
    "        words = pos_result[0]  # Danh s√°ch t·ª´ ƒë√£ t√°ch\n",
    "        pos_tags = pos_result[1]  # Danh s√°ch nh√£n t·ª´ lo·∫°i\n",
    "        \n",
    "        # T·∫°o list c√°c tuple (word, pos_tag)\n",
    "        word_pos_pairs = list(zip(words, pos_tags))\n",
    "        \n",
    "        # Stemming ƒë∆°n gi·∫£n: lo·∫°i b·ªè c√°c t·ª´ ph·ª• v√† gi·ªØ l·∫°i t·ª´ ch√≠nh\n",
    "        stemmed_words = []\n",
    "        important_pos = ['N', 'V', 'A', 'M']  # Danh t·ª´, ƒê·ªông t·ª´, T√≠nh t·ª´, S·ªë\n",
    "        \n",
    "        for word, pos in word_pos_pairs:\n",
    "            # Ch·ªâ gi·ªØ l·∫°i t·ª´ c√≥ √Ω nghƒ©a quan tr·ªçng\n",
    "            if any(pos.startswith(p) for p in important_pos):\n",
    "                stemmed_words.append(word)\n",
    "        \n",
    "        return {\n",
    "            'original_tokens': words,\n",
    "            'pos_tags': pos_tags,\n",
    "            'word_pos_pairs': word_pos_pairs,\n",
    "            'stemmed_tokens': stemmed_words\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi x·ª≠ l√Ω POS tagging: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì apply_stemming_and_pos function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55752ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_document_with_pos function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def analyze_document_with_pos(collection, doc_id=None, limit=1):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch document v·ªõi POS Tagging v√† Stemming\n",
    "    \"\"\"\n",
    "    processor = VietnameseTextProcessor()\n",
    "    \n",
    "    if doc_id:\n",
    "        from bson import ObjectId\n",
    "        doc = collection.find_one({\"_id\": ObjectId(doc_id)})\n",
    "        docs = [doc] if doc else []\n",
    "    else:\n",
    "        docs = list(collection.find().limit(limit))\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y document!\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üìÑ PH√ÇN T√çCH DOCUMENT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Title: {doc.get('title', 'N/A')}\")\n",
    "        print(f\"URL: {doc.get('url', 'N/A')}\")\n",
    "        print(f\"Date: {doc.get('date', 'N/A')}\")\n",
    "        \n",
    "        # X·ª≠ l√Ω c∆° b·∫£n\n",
    "        basic_result = processor.process_document(doc)\n",
    "        \n",
    "        # L·∫•y cleaned text\n",
    "        cleaned_text = basic_result['cleaned_text'][:1000]  # Gi·ªõi h·∫°n 1000 k√Ω t·ª± ƒë·ªÉ demo\n",
    "        \n",
    "        print(f\"\\nüìù Text m·∫´u (1000 k√Ω t·ª± ƒë·∫ßu):\")\n",
    "        print(cleaned_text)\n",
    "        \n",
    "        # √Åp d·ª•ng POS Tagging v√† Stemming\n",
    "        pos_result = apply_stemming_and_pos(cleaned_text)\n",
    "        \n",
    "        if pos_result:\n",
    "            print(f\"\\nüî§ TOKENIZATION:\")\n",
    "            print(f\"S·ªë t·ª´ sau t√°ch: {len(pos_result['original_tokens'])}\")\n",
    "            print(f\"10 t·ª´ ƒë·∫ßu: {pos_result['original_tokens'][:10]}\")\n",
    "            \n",
    "            print(f\"\\nüè∑Ô∏è  POS TAGGING (s·ª≠ d·ª•ng HMM Model):\")\n",
    "            print(f\"{'STT':<5} {'T·ª´':<30} {'POS Tag':<10}\")\n",
    "            print(\"-\" * 50)\n",
    "            for i, (word, pos) in enumerate(pos_result['word_pos_pairs'][:20], 1):\n",
    "                print(f\"{i:<5} {word:<30} {pos:<10}\")\n",
    "            \n",
    "            print(f\"\\nüå± STEMMING (Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng):\")\n",
    "            print(f\"S·ªë t·ª´ sau stemming: {len(pos_result['stemmed_tokens'])}\")\n",
    "            print(f\"T·ª´ ƒë√£ stemming: {pos_result['stemmed_tokens'][:30]}\")\n",
    "            \n",
    "            # Th·ªëng k√™ POS tags\n",
    "            from collections import Counter\n",
    "            pos_counter = Counter(pos_result['pos_tags'])\n",
    "            print(f\"\\nüìä TH·ªêNG K√ä POS TAGS:\")\n",
    "            for pos_tag, count in pos_counter.most_common(10):\n",
    "                print(f\"  {pos_tag}: {count} t·ª´\")\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': str(doc.get('_id')),\n",
    "                'title': doc.get('title', ''),\n",
    "                'basic_result': basic_result,\n",
    "                'pos_result': pos_result,\n",
    "                'pos_statistics': dict(pos_counter)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì analyze_document_with_pos function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609559a9",
   "metadata": {},
   "source": [
    "### Demo: Ch·∫°y ph√¢n t√≠ch POS Tagging v√† Stemming\n",
    "\n",
    "Ch·∫°y cell b√™n d∆∞·ªõi ƒë·ªÉ xem demo v·ªõi 1 document ng·∫´u nhi√™n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4151f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\n",
      "\n",
      "================================================================================\n",
      "üìÑ PH√ÇN T√çCH DOCUMENT\n",
      "================================================================================\n",
      "Title: '·∫¢o t∆∞·ªüng b√≥ng ƒë√° Vi·ªát Nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± VCK U23 ch√¢u √Å'\n",
      "URL: https://vnexpress.net/u23-viet-nam-u23-yemen-u23-viet-nam-thuong-chau-ao-tuong-bong-da-viet-nam-vuon-tam-khi-gianh-ve-du-vck-u23-chau-a-4937349.html\n",
      "Date: Th·ª© t∆∞, 10/9/2025, 16:30 (GMT+7)\n",
      "\n",
      "üìù Text m·∫´u (1000 k√Ω t·ª± ƒë·∫ßu):\n",
      "·∫£o t∆∞·ªüng b√≥ng ƒë√° vi·ªát nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± vck u23 ch√¢u √° u23 vi·ªát nam v·ª´a gi√†nh v√© d·ª± v√≤ng chung k·∫øt u23 ch√¢u √° 2026 sau tr·∫≠n th·∫Øng 1 0 tr∆∞·ªõc yemen ƒë√°nh d·∫•u l·∫ßn th·ª© s√°u li√™n ti·∫øp g√≥p m·∫∑t ·ªü ƒë·∫•u tr∆∞·ªùng ch√¢u l·ª•c n√†y ng∆∞·ªùi h√¢m m·ªô v·ª° √≤a truy·ªÅn th√¥ng r·ªôn r√†ng v·ªõi nh·ªØng l·ªùi tung h√¥ chi·∫øn th·∫Øng x·ª©ng ƒë√°ng t·∫•m v√© ng·ªçt ng√†o hay kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø nh∆∞ng khi nh√¨n l·∫°i m·ªôt c√°ch t·ªânh t√°o t√¥i cho r·∫±ng s·ª± hi·ªán di·ªán li√™n t·ª•c c·ªßa u23 vi·ªát nam ·ªü vck ch√¢u √° kh√¥ng ph·∫£n √°nh m·ªôt n·ªÅn b√≥ng ƒë√° ƒëang l·ªõn m·∫°nh ƒë√≥ nhi·ªÅu ph·∫ßn l√† nh·ªù may m·∫Øn tr∆∞·ªõc h·∫øt c·∫ßn n√≥i r√µ vi·ªác gi√†nh v√© d·ª± vck u23 ch√¢u √° kh√¥ng ƒë·ªìng nghƒ©a v·ªõi vi·ªác ƒë·ªôi tuy·ªÉn ƒë√£ ƒë·∫°t t·∫ßm ch√¢u l·ª•c th·ªÉ th·ª©c v√≤ng lo·∫°i hi·ªán t·∫°i c·ªßa afc gi√∫p c√°c ƒë·ªôi ƒë√¥ng nam √° trong ƒë√≥ c√≥ vi·ªát nam c√≥ nhi·ªÅu c∆° h·ªôi h∆°n bao gi·ªù h·∫øt ch·ªâ c·∫ßn ƒë·ª©ng nh·∫•t b·∫£ng ho·∫∑c trong s·ªë b·ªën ƒë·ªôi nh√¨ c√≥ th√†nh t√≠ch t·ªët nh·∫•t trong t·ªïng s·ªë g·∫ßn 11 b·∫£ng ƒë·∫•u l√† s·∫Ω ƒë∆∞·ª£c ƒëi ti·∫øp so v·ªõi nh·ªØng n·ªÅn b√≥ng ƒë√° t·∫ßm trung ch√¢u √° nh∆∞ saudi arabia uzbekistan nh·∫≠t b·∫£n hay th·∫≠m ch√≠ l√† qatar vi·ªác ƒë∆∞·ª£c x·∫øp v√†o nh√≥m h·∫°t gi·ªëng s\n",
      "\n",
      "üî§ TOKENIZATION:\n",
      "S·ªë t·ª´ sau t√°ch: 186\n",
      "10 t·ª´ ƒë·∫ßu: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±']\n",
      "\n",
      "üè∑Ô∏è  POS TAGGING (s·ª≠ d·ª•ng HMM Model):\n",
      "STT   T·ª´                             POS Tag   \n",
      "--------------------------------------------------\n",
      "1     ·∫£o_t∆∞·ªüng                       N         \n",
      "2     b√≥ng_ƒë√°                        N         \n",
      "3     vi·ªát                           N         \n",
      "4     nam                            N         \n",
      "5     v∆∞∆°n                           V         \n",
      "6     t·∫ßm                            N         \n",
      "7     khi                            N         \n",
      "8     gi√†nh                          V         \n",
      "9     v√©                             N         \n",
      "10    d·ª±                             V         \n",
      "11    vck                            N         \n",
      "12    u23                            V         \n",
      "13    ch√¢u                           N         \n",
      "14    √°                              V         \n",
      "15    u23                            N         \n",
      "16    vi·ªát_nam                       N         \n",
      "17    v·ª´a                            R         \n",
      "18    gi√†nh                          V         \n",
      "19    v√©                             N         \n",
      "20    d·ª±                             V         \n",
      "\n",
      "üå± STEMMING (Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng):\n",
      "S·ªë t·ª´ sau stemming: 149\n",
      "T·ª´ ƒë√£ stemming: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±', 'vck', 'u23', 'ch√¢u', '√°', 'u23', 'vi·ªát_nam', 'gi√†nh', 'v√©', 'd·ª±', 'v√≤ng', 'chung_k·∫øt', 'u23', 'ch√¢u', '√°', '2026', 'tr·∫≠n', 'th·∫Øng', '1', '0', 'yemen']\n",
      "\n",
      "üìä TH·ªêNG K√ä POS TAGS:\n",
      "  N: 75 t·ª´\n",
      "  V: 47 t·ª´\n",
      "  A: 17 t·ª´\n",
      "  E: 13 t·ª´\n",
      "  R: 11 t·ª´\n",
      "  M: 7 t·ª´\n",
      "  P: 5 t·ª´\n",
      "  C: 5 t·ª´\n",
      "  L: 3 t·ª´\n",
      "  Nc: 3 t·ª´\n",
      "\n",
      "üìù Text m·∫´u (1000 k√Ω t·ª± ƒë·∫ßu):\n",
      "·∫£o t∆∞·ªüng b√≥ng ƒë√° vi·ªát nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± vck u23 ch√¢u √° u23 vi·ªát nam v·ª´a gi√†nh v√© d·ª± v√≤ng chung k·∫øt u23 ch√¢u √° 2026 sau tr·∫≠n th·∫Øng 1 0 tr∆∞·ªõc yemen ƒë√°nh d·∫•u l·∫ßn th·ª© s√°u li√™n ti·∫øp g√≥p m·∫∑t ·ªü ƒë·∫•u tr∆∞·ªùng ch√¢u l·ª•c n√†y ng∆∞·ªùi h√¢m m·ªô v·ª° √≤a truy·ªÅn th√¥ng r·ªôn r√†ng v·ªõi nh·ªØng l·ªùi tung h√¥ chi·∫øn th·∫Øng x·ª©ng ƒë√°ng t·∫•m v√© ng·ªçt ng√†o hay kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø nh∆∞ng khi nh√¨n l·∫°i m·ªôt c√°ch t·ªânh t√°o t√¥i cho r·∫±ng s·ª± hi·ªán di·ªán li√™n t·ª•c c·ªßa u23 vi·ªát nam ·ªü vck ch√¢u √° kh√¥ng ph·∫£n √°nh m·ªôt n·ªÅn b√≥ng ƒë√° ƒëang l·ªõn m·∫°nh ƒë√≥ nhi·ªÅu ph·∫ßn l√† nh·ªù may m·∫Øn tr∆∞·ªõc h·∫øt c·∫ßn n√≥i r√µ vi·ªác gi√†nh v√© d·ª± vck u23 ch√¢u √° kh√¥ng ƒë·ªìng nghƒ©a v·ªõi vi·ªác ƒë·ªôi tuy·ªÉn ƒë√£ ƒë·∫°t t·∫ßm ch√¢u l·ª•c th·ªÉ th·ª©c v√≤ng lo·∫°i hi·ªán t·∫°i c·ªßa afc gi√∫p c√°c ƒë·ªôi ƒë√¥ng nam √° trong ƒë√≥ c√≥ vi·ªát nam c√≥ nhi·ªÅu c∆° h·ªôi h∆°n bao gi·ªù h·∫øt ch·ªâ c·∫ßn ƒë·ª©ng nh·∫•t b·∫£ng ho·∫∑c trong s·ªë b·ªën ƒë·ªôi nh√¨ c√≥ th√†nh t√≠ch t·ªët nh·∫•t trong t·ªïng s·ªë g·∫ßn 11 b·∫£ng ƒë·∫•u l√† s·∫Ω ƒë∆∞·ª£c ƒëi ti·∫øp so v·ªõi nh·ªØng n·ªÅn b√≥ng ƒë√° t·∫ßm trung ch√¢u √° nh∆∞ saudi arabia uzbekistan nh·∫≠t b·∫£n hay th·∫≠m ch√≠ l√† qatar vi·ªác ƒë∆∞·ª£c x·∫øp v√†o nh√≥m h·∫°t gi·ªëng s\n",
      "\n",
      "üî§ TOKENIZATION:\n",
      "S·ªë t·ª´ sau t√°ch: 186\n",
      "10 t·ª´ ƒë·∫ßu: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±']\n",
      "\n",
      "üè∑Ô∏è  POS TAGGING (s·ª≠ d·ª•ng HMM Model):\n",
      "STT   T·ª´                             POS Tag   \n",
      "--------------------------------------------------\n",
      "1     ·∫£o_t∆∞·ªüng                       N         \n",
      "2     b√≥ng_ƒë√°                        N         \n",
      "3     vi·ªát                           N         \n",
      "4     nam                            N         \n",
      "5     v∆∞∆°n                           V         \n",
      "6     t·∫ßm                            N         \n",
      "7     khi                            N         \n",
      "8     gi√†nh                          V         \n",
      "9     v√©                             N         \n",
      "10    d·ª±                             V         \n",
      "11    vck                            N         \n",
      "12    u23                            V         \n",
      "13    ch√¢u                           N         \n",
      "14    √°                              V         \n",
      "15    u23                            N         \n",
      "16    vi·ªát_nam                       N         \n",
      "17    v·ª´a                            R         \n",
      "18    gi√†nh                          V         \n",
      "19    v√©                             N         \n",
      "20    d·ª±                             V         \n",
      "\n",
      "üå± STEMMING (Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng):\n",
      "S·ªë t·ª´ sau stemming: 149\n",
      "T·ª´ ƒë√£ stemming: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±', 'vck', 'u23', 'ch√¢u', '√°', 'u23', 'vi·ªát_nam', 'gi√†nh', 'v√©', 'd·ª±', 'v√≤ng', 'chung_k·∫øt', 'u23', 'ch√¢u', '√°', '2026', 'tr·∫≠n', 'th·∫Øng', '1', '0', 'yemen']\n",
      "\n",
      "üìä TH·ªêNG K√ä POS TAGS:\n",
      "  N: 75 t·ª´\n",
      "  V: 47 t·ª´\n",
      "  A: 17 t·ª´\n",
      "  E: 13 t·ª´\n",
      "  R: 11 t·ª´\n",
      "  M: 7 t·ª´\n",
      "  P: 5 t·ª´\n",
      "  C: 5 t·ª´\n",
      "  L: 3 t·ª´\n",
      "  Nc: 3 t·ª´\n"
     ]
    }
   ],
   "source": [
    "# Demo: Ph√¢n t√≠ch 1 document v·ªõi POS Tagging v√† Stemming\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt n·ªëi database\n",
    "try:\n",
    "    collection.count_documents({})\n",
    "    print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "except:\n",
    "    print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "    client, db, collection = connect_to_database()\n",
    "\n",
    "# Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "if collection is None:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi database. Vui l√≤ng ki·ªÉm tra MONGO_URI!\")\n",
    "else:\n",
    "    # Ph√¢n t√≠ch document\n",
    "    results = analyze_document_with_pos(collection, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e048f4",
   "metadata": {},
   "source": [
    "### Gi·∫£i th√≠ch c√°c POS Tags ph·ªï bi·∫øn trong ti·∫øng Vi·ªát:\n",
    "\n",
    "- **N** (Noun): Danh t·ª´ - VD: \"b√≥ng_ƒë√°\", \"c·∫ßu_th·ªß\", \"tr·∫≠n_ƒë·∫•u\"\n",
    "- **V** (Verb): ƒê·ªông t·ª´ - VD: \"ghi\", \"ch∆°i\", \"thi_ƒë·∫•u\"\n",
    "- **A** (Adjective): T√≠nh t·ª´ - VD: \"t·ªët\", \"m·∫°nh\", \"xu·∫•t_s·∫Øc\"\n",
    "- **P** (Pronoun): ƒê·∫°i t·ª´ - VD: \"h·ªç\", \"ch√∫ng_t√¥i\", \"anh_·∫•y\"\n",
    "- **R** (Adverb): Ph√≥ t·ª´ - VD: \"r·∫•t\", \"ƒë√£\", \"s·∫Ω\"\n",
    "- **L** (Determiner): H·∫°n ƒë·ªãnh t·ª´ - VD: \"nh·ªØng\", \"c√°c\", \"m·ªói\"\n",
    "- **M** (Numeral): S·ªë t·ª´ - VD: \"m·ªôt\", \"hai\", \"100\"\n",
    "- **E** (Preposition): Gi·ªõi t·ª´ - VD: \"·ªü\", \"t·∫°i\", \"trong\"\n",
    "- **C** (Conjunction): Li√™n t·ª´ - VD: \"v√†\", \"ho·∫∑c\", \"nh∆∞ng\"\n",
    "- **I** (Interjection): Th√°n t·ª´ - VD: \"·ªì\", \"√†\", \"√¥i\"\n",
    "\n",
    "**HMM (Hidden Markov Model)** trong PyVi ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n POS tags d·ª±a tr√™n:\n",
    "1. X√°c su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ v·ªõi tag (Emission probability)\n",
    "2. X√°c su·∫•t chuy·ªÉn ƒë·ªïi gi·ªØa c√°c tags (Transition probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2263df2",
   "metadata": {},
   "source": [
    "### Quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n ƒë·∫ßy ƒë·ªß:\n",
    "\n",
    "**C√°c b∆∞·ªõc x·ª≠ l√Ω text theo th·ª© t·ª±:**\n",
    "\n",
    "1. **Cleaning** - L√†m s·∫°ch text (lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát, chu·∫©n h√≥a)\n",
    "2. **Tokenization** - T√°ch t·ª´ (d√πng ViTokenizer)\n",
    "3. **Stopwords Removal** - Lo·∫°i b·ªè t·ª´ d·ª´ng (c√°c t·ª´ kh√¥ng mang √Ω nghƒ©a nh∆∞ \"v√†\", \"c·ªßa\", \"trong\"...)\n",
    "4. **POS Tagging** - G·∫Øn nh√£n t·ª´ lo·∫°i (d√πng HMM)\n",
    "5. **Stemming/Lemmatization** - Chu·∫©n h√≥a t·ª´ v·ªÅ d·∫°ng g·ªëc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b48ee",
   "metadata": {},
   "source": [
    "## 6.6. Text Preprocessing N√¢ng Cao\n",
    "\n",
    "### C√°c b∆∞·ªõc b·ªï sung:\n",
    "1. **Normalization** - Chu·∫©n h√≥a s·ªë, ng√†y th√°ng, t·ª´ vi·∫øt t·∫Øt\n",
    "2. **Spell Correction** - S·ª≠a l·ªói ch√≠nh t·∫£\n",
    "3. **Advanced Word Segmentation** - X·ª≠ l√Ω t·ª´ gh√©p ph·ª©c t·∫°p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c003e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì AdvancedTextNormalizer class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "class AdvancedTextNormalizer:\n",
    "    \"\"\"Class x·ª≠ l√Ω chu·∫©n h√≥a vƒÉn b·∫£n n√¢ng cao\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Chu·∫©n h√≥a s·ªë ti·∫øng Vi·ªát\n",
    "        self.number_mappings = {\n",
    "            'kh√¥ng': '0', 'm·ªôt': '1', 'hai': '2', 'ba': '3', 'b·ªën': '4',\n",
    "            'nƒÉm': '5', 's√°u': '6', 'b·∫£y': '7', 't√°m': '8', 'ch√≠n': '9',\n",
    "            'm∆∞·ªùi': '10', 'm∆∞∆°i': '0', 'trƒÉm': '00', 'ngh√¨n': '000',\n",
    "            'tri·ªáu': '000000', 't·ª∑': '000000000'\n",
    "        }\n",
    "        \n",
    "        # T·ª´ vi·∫øt t·∫Øt ph·ªï bi·∫øn trong b√≥ng ƒë√°\n",
    "        self.abbreviations = {\n",
    "            'hlv': 'hu·∫•n_luy·ªán_vi√™n',\n",
    "            'gƒë': 'gi√°m_ƒë·ªëc',\n",
    "            'clb': 'c√¢u_l·∫°c_b·ªô',\n",
    "            'vff': 'li√™n_ƒëo√†n_b√≥ng_ƒë√°_vi·ªát_nam',\n",
    "            'aff': 'asean_football_federation',\n",
    "            'uefa': 'union_of_european_football_associations',\n",
    "            'fifa': 'federation_internationale_de_football_association',\n",
    "            'vtv': 'ƒë√†i_truy·ªÅn_h√¨nh_vi·ªát_nam',\n",
    "            'u23': 'under_23',\n",
    "            'u22': 'under_22',\n",
    "            'u19': 'under_19',\n",
    "            'var': 'video_assistant_referee',\n",
    "            'hvl': 'hu·∫•n_luy·ªán_vi√™n',\n",
    "            'ƒëtqg': 'ƒë·ªôi_tuy·ªÉn_qu·ªëc_gia',\n",
    "            'slna': 's√¥ng_lam_ngh·ªá_an',\n",
    "            'hagl': 'ho√†ng_anh_gia_lai',\n",
    "            'tphcm': 'th√†nh_ph·ªë_h·ªì_ch√≠_minh'\n",
    "        }\n",
    "        \n",
    "        # Pattern ng√†y th√°ng\n",
    "        self.date_patterns = [\n",
    "            r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})',  # 01/12/2024\n",
    "            r'ng√†y\\s+(\\d{1,2})\\s+th√°ng\\s+(\\d{1,2})\\s+nƒÉm\\s+(\\d{4})',  # ng√†y 1 th√°ng 12 nƒÉm 2024\n",
    "            r'(\\d{1,2})\\s*[/-]\\s*(\\d{1,2})\\s*[/-]\\s*(\\d{2,4})',  # 1-12-24\n",
    "        ]\n",
    "    \n",
    "    def normalize_numbers(self, text):\n",
    "        \"\"\"Chu·∫©n h√≥a s·ªë t·ª´ ch·ªØ sang s·ªë\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Chu·∫©n h√≥a s·ªë ƒë∆°n gi·∫£n\n",
    "        for word, digit in self.number_mappings.items():\n",
    "            text_lower = text_lower.replace(f' {word} ', f' {digit} ')\n",
    "        \n",
    "        # X·ª≠ l√Ω s·ªë ph·ª©c t·∫°p (VD: \"hai m∆∞∆°i ba\" -> \"23\")\n",
    "        import re\n",
    "        \n",
    "        # Pattern: [s·ªë ch·ª•c] m∆∞∆°i [s·ªë ƒë∆°n v·ªã]\n",
    "        pattern = r'(\\w+)\\s+m∆∞∆°i\\s+(\\w+)'\n",
    "        def replace_complex_number(match):\n",
    "            tens = self.number_mappings.get(match.group(1), match.group(1))\n",
    "            ones = self.number_mappings.get(match.group(2), match.group(2))\n",
    "            if tens.isdigit() and ones.isdigit():\n",
    "                return str(int(tens) * 10 + int(ones))\n",
    "            return match.group(0)\n",
    "        \n",
    "        text_lower = re.sub(pattern, replace_complex_number, text_lower)\n",
    "        \n",
    "        return text_lower\n",
    "    \n",
    "    def expand_abbreviations(self, text):\n",
    "        \"\"\"M·ªü r·ªông t·ª´ vi·∫øt t·∫Øt\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for abbr, full in self.abbreviations.items():\n",
    "            # Thay th·∫ø t·ª´ vi·∫øt t·∫Øt (v·ªõi word boundary)\n",
    "            pattern = r'\\b' + abbr + r'\\b'\n",
    "            text_lower = re.sub(pattern, full, text_lower)\n",
    "        \n",
    "        return text_lower\n",
    "    \n",
    "    def normalize_dates(self, text):\n",
    "        \"\"\"Chu·∫©n h√≥a ng√†y th√°ng v·ªÅ format chu·∫©n YYYY-MM-DD\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Pattern 1: dd/mm/yyyy ho·∫∑c dd-mm-yyyy\n",
    "        pattern1 = r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})'\n",
    "        text = re.sub(pattern1, r'\\3-\\2-\\1', text)\n",
    "        \n",
    "        # Pattern 2: ng√†y X th√°ng Y nƒÉm Z\n",
    "        pattern2 = r'ng√†y\\s+(\\d{1,2})\\s+th√°ng\\s+(\\d{1,2})\\s+nƒÉm\\s+(\\d{4})'\n",
    "        text = re.sub(pattern2, r'\\3-\\2-\\1', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_all(self, text):\n",
    "        \"\"\"√Åp d·ª•ng t·∫•t c·∫£ c√°c chu·∫©n h√≥a\"\"\"\n",
    "        text = self.normalize_dates(text)\n",
    "        text = self.expand_abbreviations(text)\n",
    "        text = self.normalize_numbers(text)\n",
    "        return text\n",
    "\n",
    "print(\"‚úì AdvancedTextNormalizer class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46dd9cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VietnameseSpellChecker class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "class VietnameseSpellChecker:\n",
    "    \"\"\"Class ki·ªÉm tra v√† s·ª≠a l·ªói ch√≠nh t·∫£ ti·∫øng Vi·ªát\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_file=None):\n",
    "        # Load vocabulary t·ª´ file ho·∫∑c t·∫°o t·ª´ corpus\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        if vocab_file and os.path.exists(vocab_file):\n",
    "            with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "                self.vocabulary = set(line.strip().lower() for line in f)\n",
    "            print(f\"‚úì ƒê√£ load {len(self.vocabulary)} t·ª´ t·ª´ vocab file\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Ch∆∞a c√≥ vocab file, s·∫Ω t·∫°o t·ª´ corpus\")\n",
    "    \n",
    "    def build_vocabulary_from_collection(self, collection, limit=1000):\n",
    "        \"\"\"X√¢y d·ª±ng vocabulary t·ª´ MongoDB collection\"\"\"\n",
    "        processor = VietnameseTextProcessor()\n",
    "        docs = collection.find().limit(limit)\n",
    "        \n",
    "        print(f\"ƒêang x√¢y d·ª±ng vocabulary t·ª´ {limit} documents...\")\n",
    "        for doc in tqdm(docs, desc=\"Building vocab\", total=limit):\n",
    "            result = processor.process_document(doc)\n",
    "            tokens = result['filtered_tokens']\n",
    "            self.vocabulary.update(tokens)\n",
    "        \n",
    "        print(f\"‚úì Vocabulary c√≥ {len(self.vocabulary)} t·ª´ duy nh·∫•t\")\n",
    "        return self.vocabulary\n",
    "    \n",
    "    def save_vocabulary(self, filename=\"vocab.txt\"):\n",
    "        \"\"\"L∆∞u vocabulary ra file\"\"\"\n",
    "        filepath = os.path.join(os.getcwd(), filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for word in sorted(self.vocabulary):\n",
    "                f.write(f\"{word}\\n\")\n",
    "        print(f\"‚úì ƒê√£ l∆∞u vocabulary v√†o {filepath}\")\n",
    "    \n",
    "    def edit_distance(self, s1, s2):\n",
    "        \"\"\"T√≠nh Levenshtein distance gi·ªØa 2 chu·ªói\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self.edit_distance(s2, s1)\n",
    "        \n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "        \n",
    "        return previous_row[-1]\n",
    "    \n",
    "    def suggest_corrections(self, word, max_distance=2, max_suggestions=5):\n",
    "        \"\"\"G·ª£i √Ω t·ª´ s·ª≠a l·ªói ch√≠nh t·∫£\"\"\"\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        # N·∫øu t·ª´ ƒë√£ ƒë√∫ng\n",
    "        if word_lower in self.vocabulary:\n",
    "            return [word_lower]\n",
    "        \n",
    "        # T√¨m t·ª´ g·∫ßn nh·∫•t\n",
    "        suggestions = []\n",
    "        for vocab_word in self.vocabulary:\n",
    "            distance = self.edit_distance(word_lower, vocab_word)\n",
    "            if distance <= max_distance:\n",
    "                suggestions.append((vocab_word, distance))\n",
    "        \n",
    "        # S·∫Øp x·∫øp theo kho·∫£ng c√°ch\n",
    "        suggestions.sort(key=lambda x: x[1])\n",
    "        \n",
    "        return [word for word, _ in suggestions[:max_suggestions]]\n",
    "    \n",
    "    def correct_text(self, text, auto_correct=False):\n",
    "        \"\"\"Ki·ªÉm tra v√† s·ª≠a l·ªói ch√≠nh t·∫£ cho text\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        corrected_tokens = []\n",
    "        errors_found = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.vocabulary:\n",
    "                corrected_tokens.append(token)\n",
    "            else:\n",
    "                suggestions = self.suggest_corrections(token, max_distance=2, max_suggestions=3)\n",
    "                \n",
    "                if suggestions:\n",
    "                    if auto_correct:\n",
    "                        # T·ª± ƒë·ªông ch·ªçn t·ª´ ƒë·∫ßu ti√™n\n",
    "                        corrected_tokens.append(suggestions[0])\n",
    "                        errors_found.append({\n",
    "                            'original': token,\n",
    "                            'corrected': suggestions[0],\n",
    "                            'suggestions': suggestions\n",
    "                        })\n",
    "                    else:\n",
    "                        corrected_tokens.append(token)\n",
    "                        errors_found.append({\n",
    "                            'original': token,\n",
    "                            'suggestions': suggestions\n",
    "                        })\n",
    "                else:\n",
    "                    corrected_tokens.append(token)\n",
    "        \n",
    "        return {\n",
    "            'original': text,\n",
    "            'corrected': ' '.join(corrected_tokens),\n",
    "            'errors': errors_found\n",
    "        }\n",
    "\n",
    "print(\"‚úì VietnameseSpellChecker class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65793e",
   "metadata": {},
   "source": [
    "### Demo: Chu·∫©n h√≥a n√¢ng cao\n",
    "\n",
    "Test c√°c t√≠nh nƒÉng normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c232757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ DEMO CHU·∫®N H√ìA N√ÇNG CAO\n",
      "================================================================================\n",
      "\n",
      "1. Text g·ªëc:\n",
      "   HLV Park Hang-seo d·∫´n d·∫Øt ƒë·ªôi U23 Vi·ªát Nam\n",
      "   Sau expand abbreviations:\n",
      "   hu·∫•n_luy·ªán_vi√™n park hang-seo d·∫´n d·∫Øt ƒë·ªôi under_23 vi·ªát nam\n",
      "   Sau normalize dates:\n",
      "   hu·∫•n_luy·ªán_vi√™n park hang-seo d·∫´n d·∫Øt ƒë·ªôi under_23 vi·ªát nam\n",
      "   Sau normalize numbers:\n",
      "   hu·∫•n_luy·ªán_vi√™n park hang-seo d·∫´n d·∫Øt ƒë·ªôi under_23 vi·ªát nam\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Text g·ªëc:\n",
      "   Tr·∫≠n ƒë·∫•u di·ªÖn ra v√†o ng√†y 15 th√°ng 10 nƒÉm 2024\n",
      "   Sau expand abbreviations:\n",
      "   tr·∫≠n ƒë·∫•u di·ªÖn ra v√†o ng√†y 15 th√°ng 10 nƒÉm 2024\n",
      "   Sau normalize dates:\n",
      "   tr·∫≠n ƒë·∫•u di·ªÖn ra v√†o 2024-10-15\n",
      "   Sau normalize numbers:\n",
      "   tr·∫≠n ƒë·∫•u di·ªÖn ra v√†o 2024-10-15\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Text g·ªëc:\n",
      "   CLB HAGL th·∫Øng SLNA v·ªõi t·ª∑ s·ªë ba kh√¥ng\n",
      "   Sau expand abbreviations:\n",
      "   c√¢u_l·∫°c_b·ªô ho√†ng_anh_gia_lai th·∫Øng s√¥ng_lam_ngh·ªá_an v·ªõi t·ª∑ s·ªë ba kh√¥ng\n",
      "   Sau normalize dates:\n",
      "   c√¢u_l·∫°c_b·ªô ho√†ng_anh_gia_lai th·∫Øng s√¥ng_lam_ngh·ªá_an v·ªõi t·ª∑ s·ªë ba kh√¥ng\n",
      "   Sau normalize numbers:\n",
      "   c√¢u_l·∫°c_b·ªô ho√†ng_anh_gia_lai th·∫Øng s√¥ng_lam_ngh·ªá_an v·ªõi 000000000 s·ªë 3 kh√¥ng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Text g·ªëc:\n",
      "   C·∫ßu th·ªß ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng\n",
      "   Sau expand abbreviations:\n",
      "   c·∫ßu th·ªß ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng\n",
      "   Sau normalize dates:\n",
      "   c·∫ßu th·ªß ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng\n",
      "   Sau normalize numbers:\n",
      "   c·∫ßu th·ªß ghi ƒë∆∞·ª£c 2 0 3 b√†n th·∫Øng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Text g·ªëc:\n",
      "   VFF c√¥ng b·ªë danh s√°ch ƒë·ªôi tuy·ªÉn v√†o 01/12/2024\n",
      "   Sau expand abbreviations:\n",
      "   li√™n_ƒëo√†n_b√≥ng_ƒë√°_vi·ªát_nam c√¥ng b·ªë danh s√°ch ƒë·ªôi tuy·ªÉn v√†o 01/12/2024\n",
      "   Sau normalize dates:\n",
      "   li√™n_ƒëo√†n_b√≥ng_ƒë√°_vi·ªát_nam c√¥ng b·ªë danh s√°ch ƒë·ªôi tuy·ªÉn v√†o 2024-12-01\n",
      "   Sau normalize numbers:\n",
      "   li√™n_ƒëo√†n_b√≥ng_ƒë√°_vi·ªát_nam c√¥ng b·ªë danh s√°ch ƒë·ªôi tuy·ªÉn v√†o 2024-12-01\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Demo: Chu·∫©n h√≥a n√¢ng cao\n",
    "normalizer = AdvancedTextNormalizer()\n",
    "\n",
    "# Test cases\n",
    "test_texts = [\n",
    "    \"HLV Park Hang-seo d·∫´n d·∫Øt ƒë·ªôi U23 Vi·ªát Nam\",\n",
    "    \"Tr·∫≠n ƒë·∫•u di·ªÖn ra v√†o ng√†y 15 th√°ng 10 nƒÉm 2024\",\n",
    "    \"CLB HAGL th·∫Øng SLNA v·ªõi t·ª∑ s·ªë ba kh√¥ng\",\n",
    "    \"C·∫ßu th·ªß ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng\",\n",
    "    \"VFF c√¥ng b·ªë danh s√°ch ƒë·ªôi tuy·ªÉn v√†o 01/12/2024\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ DEMO CHU·∫®N H√ìA N√ÇNG CAO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n{i}. Text g·ªëc:\")\n",
    "    print(f\"   {text}\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt\n",
    "    abbr_normalized = normalizer.expand_abbreviations(text)\n",
    "    print(f\"   Sau expand abbreviations:\")\n",
    "    print(f\"   {abbr_normalized}\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a ng√†y th√°ng\n",
    "    date_normalized = normalizer.normalize_dates(abbr_normalized)\n",
    "    print(f\"   Sau normalize dates:\")\n",
    "    print(f\"   {date_normalized}\")\n",
    "    \n",
    "    # Chu·∫©n h√≥a s·ªë\n",
    "    number_normalized = normalizer.normalize_numbers(date_normalized)\n",
    "    print(f\"   Sau normalize numbers:\")\n",
    "    print(f\"   {number_normalized}\")\n",
    "    \n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d93cb",
   "metadata": {},
   "source": [
    "### Demo: Spell Checker\n",
    "\n",
    "X√¢y d·ª±ng vocabulary v√† ki·ªÉm tra l·ªói ch√≠nh t·∫£:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9e472d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ t√¨m th·∫•y file vocab.txt\n",
      "‚úì ƒê√£ load 171 t·ª´ t·ª´ vocab file\n",
      "\n",
      "================================================================================\n",
      "üîç DEMO SPELL CHECKER\n",
      "================================================================================\n",
      "\n",
      "üìù Text: b√≥ng ƒë√° vi·ªát nam\n",
      "‚ö†Ô∏è T√¨m th·∫•y 1 l·ªói ti·ªÅm nƒÉng:\n",
      "   - 'ƒë√°' ‚Üí G·ª£i √Ω: ['']\n",
      "\n",
      "üìù Text: c·∫ßu th·ªß thi ƒë·∫•u\n",
      "‚úì Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£\n",
      "\n",
      "üìù Text: hu·∫•n luy·ªán vi√™n\n",
      "‚úì Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£\n",
      "\n",
      "üìù Text: tr·∫≠n ƒë·∫•u quan tr·ªçng\n",
      "‚úì Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£\n"
     ]
    }
   ],
   "source": [
    "# Demo: Spell Checker\n",
    "\n",
    "# Ki·ªÉm tra xem ƒë√£ c√≥ vocab file ch∆∞a\n",
    "vocab_file = \"vocab.txt\"\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    print(f\"‚úì ƒê√£ t√¨m th·∫•y file {vocab_file}\")\n",
    "    spell_checker = VietnameseSpellChecker(vocab_file)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Ch∆∞a c√≥ file {vocab_file}, ƒëang t·∫°o vocabulary t·ª´ database...\")\n",
    "    \n",
    "    # K·∫øt n·ªëi database\n",
    "    try:\n",
    "        collection.count_documents({})\n",
    "        print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "    except:\n",
    "        print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "        client, db, collection = connect_to_database()\n",
    "    \n",
    "    if collection is not None:\n",
    "        # T·∫°o spell checker v√† build vocabulary\n",
    "        spell_checker = VietnameseSpellChecker()\n",
    "        spell_checker.build_vocabulary_from_collection(collection, limit=1000)\n",
    "        spell_checker.save_vocabulary(vocab_file)\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database ƒë·ªÉ build vocabulary!\")\n",
    "        spell_checker = None\n",
    "\n",
    "# Test spell checker\n",
    "if spell_checker and len(spell_checker.vocabulary) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç DEMO SPELL CHECKER\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test cases v·ªõi l·ªói ch√≠nh t·∫£\n",
    "    test_cases = [\n",
    "        \"b√≥ng ƒë√° vi·ªát nam\",\n",
    "        \"c·∫ßu th·ªß thi ƒë·∫•u\",\n",
    "        \"hu·∫•n luy·ªán vi√™n\", \n",
    "        \"tr·∫≠n ƒë·∫•u quan tr·ªçng\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_cases:\n",
    "        print(f\"\\nüìù Text: {text}\")\n",
    "        result = spell_checker.correct_text(text, auto_correct=False)\n",
    "        \n",
    "        if result['errors']:\n",
    "            print(f\"‚ö†Ô∏è T√¨m th·∫•y {len(result['errors'])} l·ªói ti·ªÅm nƒÉng:\")\n",
    "            for error in result['errors']:\n",
    "                print(f\"   - '{error['original']}' ‚Üí G·ª£i √Ω: {error['suggestions']}\")\n",
    "        else:\n",
    "            print(f\"‚úì Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Spell checker ch∆∞a s·∫µn s√†ng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f8df4",
   "metadata": {},
   "source": [
    "### Quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n HO√ÄN CH·ªàNH (v·ªõi c√°c b∆∞·ªõc n√¢ng cao)\n",
    "\n",
    "K·∫øt h·ª£p t·∫•t c·∫£ c√°c b∆∞·ªõc c∆° b·∫£n + n√¢ng cao:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8948ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì complete_advanced_pipeline function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def complete_advanced_pipeline(text, processor, normalizer=None, spell_checker=None):\n",
    "    \"\"\"\n",
    "    Quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n HO√ÄN CH·ªàNH:\n",
    "    \n",
    "    B∆Ø·ªöC C∆† B·∫¢N:\n",
    "    1. Normalization (n√¢ng cao)\n",
    "    2. Spell Correction (optional)\n",
    "    3. Cleaning\n",
    "    4. Tokenization\n",
    "    5. Stopwords Removal\n",
    "    6. POS Tagging (HMM)\n",
    "    7. Stemming\n",
    "    \"\"\"\n",
    "    print(\"üîÑ QUY TR√åNH X·ª¨ L√ù VƒÇN B·∫¢N HO√ÄN CH·ªàNH (C∆† B·∫¢N + N√ÇNG CAO)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    original_text = text\n",
    "    \n",
    "    # ============= PH·∫¶N N√ÇNG CAO =============\n",
    "    \n",
    "    # B∆∞·ªõc 0.1: ADVANCED NORMALIZATION\n",
    "    if normalizer:\n",
    "        print(\"\\nüìç B∆Ø·ªöC 0.1: ADVANCED NORMALIZATION\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Text g·ªëc: {text[:200]}...\")\n",
    "        \n",
    "        text = normalizer.normalize_all(text)\n",
    "        print(f\"Sau normalization: {text[:200]}...\")\n",
    "    \n",
    "    # B∆∞·ªõc 0.2: SPELL CORRECTION (optional)\n",
    "    if spell_checker and len(spell_checker.vocabulary) > 0:\n",
    "        print(\"\\nüìç B∆Ø·ªöC 0.2: SPELL CORRECTION\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        result = spell_checker.correct_text(text, auto_correct=True)\n",
    "        if result['errors']:\n",
    "            print(f\"T√¨m th·∫•y v√† s·ª≠a {len(result['errors'])} l·ªói:\")\n",
    "            for error in result['errors'][:5]:  # Hi·ªÉn th·ªã 5 l·ªói ƒë·∫ßu\n",
    "                print(f\"  '{error['original']}' ‚Üí '{error['corrected']}'\")\n",
    "            text = result['corrected']\n",
    "        else:\n",
    "            print(\"‚úì Kh√¥ng t√¨m th·∫•y l·ªói ch√≠nh t·∫£\")\n",
    "    \n",
    "    # ============= PH·∫¶N C∆† B·∫¢N =============\n",
    "    \n",
    "    # B∆∞·ªõc 1: CLEANING\n",
    "    print(\"\\nüìç B∆Ø·ªöC 1: CLEANING\")\n",
    "    print(\"-\"*80)\n",
    "    cleaned = processor.clean_text(text)\n",
    "    print(f\"Sau cleaning: {cleaned[:200]}...\")\n",
    "    \n",
    "    # B∆∞·ªõc 2: TOKENIZATION\n",
    "    print(\"\\nüìç B∆Ø·ªöC 2: TOKENIZATION\")\n",
    "    print(\"-\"*80)\n",
    "    tokens = processor.tokenize_vietnamese(cleaned)\n",
    "    print(f\"S·ªë tokens: {len(tokens)}\")\n",
    "    print(f\"20 tokens ƒë·∫ßu: {tokens[:20]}\")\n",
    "    \n",
    "    # B∆∞·ªõc 3: STOPWORDS REMOVAL\n",
    "    print(\"\\nüìç B∆Ø·ªöC 3: STOPWORDS REMOVAL\")\n",
    "    print(\"-\"*80)\n",
    "    filtered_tokens = processor.remove_stopwords(tokens)\n",
    "    removed = len(tokens) - len(filtered_tokens)\n",
    "    print(f\"Lo·∫°i b·ªè {removed} stopwords ({removed/len(tokens)*100:.1f}%)\")\n",
    "    print(f\"C√≤n l·∫°i: {len(filtered_tokens)} tokens\")\n",
    "    \n",
    "    # B∆∞·ªõc 4: POS TAGGING (HMM)\n",
    "    print(\"\\nüìç B∆Ø·ªöC 4: POS TAGGING - HMM\")\n",
    "    print(\"-\"*80)\n",
    "    if PYVI_AVAILABLE:\n",
    "        pos_result = ViPosTagger.postagging(ViTokenizer.tokenize(cleaned))\n",
    "        words, pos_tags = pos_result[0], pos_result[1]\n",
    "        \n",
    "        from collections import Counter\n",
    "        pos_counter = Counter(pos_tags)\n",
    "        print(f\"Top 5 POS tags:\")\n",
    "        for tag, count in pos_counter.most_common(5):\n",
    "            print(f\"  {tag}: {count}\")\n",
    "    else:\n",
    "        words, pos_tags = tokens, []\n",
    "    \n",
    "    # B∆∞·ªõc 5: STEMMING\n",
    "    print(\"\\nüìç B∆Ø·ªöC 5: STEMMING\")\n",
    "    print(\"-\"*80)\n",
    "    if PYVI_AVAILABLE and len(pos_tags) > 0:\n",
    "        important_pos = ['N', 'V', 'A', 'M']\n",
    "        stemmed = [w for w, p in zip(words, pos_tags) if any(p.startswith(x) for x in important_pos)]\n",
    "        print(f\"Gi·ªØ l·∫°i: {len(stemmed)}/{len(words)} t·ª´ quan tr·ªçng\")\n",
    "    else:\n",
    "        stemmed = filtered_tokens\n",
    "    \n",
    "    # K·∫æT QU·∫¢\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ HO√ÄN TH√ÄNH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä Th·ªëng k√™:\")\n",
    "    print(f\"  ‚Ä¢ Text g·ªëc: {len(original_text)} k√Ω t·ª±\")\n",
    "    print(f\"  ‚Ä¢ Sau normalization: {len(text)} k√Ω t·ª±\")\n",
    "    print(f\"  ‚Ä¢ Tokens: {len(tokens)}\")\n",
    "    print(f\"  ‚Ä¢ Sau filter: {len(filtered_tokens)}\")\n",
    "    print(f\"  ‚Ä¢ Sau stemming: {len(stemmed)}\")\n",
    "    print(f\"  ‚Ä¢ Gi·∫£m: {(1 - len(stemmed)/len(tokens))*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'original': original_text,\n",
    "        'normalized': text,\n",
    "        'cleaned': cleaned,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed,\n",
    "        'pos_tags': pos_tags if PYVI_AVAILABLE else []\n",
    "    }\n",
    "\n",
    "print(\"‚úì complete_advanced_pipeline function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b7ed8",
   "metadata": {},
   "source": [
    "### Demo: Ch·∫°y quy tr√¨nh HO√ÄN CH·ªàNH\n",
    "\n",
    "Test quy tr√¨nh v·ªõi t·∫•t c·∫£ c√°c b∆∞·ªõc n√¢ng cao:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4b6634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ c√≥ processor\n",
      "‚úì ƒê√£ c√≥ normalizer\n",
      "‚úì ƒê√£ c√≥ spell_checker v·ªõi 171 t·ª´\n",
      "\n",
      "üìÑ Test Text:\n",
      "\n",
      "HLV Park Hang-seo ƒë√£ d·∫´n d·∫Øt ƒë·ªôi tuy·ªÉn U23 Vi·ªát Nam \n",
      "thi ƒë·∫•u r·∫•t xu·∫•t s·∫Øc t·∫°i gi·∫£i ƒë·∫•u ng√†y 15 th√°ng 1 nƒÉm 2024.\n",
      "CLB HAGL ƒë√£ th·∫Øng SLNA v·ªõi t·ª∑ s·ªë ba kh√¥ng trong tr·∫≠n ƒë·∫•u v√≤ng nƒÉm V-League.\n",
      "C·∫ßu th·ªß Nguy·ªÖn Quang H·∫£i ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng trong m√πa gi·∫£i n√†y.\n",
      "\n",
      "\n",
      "\n",
      "üîÑ QUY TR√åNH X·ª¨ L√ù VƒÇN B·∫¢N HO√ÄN CH·ªàNH (C∆† B·∫¢N + N√ÇNG CAO)\n",
      "================================================================================\n",
      "\n",
      "üìç B∆Ø·ªöC 0.1: ADVANCED NORMALIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "Text g·ªëc: \n",
      "HLV Park Hang-seo ƒë√£ d·∫´n d·∫Øt ƒë·ªôi tuy·ªÉn U23 Vi·ªát Nam \n",
      "thi ƒë·∫•u r·∫•t xu·∫•t s·∫Øc t·∫°i gi·∫£i ƒë·∫•u ng√†y 15 th√°ng 1 nƒÉm 2024.\n",
      "CLB HAGL ƒë√£ th·∫Øng SLNA v·ªõi t·ª∑ s·ªë ba kh√¥ng trong tr·∫≠n ƒë·∫•u v√≤ng nƒÉm V-League.\n",
      "C·∫ßu th·ªß Ng...\n",
      "Sau normalization: \n",
      "hu·∫•n_luy·ªán_vi√™n park hang-seo ƒë√£ d·∫´n d·∫Øt ƒë·ªôi tuy·ªÉn under_23 vi·ªát nam \n",
      "thi ƒë·∫•u r·∫•t xu·∫•t s·∫Øc t·∫°i gi·∫£i ƒë·∫•u 2024-1-15.\n",
      "c√¢u_l·∫°c_b·ªô ho√†ng_anh_gia_lai ƒë√£ th·∫Øng s√¥ng_lam_ngh·ªá_an v·ªõi 000000000 s·ªë 3 0 trong tr...\n",
      "\n",
      "üìç B∆Ø·ªöC 0.2: SPELL CORRECTION\n",
      "--------------------------------------------------------------------------------\n",
      "T√¨m th·∫•y v√† s·ª≠a 9 l·ªói:\n",
      "  'ƒë√£' ‚Üí ''\n",
      "  'ƒë√£' ‚Üí ''\n",
      "  's·ªë' ‚Üí ''\n",
      "  '3' ‚Üí ''\n",
      "  '0' ‚Üí ''\n",
      "\n",
      "üìç B∆Ø·ªöC 1: CLEANING\n",
      "--------------------------------------------------------------------------------\n",
      "Sau cleaning: hu·∫•n_luy·ªán_vi√™n park hang seo d·∫´n d·∫Øt ƒë·ªôi tuy·ªÉn under_23 vi·ªát nam thi ƒë·∫•u r·∫•t xu·∫•t s·∫Øc t·∫°i gi·∫£i ƒë·∫•u 2024 1 15 c√¢u_l·∫°c_b·ªô ho√†ng_anh_gia_lai th·∫Øng s√¥ng_lam_ngh·ªá_an v·ªõi 000000000 trong tr·∫≠n ƒë·∫•u v√≤ng v le...\n",
      "\n",
      "üìç B∆Ø·ªöC 2: TOKENIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "S·ªë tokens: 40\n",
      "20 tokens ƒë·∫ßu: ['hu·∫•n_luy·ªán_vi√™n', 'park', 'hang', 'seo', 'd·∫´n_d·∫Øt', 'ƒë·ªôi_tuy·ªÉn', 'under_23', 'vi·ªát_nam', 'thi_ƒë·∫•u', 'r·∫•t', 'xu·∫•t_s·∫Øc', 't·∫°i', 'gi·∫£i_ƒë·∫•u', '2024', '1', '15', 'c√¢u_l·∫°c_b·ªô', 'ho√†ng_anh_gia_lai', 'th·∫Øng', 's√¥ng_lam_ngh·ªá_an']\n",
      "\n",
      "üìç B∆Ø·ªöC 3: STOPWORDS REMOVAL\n",
      "--------------------------------------------------------------------------------\n",
      "Lo·∫°i b·ªè 8 stopwords (20.0%)\n",
      "C√≤n l·∫°i: 32 tokens\n",
      "\n",
      "üìç B∆Ø·ªöC 4: POS TAGGING - HMM\n",
      "--------------------------------------------------------------------------------\n",
      "Top 5 POS tags:\n",
      "  N: 17\n",
      "  V: 10\n",
      "  E: 4\n",
      "  M: 4\n",
      "  R: 2\n",
      "\n",
      "üìç B∆Ø·ªöC 5: STEMMING\n",
      "--------------------------------------------------------------------------------\n",
      "Gi·ªØ l·∫°i: 33/40 t·ª´ quan tr·ªçng\n",
      "\n",
      "================================================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH\n",
      "================================================================================\n",
      "üìä Th·ªëng k√™:\n",
      "  ‚Ä¢ Text g·ªëc: 266 k√Ω t·ª±\n",
      "  ‚Ä¢ Sau normalization: 279 k√Ω t·ª±\n",
      "  ‚Ä¢ Tokens: 40\n",
      "  ‚Ä¢ Sau filter: 32\n",
      "  ‚Ä¢ Sau stemming: 33\n",
      "  ‚Ä¢ Gi·∫£m: 17.5%\n"
     ]
    }
   ],
   "source": [
    "# Demo: Quy tr√¨nh HO√ÄN CH·ªàNH\n",
    "\n",
    "# Kh·ªüi t·∫°o c√°c components\n",
    "try:\n",
    "    processor\n",
    "    print(\"‚úì ƒê√£ c√≥ processor\")\n",
    "except:\n",
    "    processor = VietnameseTextProcessor()\n",
    "    print(\"‚úì ƒê√£ kh·ªüi t·∫°o processor\")\n",
    "\n",
    "try:\n",
    "    normalizer\n",
    "    print(\"‚úì ƒê√£ c√≥ normalizer\")\n",
    "except:\n",
    "    normalizer = AdvancedTextNormalizer()\n",
    "    print(\"‚úì ƒê√£ kh·ªüi t·∫°o normalizer\")\n",
    "\n",
    "# Spell checker (optional)\n",
    "try:\n",
    "    spell_checker\n",
    "    if spell_checker and len(spell_checker.vocabulary) > 0:\n",
    "        print(f\"‚úì ƒê√£ c√≥ spell_checker v·ªõi {len(spell_checker.vocabulary)} t·ª´\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Spell checker ch∆∞a c√≥ vocabulary, s·∫Ω b·ªè qua b∆∞·ªõc n√†y\")\n",
    "        spell_checker = None\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Spell checker ch∆∞a kh·ªüi t·∫°o, s·∫Ω b·ªè qua b∆∞·ªõc n√†y\")\n",
    "    spell_checker = None\n",
    "\n",
    "# Test text\n",
    "test_text = \"\"\"\n",
    "HLV Park Hang-seo ƒë√£ d·∫´n d·∫Øt ƒë·ªôi tuy·ªÉn U23 Vi·ªát Nam \n",
    "thi ƒë·∫•u r·∫•t xu·∫•t s·∫Øc t·∫°i gi·∫£i ƒë·∫•u ng√†y 15 th√°ng 1 nƒÉm 2024.\n",
    "CLB HAGL ƒë√£ th·∫Øng SLNA v·ªõi t·ª∑ s·ªë ba kh√¥ng trong tr·∫≠n ƒë·∫•u v√≤ng nƒÉm V-League.\n",
    "C·∫ßu th·ªß Nguy·ªÖn Quang H·∫£i ghi ƒë∆∞·ª£c hai m∆∞∆°i ba b√†n th·∫Øng trong m√πa gi·∫£i n√†y.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìÑ Test Text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ch·∫°y quy tr√¨nh ho√†n ch·ªânh\n",
    "result = complete_advanced_pipeline(\n",
    "    test_text[:500],  # Gi·ªõi h·∫°n ƒë·ªÉ demo\n",
    "    processor=processor,\n",
    "    normalizer=normalizer,\n",
    "    spell_checker=spell_checker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a827bd",
   "metadata": {},
   "source": [
    "### L∆∞u k·∫øt qu·∫£ preprocessing v√†o file\n",
    "\n",
    "C√°c function ƒë·ªÉ l∆∞u output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa03d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessing_results(results, output_dir=\"d:\\\\data\\\\Search_Engine\\\\outputs\", prefix=\"preprocessing\"):\n",
    "    \"\"\"\n",
    "    L∆∞u k·∫øt qu·∫£ preprocessing v√†o c√°c file kh√°c nhau\n",
    "    \n",
    "    Args:\n",
    "        results: dict ch·ª©a k·∫øt qu·∫£ preprocessing\n",
    "        output_dir: th∆∞ m·ª•c l∆∞u output\n",
    "        prefix: ti·ªÅn t·ªë t√™n file\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # T·∫°o folder outputs\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 1. L∆∞u summary d·∫°ng JSON\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'original_length': len(results['original']),\n",
    "        'normalized_length': len(results.get('normalized', results['original'])),\n",
    "        'cleaned_length': len(results['cleaned']),\n",
    "        'token_count': len(results['tokens']),\n",
    "        'filtered_count': len(results['filtered_tokens']),\n",
    "        'stemmed_count': len(results['stemmed_tokens']),\n",
    "        'reduction_rate': f\"{(1 - len(results['stemmed_tokens'])/len(results['tokens']))*100:.1f}%\"\n",
    "    }\n",
    "    \n",
    "    summary_file = os.path.join(output_dir, f\"{prefix}_summary_{timestamp}.json\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úì ƒê√£ l∆∞u summary v√†o: {summary_file}\")\n",
    "    \n",
    "    # 2. L∆∞u tokens v√†o text file\n",
    "    tokens_file = os.path.join(output_dir, f\"{prefix}_tokens_{timestamp}.txt\")\n",
    "    with open(tokens_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== ALL TOKENS ===\\n\")\n",
    "        f.write(' '.join(results['tokens']))\n",
    "        f.write(\"\\n\\n=== FILTERED TOKENS (sau stopwords removal) ===\\n\")\n",
    "        f.write(' '.join(results['filtered_tokens']))\n",
    "        f.write(\"\\n\\n=== STEMMED TOKENS ===\\n\")\n",
    "        f.write(' '.join(results['stemmed_tokens']))\n",
    "    print(f\"‚úì ƒê√£ l∆∞u tokens v√†o: {tokens_file}\")\n",
    "    \n",
    "    # 3. L∆∞u POS tags n·∫øu c√≥\n",
    "    if results.get('pos_tags') and len(results['pos_tags']) > 0:\n",
    "        pos_file = os.path.join(output_dir, f\"{prefix}_pos_tags_{timestamp}.csv\")\n",
    "        \n",
    "        # T·∫°o dataframe v·ªõi t·ª´ v√† POS tag\n",
    "        if 'normalized' in results:\n",
    "            # N·∫øu c√≥ normalized, c·∫ßn tokenize l·∫°i\n",
    "            if PYVI_AVAILABLE:\n",
    "                from pyvi import ViTokenizer\n",
    "                tokens_for_pos = ViTokenizer.tokenize(results['cleaned']).split()\n",
    "            else:\n",
    "                tokens_for_pos = results['tokens']\n",
    "        else:\n",
    "            tokens_for_pos = results['tokens']\n",
    "        \n",
    "        # T·∫°o DataFrame\n",
    "        pos_data = []\n",
    "        for i, (word, pos) in enumerate(zip(tokens_for_pos, results['pos_tags']), 1):\n",
    "            pos_data.append({\n",
    "                'Index': i,\n",
    "                'Word': word,\n",
    "                'POS_Tag': pos\n",
    "            })\n",
    "        \n",
    "        df_pos = pd.DataFrame(pos_data)\n",
    "        df_pos.to_csv(pos_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úì ƒê√£ l∆∞u POS tags v√†o: {pos_file}\")\n",
    "    \n",
    "    # 4. L∆∞u text ƒë√£ x·ª≠ l√Ω t·ª´ng b∆∞·ªõc\n",
    "    steps_file = os.path.join(output_dir, f\"{prefix}_steps_{timestamp}.txt\")\n",
    "    with open(steps_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"TEXT PREPROCESSING - T·ª™NG B∆Ø·ªöC\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. TEXT G·ªêC:\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(results['original'][:500] + \"...\\n\\n\")\n",
    "        \n",
    "        if 'normalized' in results:\n",
    "            f.write(\"2. SAU NORMALIZATION:\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(results['normalized'][:500] + \"...\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. SAU CLEANING:\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(results['cleaned'][:500] + \"...\\n\\n\")\n",
    "        \n",
    "        f.write(\"4. SAU TOKENIZATION:\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(' '.join(results['tokens'][:100]) + \"...\\n\\n\")\n",
    "        \n",
    "        f.write(\"5. SAU STOPWORDS REMOVAL:\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(' '.join(results['filtered_tokens'][:100]) + \"...\\n\\n\")\n",
    "        \n",
    "        f.write(\"6. SAU STEMMING:\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(' '.join(results['stemmed_tokens'][:100]) + \"...\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úì ƒê√£ l∆∞u c√°c b∆∞·ªõc x·ª≠ l√Ω v√†o: {steps_file}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ T·∫•t c·∫£ output ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o folder: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'summary_file': summary_file,\n",
    "        'tokens_file': tokens_file,\n",
    "        'pos_file': pos_file if results.get('pos_tags') else None,\n",
    "        'steps_file': steps_file\n",
    "    }\n",
    "\n",
    "print(\"‚úì save_preprocessing_results function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12569448",
   "metadata": {},
   "source": [
    "### Demo: L∆∞u k·∫øt qu·∫£ preprocessing\n",
    "\n",
    "Ch·∫°y quy tr√¨nh v√† l∆∞u output v√†o folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: L∆∞u k·∫øt qu·∫£ preprocessing\n",
    "\n",
    "# Ki·ªÉm tra c√°c components\n",
    "try:\n",
    "    collection.count_documents({})\n",
    "    print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "except:\n",
    "    print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "    client, db, collection = connect_to_database()\n",
    "\n",
    "try:\n",
    "    processor\n",
    "except:\n",
    "    processor = VietnameseTextProcessor()\n",
    "\n",
    "try:\n",
    "    normalizer\n",
    "except:\n",
    "    normalizer = AdvancedTextNormalizer()\n",
    "\n",
    "# Spell checker (optional)\n",
    "try:\n",
    "    spell_checker\n",
    "    if not spell_checker or len(spell_checker.vocabulary) == 0:\n",
    "        spell_checker = None\n",
    "except:\n",
    "    spell_checker = None\n",
    "\n",
    "if collection is not None:\n",
    "    # L·∫•y 1 document\n",
    "    doc = collection.find_one()\n",
    "    if doc:\n",
    "        full_text = f\"{doc.get('title', '')} {doc.get('content', '')}\"\n",
    "        \n",
    "        print(f\"\\nüìÑ Processing document: {doc.get('title', 'N/A')[:100]}...\\n\")\n",
    "        \n",
    "        # Ch·∫°y quy tr√¨nh ho√†n ch·ªânh\n",
    "        result = complete_advanced_pipeline(\n",
    "            full_text[:2000],\n",
    "            processor=processor,\n",
    "            normalizer=normalizer,\n",
    "            spell_checker=spell_checker\n",
    "        )\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£ v√†o file\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üíæ ƒêANG L∆ØU K·∫æT QU·∫¢ V√ÄO FILE...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        saved_files = save_preprocessing_results(\n",
    "            result,\n",
    "            output_dir=\"d:\\\\data\\\\Search_Engine\\\\outputs\",\n",
    "            prefix=\"advanced_preprocessing\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ ƒê√£ ho√†n th√†nh v√† l∆∞u t·∫•t c·∫£ output!\")\n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y document!\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3a1dc",
   "metadata": {},
   "source": [
    "### Batch Processing: X·ª≠ l√Ω nhi·ªÅu documents v√† l∆∞u k·∫øt qu·∫£\n",
    "\n",
    "X·ª≠ l√Ω h√†ng lo·∫°t documents v√† l∆∞u th·ªëng k√™ t·ªïng h·ª£p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84781279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_and_save(collection, processor, limit=100, normalizer=None, output_dir=\"d:\\\\data\\\\Search_Engine\\\\outputs\"):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω nhi·ªÅu documents v√† l∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(f\"üîÑ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù BATCH: {limit} DOCUMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # T·∫°o folder\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # L·∫•y documents\n",
    "    docs = collection.find().limit(limit)\n",
    "    \n",
    "    all_results = []\n",
    "    stats = {\n",
    "        'total_docs': 0,\n",
    "        'total_tokens': 0,\n",
    "        'total_filtered': 0,\n",
    "        'total_stemmed': 0,\n",
    "        'avg_reduction_rate': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nƒêang x·ª≠ l√Ω {limit} documents...\")\n",
    "    \n",
    "    for doc in tqdm(docs, desc=\"Processing\", total=limit):\n",
    "        try:\n",
    "            # X·ª≠ l√Ω c∆° b·∫£n\n",
    "            result = processor.process_document(doc)\n",
    "            \n",
    "            # √Åp d·ª•ng normalization n·∫øu c√≥\n",
    "            if normalizer:\n",
    "                cleaned = normalizer.normalize_all(result['cleaned_text'])\n",
    "            else:\n",
    "                cleaned = result['cleaned_text']\n",
    "            \n",
    "            # Th·ªëng k√™\n",
    "            stats['total_docs'] += 1\n",
    "            stats['total_tokens'] += result['token_count']\n",
    "            stats['total_filtered'] += result['filtered_count']\n",
    "            stats['total_stemmed'] += result['filtered_count']  # Simplified\n",
    "            \n",
    "            all_results.append({\n",
    "                'doc_id': result['doc_id'],\n",
    "                'title': result['title'],\n",
    "                'token_count': result['token_count'],\n",
    "                'filtered_count': result['filtered_count'],\n",
    "                'reduction_rate': (1 - result['filtered_count']/result['token_count'])*100 if result['token_count'] > 0 else 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói x·ª≠ l√Ω document: {e}\")\n",
    "    \n",
    "    # T√≠nh trung b√¨nh\n",
    "    if stats['total_docs'] > 0:\n",
    "        stats['avg_tokens_per_doc'] = stats['total_tokens'] / stats['total_docs']\n",
    "        stats['avg_filtered_per_doc'] = stats['total_filtered'] / stats['total_docs']\n",
    "        stats['avg_reduction_rate'] = (1 - stats['total_filtered']/stats['total_tokens'])*100 if stats['total_tokens'] > 0 else 0\n",
    "    \n",
    "    # L∆∞u th·ªëng k√™ t·ªïng h·ª£p\n",
    "    stats_file = os.path.join(output_dir, f\"batch_stats_{timestamp}.json\")\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n‚úì ƒê√£ l∆∞u th·ªëng k√™ t·ªïng h·ª£p: {stats_file}\")\n",
    "    \n",
    "    # L∆∞u chi ti·∫øt t·ª´ng document\n",
    "    details_file = os.path.join(output_dir, f\"batch_details_{timestamp}.csv\")\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(details_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úì ƒê√£ l∆∞u chi ti·∫øt documents: {details_file}\")\n",
    "    \n",
    "    # In th·ªëng k√™\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä TH·ªêNG K√ä T·ªîNG H·ª¢P\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"T·ªïng s·ªë documents: {stats['total_docs']}\")\n",
    "    print(f\"T·ªïng s·ªë tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"Trung b√¨nh tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "    print(f\"Trung b√¨nh filtered/doc: {stats['avg_filtered_per_doc']:.1f}\")\n",
    "    print(f\"T·ª∑ l·ªá gi·∫£m trung b√¨nh: {stats['avg_reduction_rate']:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'results': all_results,\n",
    "        'stats_file': stats_file,\n",
    "        'details_file': details_file\n",
    "    }\n",
    "\n",
    "print(\"‚úì batch_process_and_save function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec6735",
   "metadata": {},
   "source": [
    "### Demo: Batch processing\n",
    "\n",
    "X·ª≠ l√Ω h√†ng lo·∫°t 100 documents (c√≥ th·ªÉ thay ƒë·ªïi s·ªë l∆∞·ª£ng):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Batch Processing 100 documents\n",
    "\n",
    "# Ki·ªÉm tra c√°c components\n",
    "try:\n",
    "    collection.count_documents({})\n",
    "    print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "except:\n",
    "    print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "    client, db, collection = connect_to_database()\n",
    "\n",
    "try:\n",
    "    processor\n",
    "    print(\"‚úì ƒê√£ c√≥ processor\")\n",
    "except:\n",
    "    processor = VietnameseTextProcessor()\n",
    "\n",
    "try:\n",
    "    normalizer\n",
    "    print(\"‚úì ƒê√£ c√≥ normalizer\")\n",
    "except:\n",
    "    normalizer = AdvancedTextNormalizer()\n",
    "\n",
    "if collection is not None:\n",
    "    # Ch·∫°y batch processing\n",
    "    batch_results = batch_process_and_save(\n",
    "        collection=collection,\n",
    "        processor=processor,\n",
    "        limit=100,  # Thay ƒë·ªïi s·ªë l∆∞·ª£ng documents t·∫°i ƒë√¢y\n",
    "        normalizer=normalizer,\n",
    "        output_dir=\"d:\\\\data\\\\Search_Engine\\\\outputs\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ho√†n th√†nh batch processing!\")\n",
    "    print(f\"üìÅ C√°c file ƒë√£ l∆∞u:\")\n",
    "    print(f\"   - Stats: {batch_results['stats_file']}\")\n",
    "    print(f\"   - Details: {batch_results['details_file']}\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64b5b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì complete_text_processing_pipeline function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def complete_text_processing_pipeline(text, processor):\n",
    "    \"\"\"\n",
    "    Quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n ƒê·∫¶Y ƒê·ª¶ theo th·ª© t·ª±:\n",
    "    1. Cleaning\n",
    "    2. Tokenization\n",
    "    3. Stopwords Removal\n",
    "    4. POS Tagging (HMM)\n",
    "    5. Stemming\n",
    "    \"\"\"\n",
    "    print(\"üîÑ B·∫ÆT ƒê·∫¶U QUY TR√åNH X·ª¨ L√ù VƒÇN B·∫¢N\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # B∆∞·ªõc 1: CLEANING\n",
    "    print(\"\\nüìç B∆Ø·ªöC 1: CLEANING (L√†m s·∫°ch vƒÉn b·∫£n)\")\n",
    "    print(\"-\"*80)\n",
    "    cleaned = processor.clean_text(text)\n",
    "    print(f\"Text g·ªëc: {text[:200]}...\")\n",
    "    print(f\"Text sau cleaning: {cleaned[:200]}...\")\n",
    "    print(f\"ƒê·ªô d√†i: {len(text)} ‚Üí {len(cleaned)} k√Ω t·ª±\")\n",
    "    \n",
    "    # B∆∞·ªõc 2: TOKENIZATION\n",
    "    print(\"\\nüìç B∆Ø·ªöC 2: TOKENIZATION (T√°ch t·ª´)\")\n",
    "    print(\"-\"*80)\n",
    "    tokens = processor.tokenize_vietnamese(cleaned)\n",
    "    print(f\"S·ªë t·ª´ sau t√°ch: {len(tokens)}\")\n",
    "    print(f\"20 t·ª´ ƒë·∫ßu ti√™n: {tokens[:20]}\")\n",
    "    \n",
    "    # B∆∞·ªõc 3: STOPWORDS REMOVAL\n",
    "    print(\"\\nüìç B∆Ø·ªöC 3: STOPWORDS REMOVAL (Lo·∫°i b·ªè t·ª´ d·ª´ng)\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Danh s√°ch stopwords ({len(processor.stop_words)} t·ª´):\")\n",
    "    print(list(processor.stop_words)[:20], \"...\")\n",
    "    \n",
    "    filtered_tokens = processor.remove_stopwords(tokens)\n",
    "    removed_count = len(tokens) - len(filtered_tokens)\n",
    "    print(f\"\\nS·ªë t·ª´ tr∆∞·ªõc: {len(tokens)}\")\n",
    "    print(f\"S·ªë t·ª´ sau: {len(filtered_tokens)}\")\n",
    "    print(f\"ƒê√£ lo·∫°i b·ªè: {removed_count} t·ª´ d·ª´ng ({removed_count/len(tokens)*100:.1f}%)\")\n",
    "    print(f\"20 t·ª´ c√≤n l·∫°i: {filtered_tokens[:20]}\")\n",
    "    \n",
    "    # B∆∞·ªõc 4: POS TAGGING (HMM)\n",
    "    print(\"\\nüìç B∆Ø·ªöC 4: POS TAGGING - HMM (G·∫Øn nh√£n t·ª´ lo·∫°i)\")\n",
    "    print(\"-\"*80)\n",
    "    if PYVI_AVAILABLE:\n",
    "        # POS tagging tr√™n text ƒë√£ cleaned (ch∆∞a remove stopwords ƒë·ªÉ ƒë·∫£m b·∫£o ng·ªØ c·∫£nh)\n",
    "        pos_result = ViPosTagger.postagging(ViTokenizer.tokenize(cleaned))\n",
    "        words = pos_result[0]\n",
    "        pos_tags = pos_result[1]\n",
    "        \n",
    "        print(f\"{'STT':<5} {'T·ª´':<25} {'POS Tag':<10}\")\n",
    "        print(\"-\"*45)\n",
    "        for i, (word, pos) in enumerate(zip(words[:15], pos_tags[:15]), 1):\n",
    "            print(f\"{i:<5} {word:<25} {pos:<10}\")\n",
    "        \n",
    "        # Th·ªëng k√™ POS\n",
    "        from collections import Counter\n",
    "        pos_counter = Counter(pos_tags)\n",
    "        print(f\"\\nüìä Th·ªëng k√™ POS Tags:\")\n",
    "        for pos_tag, count in pos_counter.most_common(5):\n",
    "            print(f\"  {pos_tag}: {count} t·ª´\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è PyVi ch∆∞a c√†i ƒë·∫∑t, b·ªè qua b∆∞·ªõc POS Tagging\")\n",
    "        words, pos_tags = tokens, []\n",
    "    \n",
    "    # B∆∞·ªõc 5: STEMMING\n",
    "    print(\"\\nüìç B∆Ø·ªöC 5: STEMMING (Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng)\")\n",
    "    print(\"-\"*80)\n",
    "    if PYVI_AVAILABLE and len(pos_tags) > 0:\n",
    "        important_pos = ['N', 'V', 'A', 'M']\n",
    "        stemmed = [word for word, pos in zip(words, pos_tags) \n",
    "                   if any(pos.startswith(p) for p in important_pos)]\n",
    "        print(f\"Ch·ªâ gi·ªØ l·∫°i: Danh t·ª´ (N), ƒê·ªông t·ª´ (V), T√≠nh t·ª´ (A), S·ªë t·ª´ (M)\")\n",
    "        print(f\"S·ªë t·ª´ tr∆∞·ªõc stemming: {len(words)}\")\n",
    "        print(f\"S·ªë t·ª´ sau stemming: {len(stemmed)}\")\n",
    "        print(f\"30 t·ª´ ƒë√£ stemming: {stemmed[:30]}\")\n",
    "    else:\n",
    "        stemmed = filtered_tokens\n",
    "        print(f\"S·ª≠ d·ª•ng filtered tokens: {len(stemmed)} t·ª´\")\n",
    "    \n",
    "    # T·ªïng k·∫øt\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ HO√ÄN TH√ÄNH QUY TR√åNH X·ª¨ L√ù\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä T√≥m t·∫Øt:\")\n",
    "    print(f\"  ‚Ä¢ Text g·ªëc: {len(text)} k√Ω t·ª±, {len(text.split())} t·ª´\")\n",
    "    print(f\"  ‚Ä¢ Sau cleaning: {len(cleaned)} k√Ω t·ª±\")\n",
    "    print(f\"  ‚Ä¢ Sau tokenization: {len(tokens)} tokens\")\n",
    "    print(f\"  ‚Ä¢ Sau stopwords removal: {len(filtered_tokens)} tokens\")\n",
    "    print(f\"  ‚Ä¢ Sau stemming: {len(stemmed)} tokens\")\n",
    "    print(f\"  ‚Ä¢ T·ª∑ l·ªá gi·∫£m: {(1 - len(stemmed)/len(tokens))*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'original': text,\n",
    "        'cleaned': cleaned,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed,\n",
    "        'pos_tags': pos_tags if PYVI_AVAILABLE else []\n",
    "    }\n",
    "\n",
    "print(\"‚úì complete_text_processing_pipeline function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd658a8",
   "metadata": {},
   "source": [
    "### Demo: Ch·∫°y quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n ƒê·∫¶Y ƒê·ª¶\n",
    "\n",
    "Ch·∫°y cell b√™n d∆∞·ªõi ƒë·ªÉ xem demo v·ªõi 1 document t·ª´ database (bao g·ªìm C·∫¢ 5 B∆Ø·ªöC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd46c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\n",
      "‚úì ƒê√£ c√≥ processor\n",
      "\n",
      "üìÑ Document: '·∫¢o t∆∞·ªüng b√≥ng ƒë√° Vi·ªát Nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± VCK U23 ch√¢u √Å'\n",
      "\n",
      "üîÑ B·∫ÆT ƒê·∫¶U QUY TR√åNH X·ª¨ L√ù VƒÇN B·∫¢N\n",
      "================================================================================\n",
      "\n",
      "üìç B∆Ø·ªöC 1: CLEANING (L√†m s·∫°ch vƒÉn b·∫£n)\n",
      "--------------------------------------------------------------------------------\n",
      "Text g·ªëc: '·∫¢o t∆∞·ªüng b√≥ng ƒë√° Vi·ªát Nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± VCK U23 ch√¢u √Å' U23 Vi·ªát Nam v·ª´a gi√†nh v√© d·ª± V√≤ng chung k·∫øt U23 ch√¢u √Å 2026 sau tr·∫≠n th·∫Øng 1-0 tr∆∞·ªõc Yemen, ƒë√°nh d·∫•u l·∫ßn th·ª© s√°u li√™n ti·∫øp g√≥p m·∫∑t ·ªü...\n",
      "Text sau cleaning: ·∫£o t∆∞·ªüng b√≥ng ƒë√° vi·ªát nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± vck u23 ch√¢u √° u23 vi·ªát nam v·ª´a gi√†nh v√© d·ª± v√≤ng chung k·∫øt u23 ch√¢u √° 2026 sau tr·∫≠n th·∫Øng 1 0 tr∆∞·ªõc yemen ƒë√°nh d·∫•u l·∫ßn th·ª© s√°u li√™n ti·∫øp g√≥p m·∫∑t ·ªü ƒë·∫•...\n",
      "ƒê·ªô d√†i: 2000 ‚Üí 1936 k√Ω t·ª±\n",
      "\n",
      "üìç B∆Ø·ªöC 2: TOKENIZATION (T√°ch t·ª´)\n",
      "--------------------------------------------------------------------------------\n",
      "S·ªë t·ª´ sau t√°ch: 353\n",
      "20 t·ª´ ƒë·∫ßu ti√™n: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±', 'vck', 'u23', 'ch√¢u', '√°', 'u23', 'vi·ªát_nam', 'v·ª´a', 'gi√†nh', 'v√©', 'd·ª±']\n",
      "\n",
      "üìç B∆Ø·ªöC 3: STOPWORDS REMOVAL (Lo·∫°i b·ªè t·ª´ d·ª´ng)\n",
      "--------------------------------------------------------------------------------\n",
      "Danh s√°ch stopwords (53 t·ª´):\n",
      "['l√™n', 'gi·ªØa', 'v·ªõi', 'ch·ªâ', 'ƒë√£', 'v√†', 'ƒë·ªÅu', 'trong', 'hay', 'theo', 'ng∆∞·ªùi', 'ƒë√¢y', 'm√†', '·ªü', 'ƒë√≥', 'ho·∫∑c', 'ƒë∆∞·ª£c', 'c·ªßa', 'do', 'ƒë·∫•y'] ...\n",
      "\n",
      "S·ªë t·ª´ tr∆∞·ªõc: 353\n",
      "S·ªë t·ª´ sau: 254\n",
      "ƒê√£ lo·∫°i b·ªè: 99 t·ª´ d·ª´ng (28.0%)\n",
      "20 t·ª´ c√≤n l·∫°i: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'gi√†nh', 'v√©', 'd·ª±', 'vck', 'u23', 'ch√¢u', 'u23', 'vi·ªát_nam', 'v·ª´a', 'gi√†nh', 'v√©', 'd·ª±', 'v√≤ng', 'chung_k·∫øt']\n",
      "\n",
      "üìç B∆Ø·ªöC 4: POS TAGGING - HMM (G·∫Øn nh√£n t·ª´ lo·∫°i)\n",
      "--------------------------------------------------------------------------------\n",
      "STT   T·ª´                        POS Tag   \n",
      "---------------------------------------------\n",
      "1     ·∫£o_t∆∞·ªüng                  N         \n",
      "2     b√≥ng_ƒë√°                   N         \n",
      "3     vi·ªát                      N         \n",
      "4     nam                       N         \n",
      "5     v∆∞∆°n                      V         \n",
      "6     t·∫ßm                       N         \n",
      "7     khi                       N         \n",
      "8     gi√†nh                     V         \n",
      "9     v√©                        N         \n",
      "10    d·ª±                        V         \n",
      "11    vck                       N         \n",
      "12    u23                       V         \n",
      "13    ch√¢u                      N         \n",
      "14    √°                         V         \n",
      "15    u23                       N         \n",
      "\n",
      "üìä Th·ªëng k√™ POS Tags:\n",
      "  N: 131 t·ª´\n",
      "  V: 88 t·ª´\n",
      "  A: 29 t·ª´\n",
      "  R: 28 t·ª´\n",
      "  E: 27 t·ª´\n",
      "\n",
      "üìç B∆Ø·ªöC 5: STEMMING (Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng)\n",
      "--------------------------------------------------------------------------------\n",
      "Ch·ªâ gi·ªØ l·∫°i: Danh t·ª´ (N), ƒê·ªông t·ª´ (V), T√≠nh t·ª´ (A), S·ªë t·ª´ (M)\n",
      "S·ªë t·ª´ tr∆∞·ªõc stemming: 353\n",
      "S·ªë t·ª´ sau stemming: 264\n",
      "30 t·ª´ ƒë√£ stemming: ['·∫£o_t∆∞·ªüng', 'b√≥ng_ƒë√°', 'vi·ªát', 'nam', 'v∆∞∆°n', 't·∫ßm', 'khi', 'gi√†nh', 'v√©', 'd·ª±', 'vck', 'u23', 'ch√¢u', '√°', 'u23', 'vi·ªát_nam', 'gi√†nh', 'v√©', 'd·ª±', 'v√≤ng', 'chung_k·∫øt', 'u23', 'ch√¢u', '√°', '2026', 'tr·∫≠n', 'th·∫Øng', '1', '0', 'yemen']\n",
      "\n",
      "================================================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH QUY TR√åNH X·ª¨ L√ù\n",
      "================================================================================\n",
      "üìä T√≥m t·∫Øt:\n",
      "  ‚Ä¢ Text g·ªëc: 2000 k√Ω t·ª±, 439 t·ª´\n",
      "  ‚Ä¢ Sau cleaning: 1936 k√Ω t·ª±\n",
      "  ‚Ä¢ Sau tokenization: 353 tokens\n",
      "  ‚Ä¢ Sau stopwords removal: 254 tokens\n",
      "  ‚Ä¢ Sau stemming: 264 tokens\n",
      "  ‚Ä¢ T·ª∑ l·ªá gi·∫£m: 25.2%\n"
     ]
    }
   ],
   "source": [
    "# Demo: Quy tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n ƒê·∫¶Y ƒê·ª¶\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt n·ªëi database\n",
    "try:\n",
    "    collection.count_documents({})\n",
    "    print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "except:\n",
    "    print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "    client, db, collection = connect_to_database()\n",
    "\n",
    "# Ki·ªÉm tra processor\n",
    "try:\n",
    "    processor\n",
    "    print(\"‚úì ƒê√£ c√≥ processor\")\n",
    "except:\n",
    "    processor = VietnameseTextProcessor()\n",
    "    print(\"‚úì ƒê√£ kh·ªüi t·∫°o processor\")\n",
    "\n",
    "# Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "if collection is None:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi database. Vui l√≤ng ki·ªÉm tra MONGO_URI!\")\n",
    "else:\n",
    "    # L·∫•y 1 document m·∫´u\n",
    "    doc = collection.find_one()\n",
    "    if doc:\n",
    "        full_text = f\"{doc.get('title', '')} {doc.get('content', '')}\"\n",
    "        print(f\"\\nüìÑ Document: {doc.get('title', 'N/A')}\\n\")\n",
    "        \n",
    "        # Ch·∫°y quy tr√¨nh ƒë·∫ßy ƒë·ªß\n",
    "        result = complete_text_processing_pipeline(full_text[:2000], processor)\n",
    "    else:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y document trong collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6ecf8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_multiple_documents function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def analyze_multiple_documents(collection, processor, n=50, top_n=50, filename=None):\n",
    "    \"\"\"L·∫•y ng·∫´u nhi√™n n documents, g·ªôp tokens v√† t√≠nh t·∫ßn su·∫•t chung\"\"\"\n",
    "    docs = collection.aggregate([{\"$sample\": {\"size\": n}}])\n",
    "    all_tokens = []\n",
    "    \n",
    "    for idx, doc in enumerate(docs, 1):\n",
    "        result = processor.process_document(doc)\n",
    "        tokens = result[\"filtered_tokens\"]\n",
    "        all_tokens.extend(tokens)   # g·ªôp t·∫•t c·∫£ tokens l·∫°i\n",
    "    \n",
    "    # T√≠nh t·∫ßn su·∫•t t·ª´ to√†n b·ªô tokens\n",
    "    counter = Counter(all_tokens)\n",
    "    total = sum(counter.values())\n",
    "    \n",
    "    data = []\n",
    "    for i, (word, freq) in enumerate(counter.most_common(top_n), 1):\n",
    "        Pr = freq / total * 100\n",
    "        data.append({\n",
    "            \"Word\": word,\n",
    "            \"Freq\": freq,\n",
    "            \"r\": i,\n",
    "            \"Pr(%)\": round(Pr, 2),\n",
    "            \"r*Pr\": round(i * Pr / 100, 3)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # T·∫°o folder outputs n·∫øu ch∆∞a c√≥\n",
    "    output_dir = os.path.join(\"d:\\\\data\\\\Search_Engine\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Xu·∫•t file CSV\n",
    "    if not filename:\n",
    "        filename = f\"wordfreq_{n}docs_total_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_csv(filepath, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ {n} documents (g·ªôp) v√†o: {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì analyze_multiple_documents function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cdb2d",
   "metadata": {},
   "source": [
    "## 8. N-grams Analysis\n",
    "\n",
    "### H√†m ph√¢n t√≠ch N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2959283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì generate_ngrams function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Sinh n-grams t·ª´ list tokens\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "print(\"‚úì generate_ngrams function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7a2885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_ngrams_combined function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def analyze_ngrams_combined(collection, processor, n_docs=1800, max_n=5, top_k=50, filename=None):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch n-grams (1 -> max_n), l∆∞u t·∫•t c·∫£ v√†o 1 file CSV duy nh·∫•t\n",
    "    \"\"\"\n",
    "    docs = collection.aggregate([{\"$sample\": {\"size\": n_docs}}])\n",
    "\n",
    "    all_tokens_list = []\n",
    "    for doc in docs:\n",
    "        result = processor.process_document(doc)\n",
    "        all_tokens_list.append(result[\"filtered_tokens\"])\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        ngrams_all = []\n",
    "        for tokens in all_tokens_list:\n",
    "            ngrams_all.extend(generate_ngrams(tokens, n))\n",
    "        \n",
    "        counter = Counter(ngrams_all)\n",
    "        for phrase, freq in counter.most_common(top_k):\n",
    "            all_data.append({\n",
    "                \"n\": n,\n",
    "                \"Frequency\": freq,\n",
    "                \"Phrase\": phrase\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # T·∫°o folder outputs n·∫øu ch∆∞a c√≥\n",
    "    output_dir = os.path.join(\"d:\\\\data\\\\Search_Engine\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not filename:\n",
    "        filename = f\"ngrams_combined_{n_docs}docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_csv(filepath, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ n-grams (1-{max_n}) v√†o: {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì analyze_ngrams_combined function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd8f3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_single_ngram function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def analyze_single_ngram(collection, processor, n=1, n_docs=1800, top_k=100, filename=None):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch m·ªôt lo·∫°i n-gram c·ª• th·ªÉ (1-gram, 2-gram, etc.)\n",
    "    \"\"\"\n",
    "    docs = collection.aggregate([{\"$sample\": {\"size\": n_docs}}])\n",
    "\n",
    "    all_tokens_list = []\n",
    "    print(f\"\\nƒêang x·ª≠ l√Ω {n_docs} documents cho {n}-gram...\")\n",
    "    for doc in tqdm(docs, desc=f\"Processing {n}-gram\", total=n_docs):\n",
    "        result = processor.process_document(doc)\n",
    "        all_tokens_list.append(result[\"filtered_tokens\"])\n",
    "\n",
    "    # Sinh n-grams\n",
    "    ngrams_all = []\n",
    "    for tokens in all_tokens_list:\n",
    "        ngrams_all.extend(generate_ngrams(tokens, n))\n",
    "    \n",
    "    # ƒê·∫øm t·∫ßn su·∫•t\n",
    "    counter = Counter(ngrams_all)\n",
    "    total = sum(counter.values())\n",
    "    \n",
    "    data = []\n",
    "    for i, (phrase, freq) in enumerate(counter.most_common(top_k), 1):\n",
    "        Pr = freq / total * 100\n",
    "        data.append({\n",
    "            \"Rank\": i,\n",
    "            \"Phrase\": phrase,\n",
    "            \"Frequency\": freq,\n",
    "            \"Pr(%)\": round(Pr, 2)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOP {top_k} {n}-GRAM\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # T·∫°o folder outputs n·∫øu ch∆∞a c√≥\n",
    "    output_dir = os.path.join(\"d:\\\\data\\\\Search_Engine\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # L∆∞u file\n",
    "    if not filename:\n",
    "        ngram_names = {1: \"unigram\", 2: \"bigram\", 3: \"trigram\", 4: \"fourgram\", 5: \"fivegram\"}\n",
    "        name = ngram_names.get(n, f\"{n}gram\")\n",
    "        filename = f\"{name}_{n_docs}docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_csv(filepath, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ {n}-gram v√†o: {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì analyze_single_ngram function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d64b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.1. Ph√¢n T√≠ch T·ª´ng Lo·∫°i N-gram\n",
    "\n",
    "### Ch·∫°y c√°c cell b√™n d∆∞·ªõi ƒë·ªÉ ph√¢n t√≠ch t·ª´ng lo·∫°i n-gram ri√™ng bi·ªát"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c9c42",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: K·∫øt n·ªëi Database v√† kh·ªüi t·∫°o Processor\n",
    "\n",
    "N·∫øu ch∆∞a k·∫øt n·ªëi, ch·∫°y cell n√†y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6e4248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\n",
      "‚úì ƒê√£ c√≥ processor\n"
     ]
    }
   ],
   "source": [
    "# K·∫øt n·ªëi database v√† kh·ªüi t·∫°o processor (n·∫øu ch∆∞a l√†m)\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt n·ªëi database\n",
    "try:\n",
    "    collection.count_documents({})\n",
    "    print(\"‚úì ƒê√£ c√≥ k·∫øt n·ªëi database\")\n",
    "except:\n",
    "    print(\"ƒêang k·∫øt n·ªëi database...\")\n",
    "    client, db, collection = connect_to_database()\n",
    "    \n",
    "    if collection is None:\n",
    "        print(\"‚ùå L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi database. Vui l√≤ng ki·ªÉm tra MONGO_URI!\")\n",
    "    \n",
    "# Ki·ªÉm tra processor\n",
    "try:\n",
    "    processor\n",
    "    print(\"‚úì ƒê√£ c√≥ processor\")\n",
    "except:\n",
    "    print(\"ƒêang kh·ªüi t·∫°o processor...\")\n",
    "    processor = VietnameseTextProcessor()\n",
    "    print(\"‚úì Processor ƒë√£ s·∫µn s√†ng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e7b73",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Ph√¢n t√≠ch 1-gram (Unigram)\n",
    "\n",
    "Ph√¢n t√≠ch c√°c t·ª´ ƒë∆°n xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eb5b00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 1-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1-gram:   0%|          | 0/1830 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:25<00:00, 71.56it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 1-GRAM\n",
      "================================================================================\n",
      " Rank    Phrase  Frequency  Pr(%)\n",
      "    1       nam       8300   1.11\n",
      "    2       ƒë·ªôi       7740   1.04\n",
      "    3      tr·∫≠n       7570   1.01\n",
      "    4       hai       6793   0.91\n",
      "    5      vi·ªát       6765   0.91\n",
      "    6   c·∫ßu_th·ªß       6651   0.89\n",
      "    7      b√≥ng       5716   0.77\n",
      "    8       hlv       5609   0.75\n",
      "    9       nƒÉm       5198   0.70\n",
      "   10    league       5168   0.69\n",
      "   11     tr∆∞·ªõc       4815   0.65\n",
      "   12       s√¢n       4734   0.63\n",
      "   13       ƒë·∫øn       4517   0.61\n",
      "   14      ph·∫£i       4336   0.58\n",
      "   15       vƒÉn       3970   0.53\n",
      "   16    nguy·ªÖn       3944   0.53\n",
      "   17       b√†n       3914   0.52\n",
      "   18     th·∫Øng       3867   0.52\n",
      "   19     nhi·ªÅu       3816   0.51\n",
      "   20      ph√∫t       3683   0.49\n",
      "   21       anh       3654   0.49\n",
      "   22      nh·∫•t       3537   0.47\n",
      "   23      v√≤ng       3494   0.47\n",
      "   24       √¥ng       3376   0.45\n",
      "   25  vi·ªát_nam       3361   0.45\n",
      "   26      gi·∫£i       3264   0.44\n",
      "   27        h·ªç       3158   0.42\n",
      "   28    h√†_n·ªôi       3130   0.42\n",
      "   29        ba       2987   0.40\n",
      "   30       cup       2971   0.40\n",
      "   31       h∆°n       2968   0.40\n",
      "   32   b√≥ng_ƒë√°       2962   0.40\n",
      "   33       th·ª©       2912   0.39\n",
      "   34      ƒëi·ªÉm       2910   0.39\n",
      "   35   thi_ƒë·∫•u       2854   0.38\n",
      "   36      ng√†y       2744   0.37\n",
      "   37       m√πa       2618   0.35\n",
      "   38      ƒë·ªãnh       2617   0.35\n",
      "   39       h·∫£i       2615   0.35\n",
      "   40      ch∆°i       2606   0.35\n",
      "   41      thua       2472   0.33\n",
      "   42       clb       2462   0.33\n",
      "   43       ƒë·ª©c       2454   0.33\n",
      "   44       t√¥i       2416   0.32\n",
      "   45        ƒë√°       2406   0.32\n",
      "   46      b√¨nh       2383   0.32\n",
      "   47 ƒë·ªôi_tuy·ªÉn       2360   0.32\n",
      "   48       l·∫ßn       2297   0.31\n",
      "   49       ·∫£nh       2262   0.30\n",
      "   50   v√¥_ƒë·ªãch       2235   0.30\n",
      "   51       nh√†       2232   0.30\n",
      "   52      c√πng       2178   0.29\n",
      "   53      th√°i       2156   0.29\n",
      "   54      hagl       2140   0.29\n",
      "   55       v·∫´n       2028   0.27\n",
      "   56  tr·∫≠n_ƒë·∫•u       2016   0.27\n",
      "   57  ti·ªÅn_ƒë·∫°o       2002   0.27\n",
      "   58      ƒëang       1993   0.27\n",
      "   59       lan       1943   0.26\n",
      "   60        s·ª±       1890   0.25\n",
      "   61      2024       1882   0.25\n",
      "   62       t·ªët       1805   0.24\n",
      "   63      c√¥ng       1797   0.24\n",
      "   64       m·ªõi       1765   0.24\n",
      "   65      b·∫£ng       1744   0.23\n",
      "   66       cao       1740   0.23\n",
      "   67       qua       1739   0.23\n",
      "   68      sang       1677   0.22\n",
      "   69    c√≥_th·ªÉ       1657   0.22\n",
      "   70       l√†m       1623   0.22\n",
      "   71    c∆°_h·ªôi       1605   0.22\n",
      "   72   th·ªß_m√¥n       1581   0.21\n",
      "   73        ƒëi       1568   0.21\n",
      "   74        fc       1565   0.21\n",
      "   75       ƒë·∫ßu       1555   0.21\n",
      "   76      m·∫°nh       1542   0.21\n",
      "   77      ch∆∞a       1539   0.21\n",
      "   78      gi√∫p       1533   0.21\n",
      "   79     ho√†ng       1497   0.20\n",
      "   80     thanh       1492   0.20\n",
      "   81      h·∫°ng       1486   0.20\n",
      "   82      ti·∫øn       1484   0.20\n",
      "   83       n√≥i       1480   0.20\n",
      "   84      h√†ng       1477   0.20\n",
      "   85 tr·ªçng_t√†i       1464   0.20\n",
      "   86      l∆∞·ª£t       1420   0.19\n",
      "   87      cu·ªëi       1413   0.19\n",
      "   88        12       1407   0.19\n",
      "   89     th√°ng       1407   0.19\n",
      "   90        10       1406   0.19\n",
      "   91       r·∫•t       1398   0.19\n",
      "   92      t·ª´ng       1389   0.19\n",
      "   93       g·∫∑p       1378   0.18\n",
      "   94     quang       1369   0.18\n",
      "   95        tp       1334   0.18\n",
      "   96       hcm       1326   0.18\n",
      "   97       h√≤a       1319   0.18\n",
      "   98     khi·∫øn       1318   0.18\n",
      "   99 tuy_nhi√™n       1315   0.18\n",
      "  100   ti·ªÅn_v·ªá       1302   0.17\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 1-gram v√†o: d:\\data\\Search_Engine\\outputs\\unigram_1830docs_20251009_155739.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch 1-gram (Unigram)\n",
    "df_unigram = analyze_single_ngram(collection, processor, n=1, n_docs=1830, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a587f0",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 3: Ph√¢n t√≠ch 2-gram (Bigram)\n",
    "\n",
    "Ph√¢n t√≠ch c√°c c·∫∑p t·ª´ li√™n ti·∫øp xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41b82112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 2-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:45<00:00, 40.11it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 2-GRAM\n",
      "================================================================================\n",
      " Rank           Phrase  Frequency  Pr(%)\n",
      "    1         vi·ªát nam       5337   0.72\n",
      "    2         ƒë·ªôi b√≥ng       2054   0.28\n",
      "    3         th√°i lan       1886   0.25\n",
      "    4         nam ƒë·ªãnh       1768   0.24\n",
      "    5           tp hcm       1318   0.18\n",
      "    6        h·∫£i ph√≤ng       1078   0.14\n",
      "    7       b√¨nh d∆∞∆°ng       1039   0.14\n",
      "    8        h√†_n·ªôi fc       1009   0.14\n",
      "    9          ch·ªß nh√†        981   0.13\n",
      "   10        b√†n th·∫Øng        933   0.13\n",
      "   11          hlv kim        874   0.12\n",
      "   12        asean cup        815   0.11\n",
      "   13         m√πa gi·∫£i        766   0.10\n",
      "   14        b√¨nh ƒë·ªãnh        762   0.10\n",
      "   15          th·ª© hai        752   0.10\n",
      "   16       hi·∫øu l∆∞∆°ng        721   0.10\n",
      "   17        quang h·∫£i        695   0.09\n",
      "   18         v√≤ng c·∫•m        671   0.09\n",
      "   19     b√≥ng_ƒë√° vi·ªát        665   0.09\n",
      "   20          aff cup        665   0.09\n",
      "   21         hi·ªáp hai        650   0.09\n",
      "   22          hai ƒë·ªôi        648   0.09\n",
      "   23         sang sik        623   0.08\n",
      "   24        ƒë·ªôi kh√°ch        622   0.08\n",
      "   25         kim sang        621   0.08\n",
      "   26         hlv park        607   0.08\n",
      "   27        asian cup        594   0.08\n",
      "   28        ti·∫øn linh        594   0.08\n",
      "   29         ƒë√¥ng nam        580   0.08\n",
      "   30         cup 2024        580   0.08\n",
      "   31         hai tr·∫≠n        579   0.08\n",
      "   32         sinh nƒÉm        573   0.08\n",
      "   33       nguy·ªÖn vƒÉn        560   0.08\n",
      "   34         m·ªü t·ª∑_s·ªë        556   0.07\n",
      "   35         th·ªÉ c√¥ng        542   0.07\n",
      "   36       ngo·∫°i binh        524   0.07\n",
      "   37         ƒë·ª©c ƒë·ªìng        519   0.07\n",
      "   38         nh·∫≠t b·∫£n        515   0.07\n",
      "   39        thanh ho√°        509   0.07\n",
      "   40       tr·∫≠n th·∫Øng        509   0.07\n",
      "   41           th·ª© ba        507   0.07\n",
      "   42         l·ªëi ch∆°i        501   0.07\n",
      "   43      league 2023        501   0.07\n",
      "   44         vƒÉn to√†n        499   0.07\n",
      "   45      c√¥ng ph∆∞·ª£ng        485   0.07\n",
      "   46        v√≤ng b·∫£ng        484   0.07\n",
      "   47        h·∫°ng nh·∫•t        475   0.06\n",
      "   48         th·∫ßy tr√≤        464   0.06\n",
      "   49         h√†ng th·ªß        463   0.06\n",
      "   50        2024 2025        451   0.06\n",
      "   51         g·∫ßn nh·∫•t        438   0.06\n",
      "   52         h√†n qu·ªëc        431   0.06\n",
      "   53         tr·ª• h·∫°ng        426   0.06\n",
      "   54         ƒë·ª©ng th·ª©        425   0.06\n",
      "   55         h√†ng ƒë·∫´y        408   0.05\n",
      "   56        vƒÉn quy·∫øt        404   0.05\n",
      "   57         ƒë·∫øn ph√∫t        399   0.05\n",
      "   58 b√≥ng_ƒë√° vi·ªát_nam        396   0.05\n",
      "   59     c·∫ßu_th·ªß vi·ªát        394   0.05\n",
      "   60     cup qu·ªëc_gia        388   0.05\n",
      "   61        ho√†ng ƒë·ª©c        385   0.05\n",
      "   62         v∆∞·ª£t qua        372   0.05\n",
      "   63     ch·ª©c v√¥_ƒë·ªãch        371   0.05\n",
      "   64          vƒÉn l√¢m        371   0.05\n",
      "   65        tr·∫≠n thua        364   0.05\n",
      "   66          ·∫£nh ƒë·ª©c        363   0.05\n",
      "   67        ti·∫øn d≈©ng        362   0.05\n",
      "   68   v√¥_ƒë·ªãch league        362   0.05\n",
      "   69        sea games        358   0.05\n",
      "   70          hai b√†n        358   0.05\n",
      "   71   c√¥ng_an h√†_n·ªôi        355   0.05\n",
      "   72         t·ªët nh·∫•t        353   0.05\n",
      "   73         duy m·∫°nh        352   0.05\n",
      "   74        park hang        351   0.05\n",
      "   75          ·∫£nh l√¢m        351   0.05\n",
      "   76         hang seo        350   0.05\n",
      "   77     nh√† c·∫ßm_qu√¢n        349   0.05\n",
      "   78         s√¢n h√†ng        349   0.05\n",
      "   79        nhi·ªÅu h∆°n        342   0.05\n",
      "   80         b√™n c·∫°nh        340   0.05\n",
      "   81       s√†i_g√≤n fc        339   0.05\n",
      "   82  nguy·ªÖn xu√¢n_son        334   0.04\n",
      "   83          l∆∞·ª£t ƒëi        332   0.04\n",
      "   84      c·∫ßu_th·ªß tr·∫ª        328   0.04\n",
      "   85         ƒë∆∞a b√≥ng        326   0.04\n",
      "   86          ph√∫t b√π        320   0.04\n",
      "   87        ninh b√¨nh        314   0.04\n",
      "   88        2023 2024        311   0.04\n",
      "   89        gi·∫£i h·∫°ng        311   0.04\n",
      "   90      league 2024        310   0.04\n",
      "   91         tu·∫•n h·∫£i        307   0.04\n",
      "   92          nƒÉm nay        305   0.04\n",
      "   93          ba tr·∫≠n        305   0.04\n",
      "   94        vƒÉn thanh        298   0.04\n",
      "   95         covid 19        292   0.04\n",
      "   96         b√†n thua        287   0.04\n",
      "   97          ƒë·ªôi ch·ªß        287   0.04\n",
      "   98          ba ƒëi·ªÉm        286   0.04\n",
      "   99   ban hu·∫•n_luy·ªán        285   0.04\n",
      "  100           clb tp        283   0.04\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 2-gram v√†o: d:\\data\\Search_Engine\\outputs\\bigram_1830docs_20251009_155826.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch 2-gram (Bigram)\n",
    "df_bigram = analyze_single_ngram(collection, processor, n=2, n_docs=1830, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aeeacb",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 4: Ph√¢n t√≠ch 3-gram (Trigram)\n",
    "\n",
    "Ph√¢n t√≠ch c√°c c·ª•m 3 t·ª´ li√™n ti·∫øp xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71fcb13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 3-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:52<00:00, 34.80it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 3-GRAM\n",
      "================================================================================\n",
      " Rank                     Phrase  Frequency  Pr(%)\n",
      "    1               kim sang sik        617   0.08\n",
      "    2           b√≥ng_ƒë√° vi·ªát nam        615   0.08\n",
      "    3               hlv kim sang        514   0.07\n",
      "    4             asean cup 2024        465   0.06\n",
      "    5              park hang seo        355   0.05\n",
      "    6           c·∫ßu_th·ªß vi·ªát nam        347   0.05\n",
      "    7               s√¢n h√†ng ƒë·∫´y        344   0.05\n",
      "    8               ·∫£nh ƒë·ª©c ƒë·ªìng        295   0.04\n",
      "    9           league 2024 2025        291   0.04\n",
      "   10                 clb tp hcm        283   0.04\n",
      "   11             ·∫£nh hi·∫øu l∆∞∆°ng        273   0.04\n",
      "   12              hlv park hang        261   0.04\n",
      "   13              b√πi ti·∫øn d≈©ng        259   0.03\n",
      "   14                ƒë·ªôi ch·ªß nh√†        247   0.03\n",
      "   15         ƒë·ªôi_tuy·ªÉn vi·ªát nam        241   0.03\n",
      "   16             tuy·ªÉn vi·ªát nam        234   0.03\n",
      "   17           nguy·ªÖn quang h·∫£i        231   0.03\n",
      "   18               s√¢n vi·ªát tr√¨        224   0.03\n",
      "   19             gi·∫£i h·∫°ng nh·∫•t        224   0.03\n",
      "   20           nguy·ªÖn ti·∫øn linh        223   0.03\n",
      "   21                ƒë·ªó duy m·∫°nh        210   0.03\n",
      "   22               qu·∫ø ng·ªçc h·∫£i        203   0.03\n",
      "   23             asian cup 2027        191   0.03\n",
      "   24           nguy·ªÖn ho√†ng ƒë·ª©c        184   0.02\n",
      "   25               th·∫ßy tr√≤ hlv        182   0.02\n",
      "   26              ph·∫°m tu·∫•n h·∫£i        181   0.02\n",
      "   27       di·ªÖn_bi·∫øn ch√≠nh tr·∫≠n        179   0.02\n",
      "   28           league 2023 2024        179   0.02\n",
      "   29                s√¢n m·ªπ ƒë√¨nh        177   0.02\n",
      "   30               ƒë·∫∑ng vƒÉn l√¢m        170   0.02\n",
      "   31           s√¢n thi√™n tr∆∞·ªùng        158   0.02\n",
      "   32               v≈© vƒÉn thanh        155   0.02\n",
      "   33            nguy·ªÖn vƒÉn to√†n        155   0.02\n",
      "   34              sang hi·ªáp hai        154   0.02\n",
      "   35          ti·ªÅn_ƒë·∫°o sinh nƒÉm        144   0.02\n",
      "   36          hlv park hang_seo        142   0.02\n",
      "   37                ph√∫t b√π th·ª©        141   0.02\n",
      "   38       v√≤ng_lo·∫°i cu·ªëi asian        141   0.02\n",
      "   39             cu·ªëi asian cup        141   0.02\n",
      "   40       afc champions league        140   0.02\n",
      "   41              v≈© ti·∫øn th√†nh        137   0.02\n",
      "   42                ƒë·ªôi b√≥ng x·ª©        137   0.02\n",
      "   43             b√πi ho√†ng vi·ªát        134   0.02\n",
      "   44               ·∫£nh l√¢m tho·∫£        134   0.02\n",
      "   45              vi·ªát nam thua        133   0.02\n",
      "   46               ƒëo√†n vƒÉn h·∫≠u        133   0.02\n",
      "   47               ƒëua tr·ª• h·∫°ng        133   0.02\n",
      "   48            nguy·ªÖn hai long        133   0.02\n",
      "   49            chu ƒë√¨nh nghi√™m        132   0.02\n",
      "   50              tr·∫≠n vi·ªát nam        132   0.02\n",
      "   51              tr·∫≠n g·∫ßn nh·∫•t        129   0.02\n",
      "   52             ho√†ng vi·ªát anh        126   0.02\n",
      "   53               b√†n m·ªü t·ª∑_s·ªë        125   0.02\n",
      "   54                 h·ªì t·∫•n t√†i        124   0.02\n",
      "   55    v√≤ng_lo·∫°i hai world_cup        123   0.02\n",
      "   56               ƒë·ªôi b√≥ng ph·ªë        123   0.02\n",
      "   57      gi·∫£i v√¥_ƒë·ªãch qu·ªëc_gia        118   0.02\n",
      "   58            ƒë·ªôi b√≥ng th·ªß_ƒë√¥        115   0.02\n",
      "   59               v≈© h·ªìng vi·ªát        115   0.02\n",
      "   60               b√≥ng ph·ªë n√∫i        113   0.02\n",
      "   61          nguy·ªÖn ƒë√¨nh tri·ªáu        111   0.01\n",
      "   62            v√¥_ƒë·ªãch aff cup        111   0.01\n",
      "   63         nguy·ªÖn th√†nh chung        109   0.01\n",
      "   64         hai world_cup 2026        108   0.01\n",
      "   65               l√™ hu·ª≥nh ƒë·ª©c        108   0.01\n",
      "   66               ƒë·ªôi b√≥ng hlv        106   0.01\n",
      "   67           nguy·ªÖn vƒÉn quy·∫øt        106   0.01\n",
      "   68                hlv v≈© h·ªìng        106   0.01\n",
      "   69          vi·ªát_nam th√°i lan        106   0.01\n",
      "   70               ƒë·∫ßu hi·ªáp hai        105   0.01\n",
      "   71             asian cup 2023        105   0.01\n",
      "   72               hlv chu ƒë√¨nh        104   0.01\n",
      "   73              mang d√≤ng m√°u        102   0.01\n",
      "   74               vi·ªát nam vff        101   0.01\n",
      "   75               aff cup 2018        101   0.01\n",
      "   76            khu·∫•t vƒÉn khang        101   0.01\n",
      "   77     chuy√™n_nghi·ªáp vi·ªát nam        100   0.01\n",
      "   78               phan vƒÉn ƒë·ª©c        100   0.01\n",
      "   79             b·∫£ng asean cup        100   0.01\n",
      "   80      hi·ªáu_s·ªë b√†n th·∫Øng_b·∫°i         99   0.01\n",
      "   81               u23 vi·ªát nam         99   0.01\n",
      "   82              do√£n ng·ªçc t√¢n         99   0.01\n",
      "   83              ƒëo√†n qu√¢n hlv         98   0.01\n",
      "   84                hlv v≈© ti·∫øn         98   0.01\n",
      "   85                ph√∫t b√π gi·ªù         98   0.01\n",
      "   86                 b√πi vƒ© h√†o         97   0.01\n",
      "   87               ƒë·ªôi b√≥ng ƒë·∫•t         97   0.01\n",
      "   88               aff cup 2024         96   0.01\n",
      "   89               d√≤ng m√°u lai         96   0.01\n",
      "   90              shin tae yong         95   0.01\n",
      "   91           t·∫°o nhi·ªÅu c∆°_h·ªôi         95   0.01\n",
      "   92              2024 2025 s√¢n         94   0.01\n",
      "   93              nguy·ªÖn vƒÉn vƒ©         94   0.01\n",
      "   94 b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         92   0.01\n",
      "   95                ghi hai b√†n         92   0.01\n",
      "   96             ph·∫°m xu√¢n m·∫°nh         91   0.01\n",
      "   97              m·ªü t·ª∑_s·ªë ph√∫t         89   0.01\n",
      "   98             tr∆∞·ªõc vi·ªát nam         88   0.01\n",
      "   99          si√™u cup qu·ªëc_gia         87   0.01\n",
      "  100               l·∫ßn g·∫ßn nh·∫•t         87   0.01\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 3-gram v√†o: d:\\data\\Search_Engine\\outputs\\trigram_1830docs_20251009_155921.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch 3-gram (Trigram)\n",
    "df_trigram = analyze_single_ngram(collection, processor, n=3, n_docs=1830, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a672060",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Ph√¢n t√≠ch 4-gram (Four-gram)\n",
    "\n",
    "Ph√¢n t√≠ch c√°c c·ª•m 4 t·ª´ li√™n ti·∫øp xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf6200d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 4-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [01:00<00:00, 30.38it/s] \n",
      "Processing 4-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [01:00<00:00, 30.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 4-GRAM\n",
      "================================================================================\n",
      " Rank                                Phrase  Frequency  Pr(%)\n",
      "    1                      hlv kim sang sik        517   0.07\n",
      "    2                     hlv park hang seo        259   0.03\n",
      "    3              v√≤ng_lo·∫°i cu·ªëi asian cup        141   0.02\n",
      "    4                    b√πi ho√†ng vi·ªát anh        128   0.02\n",
      "    5                   cu·ªëi asian cup 2027        121   0.02\n",
      "    6                      ƒë·ªôi b√≥ng ph·ªë n√∫i        112   0.02\n",
      "    7                      hlv v≈© h·ªìng vi·ªát        105   0.01\n",
      "    8                   hlv chu ƒë√¨nh nghi√™m        103   0.01\n",
      "    9          v√≤ng_lo·∫°i hai world_cup 2026        102   0.01\n",
      "   10                     hlv v≈© ti·∫øn th√†nh         98   0.01\n",
      "   11        b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam         92   0.01\n",
      "   12                     mang d√≤ng m√°u lai         89   0.01\n",
      "   13                  league 2024 2025 s√¢n         79   0.01\n",
      "   14                      vi·ªát tr√¨ ph√∫ th·ªç         78   0.01\n",
      "   15                   b·∫£ng asean cup 2024         75   0.01\n",
      "   16            li√™n_ƒëo√†n b√≥ng_ƒë√° vi·ªát nam         74   0.01\n",
      "   17 c√¥ng_ty c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp         72   0.01\n",
      "   18    c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         72   0.01\n",
      "   19             b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         72   0.01\n",
      "   20                 c·∫ßu_th·ªß mang d√≤ng m√°u         71   0.01\n",
      "   21            chuy√™n_nghi·ªáp vi·ªát nam vpf         69   0.01\n",
      "   22                   2024 ·∫£nh hi·∫øu l∆∞∆°ng         68   0.01\n",
      "   23              chung_k·∫øt asean cup 2024         68   0.01\n",
      "   24                      s√¢n vi·ªát tr√¨ ph√∫         67   0.01\n",
      "   25                night wolf league 2023         65   0.01\n",
      "   26             th·ªß_m√¥n nguy·ªÖn ƒë√¨nh tri·ªáu         64   0.01\n",
      "   27                     hlv shin tae yong         62   0.01\n",
      "   28                 nguy·ªÖn phong h·ªìng duy         61   0.01\n",
      "   29                    l√™ ph·∫°m th√†nh long         61   0.01\n",
      "   30                    ƒë·ªôi b√≥ng th√†nh nam         59   0.01\n",
      "   31                  hlv nguy·ªÖn ƒë·ª©c th·∫Øng         59   0.01\n",
      "   32                     ƒë·ªôi b√≥ng x·ª© thanh         57   0.01\n",
      "   33                 nh√† c·∫ßm_qu√¢n h√†n qu·ªëc         55   0.01\n",
      "   34              afc champions league two         55   0.01\n",
      "   35                     lƒëbƒë vi·ªát nam vff         54   0.01\n",
      "   36              ti·ªÅn_v·ªá nguy·ªÖn quang h·∫£i         54   0.01\n",
      "   37                   hlv phan thanh h√πng         54   0.01\n",
      "   38                     cu·ªôc ƒëua tr·ª• h·∫°ng         53   0.01\n",
      "   39                    asean cup 2024 ·∫£nh         52   0.01\n",
      "   40           world_cup 2026 khu_v·ª±c ch√¢u         52   0.01\n",
      "   41                      hlv l√™ hu·ª≥nh ƒë·ª©c         51   0.01\n",
      "   42                    ghi nhi·ªÅu b√†n nh·∫•t         51   0.01\n",
      "   43                ti·ªÅn_ƒë·∫°o sinh nƒÉm 1997         50   0.01\n",
      "   44    ti·ªÅn_ƒë·∫°o nh·∫≠p_t·ªãch nguy·ªÖn xu√¢n_son         50   0.01\n",
      "   45                     cup nay asean cup         49   0.01\n",
      "   46                      cup clb ƒë√¥ng nam         49   0.01\n",
      "   47                   v√≤ng b·∫£ng asean cup         49   0.01\n",
      "   48                   s√¢n h√†ng ƒë·∫´y h√†_n·ªôi         49   0.01\n",
      "   49                      s√¢n h√†ng ƒë·∫´y t·ªëi         48   0.01\n",
      "   50                  th·ªß_m√¥n ƒë·∫∑ng vƒÉn l√¢m         48   0.01\n",
      "   51                   v√≤ng b·∫£ng asian cup         47   0.01\n",
      "   52                     aff cup nay asean         47   0.01\n",
      "   53                      ƒë·ªôi b√≥ng x·ª© ngh·ªá         46   0.01\n",
      "   54                 th·ªß_m√¥n b√πi ti·∫øn d≈©ng         46   0.01\n",
      "   55                     play off tr·ª• h·∫°ng         44   0.01\n",
      "   56                 nh√† c·∫ßm_qu√¢n sinh nƒÉm         44   0.01\n",
      "   57                     m·ª´ng b√†n m·ªü t·ª∑_s·ªë         44   0.01\n",
      "   58                      th·∫ßy tr√≤ hlv kim         44   0.01\n",
      "   59                 hlv tr∆∞∆°ng vi·ªát ho√†ng         44   0.01\n",
      "   60                  b√≥ng_ƒë√° vi·ªát nam vff         43   0.01\n",
      "   61                  league 2024 2025 ·∫£nh         43   0.01\n",
      "   62                v√¥_ƒë·ªãch asean cup 2024         43   0.01\n",
      "   63                     v√≤ng b·∫£ng aff cup         43   0.01\n",
      "   64                    b√†n th·∫Øng ƒë·∫πp nh·∫•t         43   0.01\n",
      "   65                  v√¥_ƒë·ªãch aff cup 2018         42   0.01\n",
      "   66           world_cup 2022 khu_v·ª±c ch√¢u         42   0.01\n",
      "   67                   b·∫£ng asian cup 2023         41   0.01\n",
      "   68               gi·∫£i h·∫°ng nh·∫•t qu·ªëc_gia         41   0.01\n",
      "   69             ti·ªÅn_ƒë·∫°o nguy·ªÖn ti·∫øn linh         41   0.01\n",
      "   70                     th√°i lan vi·ªát nam         41   0.01\n",
      "   71                ti·ªÅn_ƒë·∫°o ph·∫°m tu·∫•n h·∫£i         41   0.01\n",
      "   72                     hlv ph·∫°m minh ƒë·ª©c         40   0.01\n",
      "   73                    th·ªùi hlv park hang         39   0.01\n",
      "   74                b√°n_k·∫øt asean cup 2024         39   0.01\n",
      "   75                     hc v√†ng sea games         38   0.01\n",
      "   76                      m√πa gi·∫£i nƒÉm nay         38   0.01\n",
      "   77                ƒë·ªôi_tr∆∞·ªüng ƒë·ªó duy m·∫°nh         38   0.01\n",
      "   78                 v√≤ng league 2025 2026         37   0.00\n",
      "   79                       ƒë·∫øn ph√∫t b√π th·ª©         37   0.00\n",
      "   80                trung_v·ªá b√πi ti·∫øn d≈©ng         37   0.00\n",
      "   81              ti·ªÅn_v·ªá nguy·ªÖn ho√†ng ƒë·ª©c         37   0.00\n",
      "   82              th·ªß_m√¥n tr·∫ßn nguy√™n m·∫°nh         37   0.00\n",
      "   83                    tr·∫≠n vi·ªát nam thua         36   0.00\n",
      "   84              lu·∫≠t b√†n th·∫Øng s√¢n_kh√°ch         36   0.00\n",
      "   85                     s√¢n h√†ng ƒë·∫´y ng√†y         36   0.00\n",
      "   86               v√≤ng chung_k·∫øt u23 ch√¢u         36   0.00\n",
      "   87                   wolf league 2023 24         35   0.00\n",
      "   88                      ph·∫£i ƒë√° play off         35   0.00\n",
      "   89               l∆∞·ª£t hai b·∫£ng v√≤ng_lo·∫°i         35   0.00\n",
      "   90               hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi         35   0.00\n",
      "   91                      tr√≤ hlv kim sang         35   0.00\n",
      "   92           v√≤ng_lo·∫°i ba world_cup 2026         34   0.00\n",
      "   93                    v√µ ho√†ng minh khoa         34   0.00\n",
      "   94                     ƒë·ªôi b√≥ng ƒë·∫•t c·∫£ng         34   0.00\n",
      "   95                 tr·∫≠n_ƒë·∫•u s√¢n h√†ng ƒë·∫´y         34   0.00\n",
      "   96         trung_t√¢m ƒë√†o_t·∫°o b√≥ng_ƒë√° tr·∫ª         34   0.00\n",
      "   97                     cup 2024 ·∫£nh hi·∫øu         33   0.00\n",
      "   98                   l∆∞·ª£t cu·ªëi v√≤ng b·∫£ng         33   0.00\n",
      "   99             th·ªß_t∆∞·ªõng ph·∫°m minh ch√≠nh         33   0.00\n",
      "  100                    asean cup 2024 s√¢n         33   0.00\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 4-gram v√†o: d:\\data\\Search_Engine\\outputs\\fourgram_1830docs_20251009_160023.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch 4-gram (Four-gram)\n",
    "df_fourgram = analyze_single_ngram(collection, processor, n=4, n_docs=1830, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca8da1",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 6: Ph√¢n t√≠ch 5-gram (Five-gram)\n",
    "\n",
    "Ph√¢n t√≠ch c√°c c·ª•m 5 t·ª´ li√™n ti·∫øp xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e6f50ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 5-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:27<00:00, 66.93it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 5-GRAM\n",
      "================================================================================\n",
      " Rank                                     Phrase  Frequency  Pr(%)\n",
      "    1              v√≤ng_lo·∫°i cu·ªëi asian cup 2027        121   0.02\n",
      "    2 c√¥ng_ty c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         72   0.01\n",
      "    3     c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam         72   0.01\n",
      "    4              b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian cup         72   0.01\n",
      "    5         b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam vpf         69   0.01\n",
      "    6                       s√¢n vi·ªát tr√¨ ph√∫ th·ªç         66   0.01\n",
      "    7                  c·∫ßu_th·ªß mang d√≤ng m√°u lai         64   0.01\n",
      "    8                      aff cup nay asean cup         47   0.01\n",
      "    9             li√™n_ƒëo√†n b√≥ng_ƒë√° vi·ªát nam vff         43   0.01\n",
      "   10                     th·ªùi hlv park hang seo         39   0.01\n",
      "   11               l∆∞·ª£t hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi         35   0.00\n",
      "   12              hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         35   0.00\n",
      "   13                  night wolf league 2023 24         35   0.00\n",
      "   14                      th·∫ßy tr√≤ hlv kim sang         35   0.00\n",
      "   15                       tr√≤ hlv kim sang sik         35   0.00\n",
      "   16                    cup 2024 ·∫£nh hi·∫øu l∆∞∆°ng         33   0.00\n",
      "   17            hai world_cup 2026 khu_v·ª±c ch√¢u         33   0.00\n",
      "   18                   v√≤ng b·∫£ng asian cup 2023         32   0.00\n",
      "   19                     2024 2025 s√¢n h√†ng ƒë·∫´y         31   0.00\n",
      "   20                    asean cup 2024 ·∫£nh hi·∫øu         31   0.00\n",
      "   21                   s√¢n h√†ng ƒë·∫´y h√†_n·ªôi ng√†y         31   0.00\n",
      "   22            h·∫°ng nh·∫•t qu·ªëc_gia cup qu·ªëc_gia         31   0.00\n",
      "   23                  fpt play https fptplay vn         31   0.00\n",
      "   24                   v√≤ng b·∫£ng asean cup 2024         31   0.00\n",
      "   25       v√≤ng_lo·∫°i hai world_cup 2026 khu_v·ª±c         30   0.00\n",
      "   26                night wolf league 2023 2024         30   0.00\n",
      "   27              league h·∫°ng nh·∫•t qu·ªëc_gia cup         30   0.00\n",
      "   28                   ƒë·ªânh nh·∫•t fpt play https         30   0.00\n",
      "   29                nh·∫•t fpt play https fptplay         30   0.00\n",
      "   30                  league 2024 2025 s√¢n h√†ng         29   0.00\n",
      "   31            truy·ªÅn_h√¨nh fpt fpt play ƒë∆°n_v·ªã         29   0.00\n",
      "   32             nh·∫•t qu·ªëc_gia cup qu·ªëc_gia xem         29   0.00\n",
      "   33            qu·ªëc_gia cup qu·ªëc_gia xem night         29   0.00\n",
      "   34                cup qu·ªëc_gia xem night wolf         29   0.00\n",
      "   35             qu·ªëc_gia xem night wolf league         29   0.00\n",
      "   36                 xem night wolf league 2023         29   0.00\n",
      "   37                   wolf league 2023 24 ƒë·ªânh         29   0.00\n",
      "   38                   league 2023 24 ƒë·ªânh nh·∫•t         29   0.00\n",
      "   39                      2023 24 ƒë·ªânh nh·∫•t fpt         29   0.00\n",
      "   40                      24 ƒë·ªânh nh·∫•t fpt play         29   0.00\n",
      "   41                    cu·ªëi asian cup 2027 s√¢n         29   0.00\n",
      "   42              l∆∞·ª£t chung_k·∫øt asean cup 2024         28   0.00\n",
      "   43                     qu·∫£ b√≥ng v√†ng vi·ªát nam         28   0.00\n",
      "   44                trung_v·ªá b√πi ho√†ng vi·ªát anh         27   0.00\n",
      "   45      c√¥ng_ty_tnhh truy·ªÅn_h√¨nh fpt fpt play         27   0.00\n",
      "   46                  v√≤ng league 2025 2026 s√¢n         27   0.00\n",
      "   47                      th·∫ßy tr√≤ kim sang sik         26   0.00\n",
      "   48                      ng√†y 12 2024 ƒë·∫øn 2025         26   0.00\n",
      "   49                  hai tr·∫≠n s√¢n_nh√† hai tr·∫≠n         26   0.00\n",
      "   50            tr·∫≠n s√¢n_nh√† hai tr·∫≠n s√¢n_kh√°ch         26   0.00\n",
      "   51                       hai ƒë·ªôi nh·∫•t hai ƒë·ªôi         26   0.00\n",
      "   52                       ƒë·ªôi nh·∫•t hai ƒë·ªôi nh√¨         26   0.00\n",
      "   53              fpt fpt play ƒë∆°n_v·ªã ph√°t_s√≥ng         26   0.00\n",
      "   54        fpt play ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp         26   0.00\n",
      "   55   play ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn         26   0.00\n",
      "   56   ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i         26   0.00\n",
      "   57   ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i league         26   0.00\n",
      "   58        tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i league h·∫°ng         26   0.00\n",
      "   59             tr·ªçn_v·∫πn gi·∫£i league h·∫°ng nh·∫•t         26   0.00\n",
      "   60             gi·∫£i league h·∫°ng nh·∫•t qu·ªëc_gia         26   0.00\n",
      "   61                ƒëi·ªÉm thi_ƒë·∫•u nhi·ªÅu h∆°n tr·∫≠n         25   0.00\n",
      "   62                        10 ƒë·ªôi chia l√†m hai         25   0.00\n",
      "   63                      ƒë·ªôi chia l√†m hai b·∫£ng         25   0.00\n",
      "   64          ra_qu√¢n b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         25   0.00\n",
      "   65                       ƒë√° play off tr·ª• h·∫°ng         25   0.00\n",
      "   66                   t·ªï_ch·ª©c ng√†y 12 2024 ƒë·∫øn         24   0.00\n",
      "   67                        12 2024 ƒë·∫øn 2025 10         24   0.00\n",
      "   68                       2024 ƒë·∫øn 2025 10 ƒë·ªôi         24   0.00\n",
      "   69                       ƒë·∫øn 2025 10 ƒë·ªôi chia         24   0.00\n",
      "   70                       2025 10 ƒë·ªôi chia l√†m         24   0.00\n",
      "   71                      ch·ªçn hai ƒë·ªôi nh·∫•t hai         24   0.00\n",
      "   72                   nh·∫•t hai ƒë·ªôi nh√¨ b√°n_k·∫øt         24   0.00\n",
      "   73                      cup clb ƒë√¥ng nam 2024         24   0.00\n",
      "   74                     clb ƒë√¥ng nam 2024 2025         24   0.00\n",
      "   75             cup qu·ªëc_gia si√™u cup qu·ªëc_gia         24   0.00\n",
      "   76           l∆∞·ª£t ra_qu√¢n b·∫£ng v√≤ng_lo·∫°i cu·ªëi         24   0.00\n",
      "   77                    b·∫£ng asean cup 2024 ·∫£nh         24   0.00\n",
      "   78                 chia_s·∫ª b√†i vi·∫øt b·∫°n trang         24   0.00\n",
      "   79                  b√†i vi·∫øt b·∫°n trang √Ω_ki·∫øn         24   0.00\n",
      "   80                  chia l√†m hai b·∫£ng thi_ƒë·∫•u         23   0.00\n",
      "   81            s√¢n_nh√† hai tr·∫≠n s√¢n_kh√°ch ch·ªçn         23   0.00\n",
      "   82                hai tr·∫≠n s√¢n_kh√°ch ch·ªçn hai         23   0.00\n",
      "   83                tr·∫≠n s√¢n_kh√°ch ch·ªçn hai ƒë·ªôi         23   0.00\n",
      "   84                s√¢n_kh√°ch ch·ªçn hai ƒë·ªôi nh·∫•t         23   0.00\n",
      "   85             c·∫∑p_ƒë·∫•u di·ªÖn hai l∆∞·ª£t th·ªÉ_th·ª©c         23   0.00\n",
      "   86             di·ªÖn hai l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi         23   0.00\n",
      "   87             hai l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh         23   0.00\n",
      "   88            l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t         23   0.00\n",
      "   89             th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t b√†n         23   0.00\n",
      "   90                l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t b√†n th·∫Øng         23   0.00\n",
      "   91              t√≠nh lu·∫≠t b√†n th·∫Øng s√¢n_kh√°ch         23   0.00\n",
      "   92                       vi·ªát tr√¨ ph√∫ th·ªç t·ªëi         22   0.00\n",
      "   93                   l√†m hai b·∫£ng thi_ƒë·∫•u hai         22   0.00\n",
      "   94                  hai b·∫£ng thi_ƒë·∫•u hai tr·∫≠n         22   0.00\n",
      "   95              b·∫£ng thi_ƒë·∫•u hai tr·∫≠n s√¢n_nh√†         22   0.00\n",
      "   96               thi_ƒë·∫•u hai tr·∫≠n s√¢n_nh√† hai         22   0.00\n",
      "   97                   hai ƒë·ªôi nh√¨ b√°n_k·∫øt v√≤ng         22   0.00\n",
      "   98               ƒë·ªôi nh√¨ b√°n_k·∫øt v√≤ng c·∫∑p_ƒë·∫•u         22   0.00\n",
      "   99              nh√¨ b√°n_k·∫øt v√≤ng c·∫∑p_ƒë·∫•u di·ªÖn         22   0.00\n",
      "  100              b√°n_k·∫øt v√≤ng c·∫∑p_ƒë·∫•u di·ªÖn hai         22   0.00\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 5-gram v√†o: d:\\data\\Search_Engine\\outputs\\fivegram_1830docs_20251009_160052.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch 5-gram (Five-gram)\n",
    "df_fivegram = analyze_single_ngram(collection, processor, n=5, n_docs=1830, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6cca3",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 7: T·ªïng h·ª£p t·∫•t c·∫£ N-grams (Optional)\n",
    "\n",
    "N·∫øu mu·ªën l∆∞u t·∫•t c·∫£ n-grams v√†o 1 file duy nh·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0acd4008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ n-grams (1-5) v√†o: d:\\data\\Search_Engine\\outputs\\ngrams_combined_1830docs_20251009_160118.csv\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch v√† l∆∞u t·∫•t c·∫£ n-grams v√†o 1 file\n",
    "df_all_ngrams = analyze_ngrams_combined(collection, processor, n_docs=1830, max_n=5, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cfb6",
   "metadata": {},
   "source": [
    "## 9. Main Execution\n",
    "\n",
    "### Ch·∫°y cell b√™n d∆∞·ªõi ƒë·ªÉ th·ª±c hi·ªán to√†n b·ªô quy tr√¨nh x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b581619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì main function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"B·∫ÆT ƒê·∫¶U X·ª¨ L√ù TEXT T·ª™ MONGODB\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    client, db, collection = connect_to_database()\n",
    "    if collection is None:\n",
    "        print(\"Kh√¥ng th·ªÉ k·∫øt n·ªëi database!\")\n",
    "        return\n",
    "    \n",
    "    # X·ª≠ l√Ω m·ªôt s·ªë documents m·∫´u\n",
    "    results = process_all_documents(collection, limit=5)  \n",
    "    if results:\n",
    "        analyze_results(results)\n",
    "        save_results(results, f\"processed_vnexpress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ª´ v·ª±ng\n",
    "    processor = VietnameseTextProcessor()\n",
    "    analyze_multiple_documents(collection, processor, n=50, top_n=50)\n",
    "    \n",
    "    # Ph√¢n t√≠ch n-grams\n",
    "    analyze_and_save_ngrams(collection, processor, n_docs=1830, max_n=5, top_k=100)\n",
    "\n",
    "    client.close()\n",
    "    print(\"\\n‚úì ƒê√£ ƒë√≥ng k·∫øt n·ªëi database\")\n",
    "    print(\"‚úì HO√ÄN TH√ÄNH!\")\n",
    "\n",
    "print(\"‚úì main function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1427e",
   "metadata": {},
   "source": [
    "### Ch·∫°y to√†n b·ªô quy tr√¨nh\n",
    "\n",
    "Ch·∫°y cell b√™n d∆∞·ªõi ƒë·ªÉ th·ª±c thi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "711ea2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì main function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
      "B·∫ÆT ƒê·∫¶U X·ª¨ L√ù TEXT T·ª™ MONGODB\n",
      "==================================================\n",
      "‚úì Folder outputs: d:\\data\\Search_Engine\\outputs\n",
      "\n",
      "‚úì K·∫øt n·ªëi database th√†nh c√¥ng!\n",
      "  T·ªïng s·ªë documents: 1838\n",
      "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 5 documents...\n",
      "‚úì K·∫øt n·ªëi database th√†nh c√¥ng!\n",
      "  T·ªïng s·ªë documents: 1838\n",
      "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 5 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 29.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ho√†n th√†nh! X·ª≠ l√Ω th√†nh c√¥ng: 5, L·ªói: 0\n",
      "\n",
      "PH√ÇN T√çCH K·∫æT QU·∫¢:\n",
      "T·ªïng s·ªë documents ƒë√£ x·ª≠ l√Ω: 5\n",
      "\n",
      "Th·ªëng k√™ Tokens:\n",
      "  - Trung b√¨nh tokens/document: 727.4\n",
      "  - Trung b√¨nh filtered tokens/document: 541.4\n",
      "  - Trung b√¨nh entities/document: 19.2\n",
      "\n",
      "Top 10 ƒê·ªôi b√≥ng ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\n",
      "  - nam ƒë·ªãnh: 2 l·∫ßn\n",
      "  - tuy·ªÉn vi·ªát nam: 2 l·∫ßn\n",
      "  - h√† n·ªôi fc: 1 l·∫ßn\n",
      "  - ƒë√† n·∫µng: 1 l·∫ßn\n",
      "  - ƒë·ªôi tuy·ªÉn vi·ªát nam: 1 l·∫ßn\n",
      "\n",
      "Top 10 C·∫ßu th·ªß ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\n",
      "  - Kim Sang: 3 l·∫ßn\n",
      "  - Park Hang: 2 l·∫ßn\n",
      "  - tr·∫ª c·∫ßn ƒë∆∞·ª£c thi ƒë·∫•u th∆∞·ªùng xu: 1 l·∫ßn\n",
      "  - nh·∫≠p t·ªãch v√¨ s·ª≠ d·ª•ng gi·∫•y t·ªù g: 1 l·∫ßn\n",
      "  - v√†o s√¢n: 1 l·∫ßn\n",
      "  - b·ªã ph·∫°t: 1 l·∫ßn\n",
      "  - c·ªßa Quy ƒë·ªãnh n√†y c√≥ ƒëo·∫°n: 1 l·∫ßn\n",
      "  - ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω ƒë√∫ng quy ƒë·ªãn: 1 l·∫ßn\n",
      "  - ƒë∆∞·ª£c coi l√† ƒë·ªß t∆∞ c√°ch thi ƒë·∫•u: 1 l·∫ßn\n",
      "  - h·ª£p l·ªá: 1 l·∫ßn\n",
      "\n",
      "Top 10 Gi·∫£i ƒë·∫•u ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu nh·∫•t:\n",
      "  - u23: 4 l·∫ßn\n",
      "  - world cup: 3 l·∫ßn\n",
      "  - asian cup: 3 l·∫ßn\n",
      "  - v-league: 2 l·∫ßn\n",
      "  - champions league: 1 l·∫ßn\n",
      "  - aff cup: 1 l·∫ßn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o d:\\data\\Search_Engine\\outputs\\processed_vnexpress_20251009_160119.json\n",
      "    Word  Freq  r  Pr(%)  r*Pr\n",
      "     ƒë·ªôi   255  1   1.16 0.012\n",
      " c·∫ßu_th·ªß   240  2   1.09 0.022\n",
      "     nam   239  3   1.08 0.033\n",
      "  league   233  4   1.06 0.042\n",
      "    tr·∫≠n   212  5   0.96 0.048\n",
      "     nƒÉm   181  6   0.82 0.049\n",
      "    vi·ªát   180  7   0.82 0.057\n",
      "     hai   174  8   0.79 0.063\n",
      "    b√≥ng   170  9   0.77 0.069\n",
      "     hlv   158 10   0.72 0.072\n",
      "    gi·∫£i   148 11   0.67 0.074\n",
      "     ƒë·∫øn   145 12   0.66 0.079\n",
      "   tr∆∞·ªõc   142 13   0.64 0.084\n",
      "     anh   139 14   0.63 0.088\n",
      "     √¥ng   136 15   0.62 0.093\n",
      "  h√†_n·ªôi   122 16   0.55 0.089\n",
      "     m√πa   120 17   0.54 0.093\n",
      "     clb   119 18   0.54 0.097\n",
      "  nguy·ªÖn   118 19   0.54 0.102\n",
      "     b√†n   117 20   0.53 0.106\n",
      "    ph·∫£i   117 21   0.53 0.112\n",
      "    nh·∫•t   113 22   0.51 0.113\n",
      "     vƒÉn   112 23   0.51 0.117\n",
      "      h·ªç   105 24   0.48 0.114\n",
      "    ng√†y   102 25   0.46 0.116\n",
      "    ch∆°i   101 26   0.46 0.119\n",
      "     h·∫£i   101 27   0.46 0.124\n",
      "   nhi·ªÅu   100 28   0.45 0.127\n",
      "    hagl   100 29   0.45 0.132\n",
      "     h∆°n    97 30   0.44 0.132\n",
      " v√¥_ƒë·ªãch    94 31   0.43 0.132\n",
      "   th·∫Øng    93 32   0.42 0.135\n",
      "    v√≤ng    92 33   0.42 0.138\n",
      " thi_ƒë·∫•u    89 34   0.40 0.137\n",
      "     s√¢n    88 35   0.40 0.140\n",
      "    ƒë·ªãnh    84 36   0.38 0.137\n",
      "      ba    84 37   0.38 0.141\n",
      "vi·ªát_nam    82 38   0.37 0.141\n",
      "    b√¨nh    77 39   0.35 0.136\n",
      "tr·∫≠n_ƒë·∫•u    74 40   0.34 0.134\n",
      "     cup    74 41   0.34 0.138\n",
      "    th√°i    74 42   0.34 0.141\n",
      " b√≥ng_ƒë√°    73 43   0.33 0.142\n",
      "     v·∫´n    69 44   0.31 0.138\n",
      "    ph√∫t    68 45   0.31 0.139\n",
      "     l·∫ßn    68 46   0.31 0.142\n",
      "  c√≥_th·ªÉ    65 47   0.29 0.139\n",
      "     th·ª©    64 48   0.29 0.139\n",
      "      fc    61 49   0.28 0.136\n",
      "   ph√≤ng    61 50   0.28 0.138\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 50 documents (g·ªôp) v√†o: d:\\data\\Search_Engine\\outputs\\wordfreq_50docs_total_20251009_160120.csv\n",
      "\n",
      "================================================================================\n",
      "B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH N-GRAMS\n",
      "================================================================================\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 1-gram...\n",
      "    Word  Freq  r  Pr(%)  r*Pr\n",
      "     ƒë·ªôi   255  1   1.16 0.012\n",
      " c·∫ßu_th·ªß   240  2   1.09 0.022\n",
      "     nam   239  3   1.08 0.033\n",
      "  league   233  4   1.06 0.042\n",
      "    tr·∫≠n   212  5   0.96 0.048\n",
      "     nƒÉm   181  6   0.82 0.049\n",
      "    vi·ªát   180  7   0.82 0.057\n",
      "     hai   174  8   0.79 0.063\n",
      "    b√≥ng   170  9   0.77 0.069\n",
      "     hlv   158 10   0.72 0.072\n",
      "    gi·∫£i   148 11   0.67 0.074\n",
      "     ƒë·∫øn   145 12   0.66 0.079\n",
      "   tr∆∞·ªõc   142 13   0.64 0.084\n",
      "     anh   139 14   0.63 0.088\n",
      "     √¥ng   136 15   0.62 0.093\n",
      "  h√†_n·ªôi   122 16   0.55 0.089\n",
      "     m√πa   120 17   0.54 0.093\n",
      "     clb   119 18   0.54 0.097\n",
      "  nguy·ªÖn   118 19   0.54 0.102\n",
      "     b√†n   117 20   0.53 0.106\n",
      "    ph·∫£i   117 21   0.53 0.112\n",
      "    nh·∫•t   113 22   0.51 0.113\n",
      "     vƒÉn   112 23   0.51 0.117\n",
      "      h·ªç   105 24   0.48 0.114\n",
      "    ng√†y   102 25   0.46 0.116\n",
      "    ch∆°i   101 26   0.46 0.119\n",
      "     h·∫£i   101 27   0.46 0.124\n",
      "   nhi·ªÅu   100 28   0.45 0.127\n",
      "    hagl   100 29   0.45 0.132\n",
      "     h∆°n    97 30   0.44 0.132\n",
      " v√¥_ƒë·ªãch    94 31   0.43 0.132\n",
      "   th·∫Øng    93 32   0.42 0.135\n",
      "    v√≤ng    92 33   0.42 0.138\n",
      " thi_ƒë·∫•u    89 34   0.40 0.137\n",
      "     s√¢n    88 35   0.40 0.140\n",
      "    ƒë·ªãnh    84 36   0.38 0.137\n",
      "      ba    84 37   0.38 0.141\n",
      "vi·ªát_nam    82 38   0.37 0.141\n",
      "    b√¨nh    77 39   0.35 0.136\n",
      "tr·∫≠n_ƒë·∫•u    74 40   0.34 0.134\n",
      "     cup    74 41   0.34 0.138\n",
      "    th√°i    74 42   0.34 0.141\n",
      " b√≥ng_ƒë√°    73 43   0.33 0.142\n",
      "     v·∫´n    69 44   0.31 0.138\n",
      "    ph√∫t    68 45   0.31 0.139\n",
      "     l·∫ßn    68 46   0.31 0.142\n",
      "  c√≥_th·ªÉ    65 47   0.29 0.139\n",
      "     th·ª©    64 48   0.29 0.139\n",
      "      fc    61 49   0.28 0.136\n",
      "   ph√≤ng    61 50   0.28 0.138\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 50 documents (g·ªôp) v√†o: d:\\data\\Search_Engine\\outputs\\wordfreq_50docs_total_20251009_160120.csv\n",
      "\n",
      "================================================================================\n",
      "B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH N-GRAMS\n",
      "================================================================================\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 1-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:26<00:00, 70.16it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 1-GRAM\n",
      "================================================================================\n",
      " Rank    Phrase  Frequency  Pr(%)\n",
      "    1       nam       8286   1.11\n",
      "    2       ƒë·ªôi       7762   1.04\n",
      "    3      tr·∫≠n       7593   1.02\n",
      "    4       hai       6793   0.91\n",
      "    5      vi·ªát       6759   0.90\n",
      "    6   c·∫ßu_th·ªß       6676   0.89\n",
      "    7      b√≥ng       5720   0.77\n",
      "    8       hlv       5611   0.75\n",
      "    9       nƒÉm       5205   0.70\n",
      "   10    league       5194   0.70\n",
      "   11     tr∆∞·ªõc       4817   0.64\n",
      "   12       s√¢n       4738   0.63\n",
      "   13       ƒë·∫øn       4519   0.60\n",
      "   14      ph·∫£i       4331   0.58\n",
      "   15       vƒÉn       3970   0.53\n",
      "   16    nguy·ªÖn       3950   0.53\n",
      "   17       b√†n       3911   0.52\n",
      "   18     th·∫Øng       3867   0.52\n",
      "   19     nhi·ªÅu       3814   0.51\n",
      "   20      ph√∫t       3692   0.49\n",
      "   21       anh       3655   0.49\n",
      "   22      nh·∫•t       3547   0.47\n",
      "   23      v√≤ng       3502   0.47\n",
      "   24       √¥ng       3388   0.45\n",
      "   25  vi·ªát_nam       3356   0.45\n",
      "   26      gi·∫£i       3263   0.44\n",
      "   27        h·ªç       3163   0.42\n",
      "   28    h√†_n·ªôi       3134   0.42\n",
      "   29        ba       2992   0.40\n",
      "   30       h∆°n       2981   0.40\n",
      "   31       cup       2975   0.40\n",
      "   32   b√≥ng_ƒë√°       2948   0.39\n",
      "   33      ƒëi·ªÉm       2922   0.39\n",
      "   34       th·ª©       2919   0.39\n",
      "   35   thi_ƒë·∫•u       2852   0.38\n",
      "   36      ng√†y       2741   0.37\n",
      "   37       m√πa       2623   0.35\n",
      "   38       h·∫£i       2620   0.35\n",
      "   39      ƒë·ªãnh       2620   0.35\n",
      "   40      ch∆°i       2611   0.35\n",
      "   41      thua       2479   0.33\n",
      "   42       clb       2468   0.33\n",
      "   43       ƒë·ª©c       2457   0.33\n",
      "   44        ƒë√°       2415   0.32\n",
      "   45       t√¥i       2415   0.32\n",
      "   46      b√¨nh       2395   0.32\n",
      "   47 ƒë·ªôi_tuy·ªÉn       2358   0.32\n",
      "   48       l·∫ßn       2296   0.31\n",
      "   49       ·∫£nh       2262   0.30\n",
      "   50   v√¥_ƒë·ªãch       2242   0.30\n",
      "   51       nh√†       2239   0.30\n",
      "   52      c√πng       2189   0.29\n",
      "   53      th√°i       2155   0.29\n",
      "   54      hagl       2150   0.29\n",
      "   55       v·∫´n       2025   0.27\n",
      "   56  tr·∫≠n_ƒë·∫•u       2019   0.27\n",
      "   57  ti·ªÅn_ƒë·∫°o       2007   0.27\n",
      "   58      ƒëang       2002   0.27\n",
      "   59       lan       1942   0.26\n",
      "   60        s·ª±       1894   0.25\n",
      "   61      2024       1893   0.25\n",
      "   62      c√¥ng       1808   0.24\n",
      "   63       t·ªët       1805   0.24\n",
      "   64       m·ªõi       1765   0.24\n",
      "   65      b·∫£ng       1750   0.23\n",
      "   66       cao       1744   0.23\n",
      "   67       qua       1741   0.23\n",
      "   68      sang       1683   0.23\n",
      "   69    c√≥_th·ªÉ       1655   0.22\n",
      "   70       l√†m       1617   0.22\n",
      "   71    c∆°_h·ªôi       1602   0.21\n",
      "   72   th·ªß_m√¥n       1582   0.21\n",
      "   73        ƒëi       1568   0.21\n",
      "   74        fc       1567   0.21\n",
      "   75       ƒë·∫ßu       1559   0.21\n",
      "   76      m·∫°nh       1546   0.21\n",
      "   77      ch∆∞a       1539   0.21\n",
      "   78      gi√∫p       1533   0.21\n",
      "   79      ti·∫øn       1503   0.20\n",
      "   80     thanh       1492   0.20\n",
      "   81     ho√†ng       1491   0.20\n",
      "   82      h·∫°ng       1490   0.20\n",
      "   83      h√†ng       1487   0.20\n",
      "   84       n√≥i       1474   0.20\n",
      "   85 tr·ªçng_t√†i       1466   0.20\n",
      "   86      l∆∞·ª£t       1422   0.19\n",
      "   87        12       1412   0.19\n",
      "   88      cu·ªëi       1411   0.19\n",
      "   89     th√°ng       1409   0.19\n",
      "   90        10       1407   0.19\n",
      "   91       r·∫•t       1404   0.19\n",
      "   92      t·ª´ng       1394   0.19\n",
      "   93     quang       1371   0.18\n",
      "   94       g·∫∑p       1370   0.18\n",
      "   95        tp       1339   0.18\n",
      "   96       hcm       1331   0.18\n",
      "   97     khi·∫øn       1325   0.18\n",
      "   98       h√≤a       1323   0.18\n",
      "   99 tuy_nhi√™n       1319   0.18\n",
      "  100       kim       1300   0.17\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 1-gram v√†o: d:\\data\\Search_Engine\\outputs\\unigram_1830docs_20251009_160147.csv\n",
      "\n",
      "\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 2-gram...\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 2-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:22<00:00, 79.62it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 2-GRAM\n",
      "================================================================================\n",
      " Rank           Phrase  Frequency  Pr(%)\n",
      "    1         vi·ªát nam       5322   0.71\n",
      "    2         ƒë·ªôi b√≥ng       2048   0.28\n",
      "    3         th√°i lan       1881   0.25\n",
      "    4         nam ƒë·ªãnh       1767   0.24\n",
      "    5           tp hcm       1335   0.18\n",
      "    6        h·∫£i ph√≤ng       1078   0.14\n",
      "    7       b√¨nh d∆∞∆°ng       1054   0.14\n",
      "    8        h√†_n·ªôi fc       1011   0.14\n",
      "    9          ch·ªß nh√†        985   0.13\n",
      "   10        b√†n th·∫Øng        933   0.13\n",
      "   11          hlv kim        871   0.12\n",
      "   12        asean cup        818   0.11\n",
      "   13         m√πa gi·∫£i        768   0.10\n",
      "   14        b√¨nh ƒë·ªãnh        766   0.10\n",
      "   15          th·ª© hai        749   0.10\n",
      "   16       hi·∫øu l∆∞∆°ng        718   0.10\n",
      "   17        quang h·∫£i        697   0.09\n",
      "   18         v√≤ng c·∫•m        672   0.09\n",
      "   19          aff cup        667   0.09\n",
      "   20     b√≥ng_ƒë√° vi·ªát        665   0.09\n",
      "   21         hi·ªáp hai        655   0.09\n",
      "   22          hai ƒë·ªôi        650   0.09\n",
      "   23        ƒë·ªôi kh√°ch        625   0.08\n",
      "   24         kim sang        616   0.08\n",
      "   25         sang sik        616   0.08\n",
      "   26         hlv park        611   0.08\n",
      "   27        ti·∫øn linh        595   0.08\n",
      "   28        asian cup        589   0.08\n",
      "   29         cup 2024        585   0.08\n",
      "   30         hai tr·∫≠n        580   0.08\n",
      "   31         ƒë√¥ng nam        576   0.08\n",
      "   32         sinh nƒÉm        572   0.08\n",
      "   33       nguy·ªÖn vƒÉn        560   0.08\n",
      "   34         m·ªü t·ª∑_s·ªë        557   0.07\n",
      "   35         th·ªÉ c√¥ng        542   0.07\n",
      "   36       ngo·∫°i binh        525   0.07\n",
      "   37         ƒë·ª©c ƒë·ªìng        519   0.07\n",
      "   38         nh·∫≠t b·∫£n        517   0.07\n",
      "   39      league 2023        509   0.07\n",
      "   40        thanh ho√°        509   0.07\n",
      "   41       tr·∫≠n th·∫Øng        509   0.07\n",
      "   42           th·ª© ba        507   0.07\n",
      "   43         l·ªëi ch∆°i        503   0.07\n",
      "   44         vƒÉn to√†n        498   0.07\n",
      "   45        v√≤ng b·∫£ng        485   0.07\n",
      "   46      c√¥ng ph∆∞·ª£ng        485   0.07\n",
      "   47        h·∫°ng nh·∫•t        475   0.06\n",
      "   48         th·∫ßy tr√≤        464   0.06\n",
      "   49         h√†ng th·ªß        459   0.06\n",
      "   50        2024 2025        453   0.06\n",
      "   51         g·∫ßn nh·∫•t        435   0.06\n",
      "   52         tr·ª• h·∫°ng        427   0.06\n",
      "   53         ƒë·ª©ng th·ª©        425   0.06\n",
      "   54         h√†n qu·ªëc        425   0.06\n",
      "   55         h√†ng ƒë·∫´y        406   0.05\n",
      "   56        vƒÉn quy·∫øt        404   0.05\n",
      "   57         ƒë·∫øn ph√∫t        400   0.05\n",
      "   58     c·∫ßu_th·ªß vi·ªát        397   0.05\n",
      "   59 b√≥ng_ƒë√° vi·ªát_nam        395   0.05\n",
      "   60     cup qu·ªëc_gia        390   0.05\n",
      "   61        ho√†ng ƒë·ª©c        385   0.05\n",
      "   62          vƒÉn l√¢m        372   0.05\n",
      "   63     ch·ª©c v√¥_ƒë·ªãch        371   0.05\n",
      "   64         v∆∞·ª£t qua        371   0.05\n",
      "   65        tr·∫≠n thua        365   0.05\n",
      "   66        ti·∫øn d≈©ng        364   0.05\n",
      "   67          ·∫£nh ƒë·ª©c        363   0.05\n",
      "   68   v√¥_ƒë·ªãch league        361   0.05\n",
      "   69        sea games        358   0.05\n",
      "   70          hai b√†n        358   0.05\n",
      "   71   c√¥ng_an h√†_n·ªôi        356   0.05\n",
      "   72        park hang        354   0.05\n",
      "   73         t·ªët nh·∫•t        353   0.05\n",
      "   74         hang seo        353   0.05\n",
      "   75         duy m·∫°nh        351   0.05\n",
      "   76          ·∫£nh l√¢m        350   0.05\n",
      "   77         s√¢n h√†ng        348   0.05\n",
      "   78     nh√† c·∫ßm_qu√¢n        347   0.05\n",
      "   79        nhi·ªÅu h∆°n        343   0.05\n",
      "   80         b√™n c·∫°nh        340   0.05\n",
      "   81       s√†i_g√≤n fc        339   0.05\n",
      "   82  nguy·ªÖn xu√¢n_son        338   0.05\n",
      "   83          l∆∞·ª£t ƒëi        330   0.04\n",
      "   84         ƒë∆∞a b√≥ng        329   0.04\n",
      "   85      c·∫ßu_th·ªß tr·∫ª        329   0.04\n",
      "   86          ph√∫t b√π        319   0.04\n",
      "   87        ninh b√¨nh        314   0.04\n",
      "   88        2023 2024        313   0.04\n",
      "   89        gi·∫£i h·∫°ng        312   0.04\n",
      "   90      league 2024        310   0.04\n",
      "   91         tu·∫•n h·∫£i        307   0.04\n",
      "   92          ba tr·∫≠n        306   0.04\n",
      "   93          nƒÉm nay        305   0.04\n",
      "   94        vƒÉn thanh        298   0.04\n",
      "   95          ƒë·ªôi ch·ªß        292   0.04\n",
      "   96         covid 19        290   0.04\n",
      "   97          ba ƒëi·ªÉm        289   0.04\n",
      "   98         b√†n thua        288   0.04\n",
      "   99   ban hu·∫•n_luy·ªán        285   0.04\n",
      "  100           clb tp        283   0.04\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 2-gram v√†o: d:\\data\\Search_Engine\\outputs\\bigram_1830docs_20251009_160210.csv\n",
      "\n",
      "\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 3-gram...\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 3-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:22<00:00, 82.75it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 3-GRAM\n",
      "================================================================================\n",
      " Rank                     Phrase  Frequency  Pr(%)\n",
      "    1               kim sang sik        621   0.08\n",
      "    2           b√≥ng_ƒë√° vi·ªát nam        612   0.08\n",
      "    3               hlv kim sang        517   0.07\n",
      "    4             asean cup 2024        475   0.06\n",
      "    5              park hang seo        356   0.05\n",
      "    6           c·∫ßu_th·ªß vi·ªát nam        347   0.05\n",
      "    7               s√¢n h√†ng ƒë·∫´y        343   0.05\n",
      "    8           league 2024 2025        294   0.04\n",
      "    9               ·∫£nh ƒë·ª©c ƒë·ªìng        292   0.04\n",
      "   10                 clb tp hcm        281   0.04\n",
      "   11             ·∫£nh hi·∫øu l∆∞∆°ng        277   0.04\n",
      "   12              hlv park hang        262   0.04\n",
      "   13              b√πi ti·∫øn d≈©ng        259   0.03\n",
      "   14                ƒë·ªôi ch·ªß nh√†        248   0.03\n",
      "   15         ƒë·ªôi_tuy·ªÉn vi·ªát nam        242   0.03\n",
      "   16             tuy·ªÉn vi·ªát nam        241   0.03\n",
      "   17           nguy·ªÖn quang h·∫£i        236   0.03\n",
      "   18               s√¢n vi·ªát tr√¨        226   0.03\n",
      "   19           nguy·ªÖn ti·∫øn linh        226   0.03\n",
      "   20             gi·∫£i h·∫°ng nh·∫•t        224   0.03\n",
      "   21                ƒë·ªó duy m·∫°nh        213   0.03\n",
      "   22               qu·∫ø ng·ªçc h·∫£i        202   0.03\n",
      "   23             asian cup 2027        192   0.03\n",
      "   24           nguy·ªÖn ho√†ng ƒë·ª©c        185   0.02\n",
      "   25              ph·∫°m tu·∫•n h·∫£i        182   0.02\n",
      "   26       di·ªÖn_bi·∫øn ch√≠nh tr·∫≠n        180   0.02\n",
      "   27               th·∫ßy tr√≤ hlv        179   0.02\n",
      "   28           league 2023 2024        179   0.02\n",
      "   29                s√¢n m·ªπ ƒë√¨nh        177   0.02\n",
      "   30               ƒë·∫∑ng vƒÉn l√¢m        170   0.02\n",
      "   31           s√¢n thi√™n tr∆∞·ªùng        158   0.02\n",
      "   32               v≈© vƒÉn thanh        157   0.02\n",
      "   33            nguy·ªÖn vƒÉn to√†n        156   0.02\n",
      "   34              sang hi·ªáp hai        155   0.02\n",
      "   35          ti·ªÅn_ƒë·∫°o sinh nƒÉm        144   0.02\n",
      "   36                ph√∫t b√π th·ª©        142   0.02\n",
      "   37          hlv park hang_seo        142   0.02\n",
      "   38       v√≤ng_lo·∫°i cu·ªëi asian        141   0.02\n",
      "   39             cu·ªëi asian cup        141   0.02\n",
      "   40       afc champions league        140   0.02\n",
      "   41                ƒë·ªôi b√≥ng x·ª©        137   0.02\n",
      "   42             b√πi ho√†ng vi·ªát        136   0.02\n",
      "   43            nguy·ªÖn hai long        136   0.02\n",
      "   44              v≈© ti·∫øn th√†nh        135   0.02\n",
      "   45              vi·ªát nam thua        134   0.02\n",
      "   46               ·∫£nh l√¢m tho·∫£        134   0.02\n",
      "   47              tr·∫≠n vi·ªát nam        132   0.02\n",
      "   48               ƒëua tr·ª• h·∫°ng        132   0.02\n",
      "   49            chu ƒë√¨nh nghi√™m        131   0.02\n",
      "   50              tr·∫≠n g·∫ßn nh·∫•t        130   0.02\n",
      "   51               ƒëo√†n vƒÉn h·∫≠u        129   0.02\n",
      "   52             ho√†ng vi·ªát anh        128   0.02\n",
      "   53               b√†n m·ªü t·ª∑_s·ªë        126   0.02\n",
      "   54                 h·ªì t·∫•n t√†i        124   0.02\n",
      "   55    v√≤ng_lo·∫°i hai world_cup        124   0.02\n",
      "   56               ƒë·ªôi b√≥ng ph·ªë        123   0.02\n",
      "   57      gi·∫£i v√¥_ƒë·ªãch qu·ªëc_gia        118   0.02\n",
      "   58               v≈© h·ªìng vi·ªát        115   0.02\n",
      "   59            ƒë·ªôi b√≥ng th·ªß_ƒë√¥        115   0.02\n",
      "   60               b√≥ng ph·ªë n√∫i        113   0.02\n",
      "   61          nguy·ªÖn ƒë√¨nh tri·ªáu        111   0.01\n",
      "   62            v√¥_ƒë·ªãch aff cup        111   0.01\n",
      "   63         nguy·ªÖn th√†nh chung        110   0.01\n",
      "   64         hai world_cup 2026        108   0.01\n",
      "   65               l√™ hu·ª≥nh ƒë·ª©c        108   0.01\n",
      "   66          vi·ªát_nam th√°i lan        106   0.01\n",
      "   67             asian cup 2023        106   0.01\n",
      "   68                hlv v≈© h·ªìng        106   0.01\n",
      "   69           nguy·ªÖn vƒÉn quy·∫øt        106   0.01\n",
      "   70               ƒë·ªôi b√≥ng hlv        105   0.01\n",
      "   71               ƒë·∫ßu hi·ªáp hai        105   0.01\n",
      "   72               vi·ªát nam vff        105   0.01\n",
      "   73               hlv chu ƒë√¨nh        103   0.01\n",
      "   74               phan vƒÉn ƒë·ª©c        103   0.01\n",
      "   75              mang d√≤ng m√°u        102   0.01\n",
      "   76               aff cup 2018        101   0.01\n",
      "   77             b·∫£ng asean cup        101   0.01\n",
      "   78            khu·∫•t vƒÉn khang        101   0.01\n",
      "   79              do√£n ng·ªçc t√¢n        100   0.01\n",
      "   80                 b√πi vƒ© h√†o        100   0.01\n",
      "   81      hi·ªáu_s·ªë b√†n th·∫Øng_b·∫°i         99   0.01\n",
      "   82               u23 vi·ªát nam         99   0.01\n",
      "   83     chuy√™n_nghi·ªáp vi·ªát nam         99   0.01\n",
      "   84              ƒëo√†n qu√¢n hlv         99   0.01\n",
      "   85           t·∫°o nhi·ªÅu c∆°_h·ªôi         98   0.01\n",
      "   86                ph√∫t b√π gi·ªù         98   0.01\n",
      "   87                hlv v≈© ti·∫øn         97   0.01\n",
      "   88               ƒë·ªôi b√≥ng ƒë·∫•t         96   0.01\n",
      "   89               d√≤ng m√°u lai         96   0.01\n",
      "   90               aff cup 2024         96   0.01\n",
      "   91              2024 2025 s√¢n         96   0.01\n",
      "   92              shin tae yong         95   0.01\n",
      "   93              nguy·ªÖn vƒÉn vƒ©         94   0.01\n",
      "   94                ghi hai b√†n         92   0.01\n",
      "   95             ph·∫°m xu√¢n m·∫°nh         92   0.01\n",
      "   96 b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         91   0.01\n",
      "   97              m·ªü t·ª∑_s·ªë ph√∫t         89   0.01\n",
      "   98             tr∆∞·ªõc vi·ªát nam         88   0.01\n",
      "   99               l·∫ßn g·∫ßn nh·∫•t         88   0.01\n",
      "  100              sinh nƒÉm 1997         87   0.01\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 3-gram v√†o: d:\\data\\Search_Engine\\outputs\\trigram_1830docs_20251009_160233.csv\n",
      "\n",
      "\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 4-gram...\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 4-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:24<00:00, 73.67it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 4-GRAM\n",
      "================================================================================\n",
      " Rank                                Phrase  Frequency  Pr(%)\n",
      "    1                      hlv kim sang sik        517   0.07\n",
      "    2                     hlv park hang seo        261   0.04\n",
      "    3              v√≤ng_lo·∫°i cu·ªëi asian cup        140   0.02\n",
      "    4                    b√πi ho√†ng vi·ªát anh        126   0.02\n",
      "    5                   cu·ªëi asian cup 2027        120   0.02\n",
      "    6                      ƒë·ªôi b√≥ng ph·ªë n√∫i        112   0.02\n",
      "    7                      hlv v≈© h·ªìng vi·ªát        105   0.01\n",
      "    8                   hlv chu ƒë√¨nh nghi√™m        103   0.01\n",
      "    9          v√≤ng_lo·∫°i hai world_cup 2026        102   0.01\n",
      "   10                     hlv v≈© ti·∫øn th√†nh         98   0.01\n",
      "   11        b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam         92   0.01\n",
      "   12                     mang d√≤ng m√°u lai         89   0.01\n",
      "   13                  league 2024 2025 s√¢n         79   0.01\n",
      "   14                      vi·ªát tr√¨ ph√∫ th·ªç         78   0.01\n",
      "   15            li√™n_ƒëo√†n b√≥ng_ƒë√° vi·ªát nam         77   0.01\n",
      "   16                   b·∫£ng asean cup 2024         75   0.01\n",
      "   17 c√¥ng_ty c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp         72   0.01\n",
      "   18    c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         72   0.01\n",
      "   19                 c·∫ßu_th·ªß mang d√≤ng m√°u         71   0.01\n",
      "   20             b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         71   0.01\n",
      "   21            chuy√™n_nghi·ªáp vi·ªát nam vpf         69   0.01\n",
      "   22                   2024 ·∫£nh hi·∫øu l∆∞∆°ng         68   0.01\n",
      "   23              chung_k·∫øt asean cup 2024         68   0.01\n",
      "   24                      s√¢n vi·ªát tr√¨ ph√∫         67   0.01\n",
      "   25             th·ªß_m√¥n nguy·ªÖn ƒë√¨nh tri·ªáu         64   0.01\n",
      "   26                night wolf league 2023         63   0.01\n",
      "   27                     hlv shin tae yong         62   0.01\n",
      "   28                    l√™ ph·∫°m th√†nh long         61   0.01\n",
      "   29                 nguy·ªÖn phong h·ªìng duy         61   0.01\n",
      "   30                    ƒë·ªôi b√≥ng th√†nh nam         59   0.01\n",
      "   31                  hlv nguy·ªÖn ƒë·ª©c th·∫Øng         59   0.01\n",
      "   32                     ƒë·ªôi b√≥ng x·ª© thanh         57   0.01\n",
      "   33              afc champions league two         55   0.01\n",
      "   34                 nh√† c·∫ßm_qu√¢n h√†n qu·ªëc         55   0.01\n",
      "   35              ti·ªÅn_v·ªá nguy·ªÖn quang h·∫£i         54   0.01\n",
      "   36                   hlv phan thanh h√πng         54   0.01\n",
      "   37                     lƒëbƒë vi·ªát nam vff         54   0.01\n",
      "   38                     cu·ªôc ƒëua tr·ª• h·∫°ng         53   0.01\n",
      "   39           world_cup 2026 khu_v·ª±c ch√¢u         52   0.01\n",
      "   40                    asean cup 2024 ·∫£nh         52   0.01\n",
      "   41                      hlv l√™ hu·ª≥nh ƒë·ª©c         51   0.01\n",
      "   42                    ghi nhi·ªÅu b√†n nh·∫•t         51   0.01\n",
      "   43                ti·ªÅn_ƒë·∫°o sinh nƒÉm 1997         50   0.01\n",
      "   44    ti·ªÅn_ƒë·∫°o nh·∫≠p_t·ªãch nguy·ªÖn xu√¢n_son         50   0.01\n",
      "   45                   s√¢n h√†ng ƒë·∫´y h√†_n·ªôi         49   0.01\n",
      "   46                      cup clb ƒë√¥ng nam         49   0.01\n",
      "   47                   v√≤ng b·∫£ng asean cup         49   0.01\n",
      "   48                     cup nay asean cup         49   0.01\n",
      "   49                      s√¢n h√†ng ƒë·∫´y t·ªëi         48   0.01\n",
      "   50                  th·ªß_m√¥n ƒë·∫∑ng vƒÉn l√¢m         48   0.01\n",
      "   51                   v√≤ng b·∫£ng asian cup         47   0.01\n",
      "   52                     aff cup nay asean         47   0.01\n",
      "   53                      ƒë·ªôi b√≥ng x·ª© ngh·ªá         46   0.01\n",
      "   54                  b√≥ng_ƒë√° vi·ªát nam vff         45   0.01\n",
      "   55                 th·ªß_m√¥n b√πi ti·∫øn d≈©ng         45   0.01\n",
      "   56                      th·∫ßy tr√≤ hlv kim         44   0.01\n",
      "   57                     play off tr·ª• h·∫°ng         44   0.01\n",
      "   58                 hlv tr∆∞∆°ng vi·ªát ho√†ng         44   0.01\n",
      "   59                     m·ª´ng b√†n m·ªü t·ª∑_s·ªë         44   0.01\n",
      "   60                v√¥_ƒë·ªãch asean cup 2024         43   0.01\n",
      "   61                 nh√† c·∫ßm_qu√¢n sinh nƒÉm         43   0.01\n",
      "   62                  league 2024 2025 ·∫£nh         43   0.01\n",
      "   63                     v√≤ng b·∫£ng aff cup         43   0.01\n",
      "   64                  v√¥_ƒë·ªãch aff cup 2018         42   0.01\n",
      "   65           world_cup 2022 khu_v·ª±c ch√¢u         42   0.01\n",
      "   66                    b√†n th·∫Øng ƒë·∫πp nh·∫•t         42   0.01\n",
      "   67                ti·ªÅn_ƒë·∫°o ph·∫°m tu·∫•n h·∫£i         41   0.01\n",
      "   68                     th√°i lan vi·ªát nam         41   0.01\n",
      "   69             ti·ªÅn_ƒë·∫°o nguy·ªÖn ti·∫øn linh         41   0.01\n",
      "   70               gi·∫£i h·∫°ng nh·∫•t qu·ªëc_gia         41   0.01\n",
      "   71                   b·∫£ng asian cup 2023         41   0.01\n",
      "   72                     hlv ph·∫°m minh ƒë·ª©c         40   0.01\n",
      "   73                b√°n_k·∫øt asean cup 2024         39   0.01\n",
      "   74                    th·ªùi hlv park hang         39   0.01\n",
      "   75                     hc v√†ng sea games         38   0.01\n",
      "   76                      m√πa gi·∫£i nƒÉm nay         38   0.01\n",
      "   77              th·ªß_m√¥n tr·∫ßn nguy√™n m·∫°nh         38   0.01\n",
      "   78                ƒë·ªôi_tr∆∞·ªüng ƒë·ªó duy m·∫°nh         38   0.01\n",
      "   79              ti·ªÅn_v·ªá nguy·ªÖn ho√†ng ƒë·ª©c         37   0.00\n",
      "   80                trung_v·ªá b√πi ti·∫øn d≈©ng         37   0.00\n",
      "   81                       ƒë·∫øn ph√∫t b√π th·ª©         37   0.00\n",
      "   82                 v√≤ng league 2025 2026         37   0.00\n",
      "   83              lu·∫≠t b√†n th·∫Øng s√¢n_kh√°ch         36   0.00\n",
      "   84               v√≤ng chung_k·∫øt u23 ch√¢u         36   0.00\n",
      "   85                    tr·∫≠n vi·ªát nam thua         36   0.00\n",
      "   86                      tr√≤ hlv kim sang         35   0.00\n",
      "   87                      ph·∫£i ƒë√° play off         35   0.00\n",
      "   88                    v√µ ho√†ng minh khoa         35   0.00\n",
      "   89               l∆∞·ª£t hai b·∫£ng v√≤ng_lo·∫°i         35   0.00\n",
      "   90               hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi         35   0.00\n",
      "   91                     s√¢n h√†ng ƒë·∫´y ng√†y         35   0.00\n",
      "   92                   wolf league 2023 24         34   0.00\n",
      "   93           v√≤ng_lo·∫°i ba world_cup 2026         34   0.00\n",
      "   94                 tr·∫≠n_ƒë·∫•u s√¢n h√†ng ƒë·∫´y         34   0.00\n",
      "   95                     ƒë·ªôi b√≥ng ƒë·∫•t c·∫£ng         34   0.00\n",
      "   96                    asean cup 2024 s√¢n         33   0.00\n",
      "   97            hai world_cup 2026 khu_v·ª±c         33   0.00\n",
      "   98             th·ªß_t∆∞·ªõng ph·∫°m minh ch√≠nh         33   0.00\n",
      "   99                     2025 s√¢n h√†ng ƒë·∫´y         33   0.00\n",
      "  100                   l∆∞·ª£t cu·ªëi v√≤ng b·∫£ng         33   0.00\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 4-gram v√†o: d:\\data\\Search_Engine\\outputs\\fourgram_1830docs_20251009_160259.csv\n",
      "\n",
      "\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 5-gram...\n",
      "\n",
      "ƒêang x·ª≠ l√Ω 1830 documents cho 5-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:24<00:00, 73.41it/s] \n",
      "Processing 5-gram: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1830/1830 [00:24<00:00, 73.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 100 5-GRAM\n",
      "================================================================================\n",
      " Rank                                     Phrase  Frequency  Pr(%)\n",
      "    1              v√≤ng_lo·∫°i cu·ªëi asian cup 2027        121   0.02\n",
      "    2              b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian cup         72   0.01\n",
      "    3 c√¥ng_ty c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát         71   0.01\n",
      "    4     c·ªï_ph·∫ßn b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam         71   0.01\n",
      "    5         b√≥ng_ƒë√° chuy√™n_nghi·ªáp vi·ªát nam vpf         68   0.01\n",
      "    6                       s√¢n vi·ªát tr√¨ ph√∫ th·ªç         67   0.01\n",
      "    7                  c·∫ßu_th·ªß mang d√≤ng m√°u lai         64   0.01\n",
      "    8                      aff cup nay asean cup         47   0.01\n",
      "    9             li√™n_ƒëo√†n b√≥ng_ƒë√° vi·ªát nam vff         43   0.01\n",
      "   10                     th·ªùi hlv park hang seo         39   0.01\n",
      "   11                  night wolf league 2023 24         35   0.00\n",
      "   12                      th·∫ßy tr√≤ hlv kim sang         35   0.00\n",
      "   13                       tr√≤ hlv kim sang sik         35   0.00\n",
      "   14               l∆∞·ª£t hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi         35   0.00\n",
      "   15              hai b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         35   0.00\n",
      "   16            hai world_cup 2026 khu_v·ª±c ch√¢u         33   0.00\n",
      "   17                    cup 2024 ·∫£nh hi·∫øu l∆∞∆°ng         33   0.00\n",
      "   18                   v√≤ng b·∫£ng asean cup 2024         32   0.00\n",
      "   19                   v√≤ng b·∫£ng asian cup 2023         32   0.00\n",
      "   20                     2024 2025 s√¢n h√†ng ƒë·∫´y         31   0.00\n",
      "   21            h·∫°ng nh·∫•t qu·ªëc_gia cup qu·ªëc_gia         31   0.00\n",
      "   22                  fpt play https fptplay vn         31   0.00\n",
      "   23                   s√¢n h√†ng ƒë·∫´y h√†_n·ªôi ng√†y         31   0.00\n",
      "   24                    asean cup 2024 ·∫£nh hi·∫øu         31   0.00\n",
      "   25                night wolf league 2023 2024         30   0.00\n",
      "   26              league h·∫°ng nh·∫•t qu·ªëc_gia cup         30   0.00\n",
      "   27                   ƒë·ªânh nh·∫•t fpt play https         30   0.00\n",
      "   28                nh·∫•t fpt play https fptplay         30   0.00\n",
      "   29       v√≤ng_lo·∫°i hai world_cup 2026 khu_v·ª±c         30   0.00\n",
      "   30                  league 2024 2025 s√¢n h√†ng         29   0.00\n",
      "   31            truy·ªÅn_h√¨nh fpt fpt play ƒë∆°n_v·ªã         29   0.00\n",
      "   32             nh·∫•t qu·ªëc_gia cup qu·ªëc_gia xem         29   0.00\n",
      "   33            qu·ªëc_gia cup qu·ªëc_gia xem night         29   0.00\n",
      "   34                cup qu·ªëc_gia xem night wolf         29   0.00\n",
      "   35             qu·ªëc_gia xem night wolf league         29   0.00\n",
      "   36                 xem night wolf league 2023         29   0.00\n",
      "   37                   wolf league 2023 24 ƒë·ªânh         29   0.00\n",
      "   38                   league 2023 24 ƒë·ªânh nh·∫•t         29   0.00\n",
      "   39                      2023 24 ƒë·ªânh nh·∫•t fpt         29   0.00\n",
      "   40                      24 ƒë·ªânh nh·∫•t fpt play         29   0.00\n",
      "   41              l∆∞·ª£t chung_k·∫øt asean cup 2024         29   0.00\n",
      "   42                    cu·ªëi asian cup 2027 s√¢n         29   0.00\n",
      "   43                     qu·∫£ b√≥ng v√†ng vi·ªát nam         28   0.00\n",
      "   44                trung_v·ªá b√πi ho√†ng vi·ªát anh         27   0.00\n",
      "   45      c√¥ng_ty_tnhh truy·ªÅn_h√¨nh fpt fpt play         27   0.00\n",
      "   46                  v√≤ng league 2025 2026 s√¢n         27   0.00\n",
      "   47                      ng√†y 12 2024 ƒë·∫øn 2025         26   0.00\n",
      "   48                  hai tr·∫≠n s√¢n_nh√† hai tr·∫≠n         26   0.00\n",
      "   49            tr·∫≠n s√¢n_nh√† hai tr·∫≠n s√¢n_kh√°ch         26   0.00\n",
      "   50                       hai ƒë·ªôi nh·∫•t hai ƒë·ªôi         26   0.00\n",
      "   51                       ƒë·ªôi nh·∫•t hai ƒë·ªôi nh√¨         26   0.00\n",
      "   52              fpt fpt play ƒë∆°n_v·ªã ph√°t_s√≥ng         26   0.00\n",
      "   53        fpt play ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp         26   0.00\n",
      "   54   play ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn         26   0.00\n",
      "   55   ƒë∆°n_v·ªã ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i         26   0.00\n",
      "   56   ph√°t_s√≥ng tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i league         26   0.00\n",
      "   57        tr·ª±c_ti·∫øp tr·ªçn_v·∫πn gi·∫£i league h·∫°ng         26   0.00\n",
      "   58             tr·ªçn_v·∫πn gi·∫£i league h·∫°ng nh·∫•t         26   0.00\n",
      "   59             gi·∫£i league h·∫°ng nh·∫•t qu·ªëc_gia         26   0.00\n",
      "   60                      th·∫ßy tr√≤ kim sang sik         26   0.00\n",
      "   61                        10 ƒë·ªôi chia l√†m hai         25   0.00\n",
      "   62                      ƒë·ªôi chia l√†m hai b·∫£ng         25   0.00\n",
      "   63                       ƒë√° play off tr·ª• h·∫°ng         25   0.00\n",
      "   64                ƒëi·ªÉm thi_ƒë·∫•u nhi·ªÅu h∆°n tr·∫≠n         25   0.00\n",
      "   65          ra_qu√¢n b·∫£ng v√≤ng_lo·∫°i cu·ªëi asian         25   0.00\n",
      "   66                   t·ªï_ch·ª©c ng√†y 12 2024 ƒë·∫øn         24   0.00\n",
      "   67                        12 2024 ƒë·∫øn 2025 10         24   0.00\n",
      "   68                       2024 ƒë·∫øn 2025 10 ƒë·ªôi         24   0.00\n",
      "   69                       ƒë·∫øn 2025 10 ƒë·ªôi chia         24   0.00\n",
      "   70                       2025 10 ƒë·ªôi chia l√†m         24   0.00\n",
      "   71                      ch·ªçn hai ƒë·ªôi nh·∫•t hai         24   0.00\n",
      "   72                   nh·∫•t hai ƒë·ªôi nh√¨ b√°n_k·∫øt         24   0.00\n",
      "   73             cup qu·ªëc_gia si√™u cup qu·ªëc_gia         24   0.00\n",
      "   74                      cup clb ƒë√¥ng nam 2024         24   0.00\n",
      "   75                     clb ƒë√¥ng nam 2024 2025         24   0.00\n",
      "   76           l∆∞·ª£t ra_qu√¢n b·∫£ng v√≤ng_lo·∫°i cu·ªëi         24   0.00\n",
      "   77                    b·∫£ng asean cup 2024 ·∫£nh         24   0.00\n",
      "   78            s√¢n_nh√† hai tr·∫≠n s√¢n_kh√°ch ch·ªçn         23   0.00\n",
      "   79                hai tr·∫≠n s√¢n_kh√°ch ch·ªçn hai         23   0.00\n",
      "   80                tr·∫≠n s√¢n_kh√°ch ch·ªçn hai ƒë·ªôi         23   0.00\n",
      "   81                s√¢n_kh√°ch ch·ªçn hai ƒë·ªôi nh·∫•t         23   0.00\n",
      "   82                  chia l√†m hai b·∫£ng thi_ƒë·∫•u         23   0.00\n",
      "   83             c·∫∑p_ƒë·∫•u di·ªÖn hai l∆∞·ª£t th·ªÉ_th·ª©c         23   0.00\n",
      "   84             di·ªÖn hai l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi         23   0.00\n",
      "   85             hai l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh         23   0.00\n",
      "   86            l∆∞·ª£t th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t         23   0.00\n",
      "   87             th·ªÉ_th·ª©c l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t b√†n         23   0.00\n",
      "   88                l∆∞·ª£t_ƒëi t√≠nh lu·∫≠t b√†n th·∫Øng         23   0.00\n",
      "   89              t√≠nh lu·∫≠t b√†n th·∫Øng s√¢n_kh√°ch         23   0.00\n",
      "   90                 chia_s·∫ª b√†i vi·∫øt b·∫°n trang         23   0.00\n",
      "   91                  b√†i vi·∫øt b·∫°n trang √Ω_ki·∫øn         23   0.00\n",
      "   92                       vi·ªát tr√¨ ph√∫ th·ªç t·ªëi         22   0.00\n",
      "   93   nh·∫•t_thi·∫øt tr√πng quan_ƒëi·ªÉm vnexpress net         22   0.00\n",
      "   94                    ƒë·ªïi t√™n th√†nh asean cup         22   0.00\n",
      "   95                   l√†m hai b·∫£ng thi_ƒë·∫•u hai         22   0.00\n",
      "   96                  hai b·∫£ng thi_ƒë·∫•u hai tr·∫≠n         22   0.00\n",
      "   97              b·∫£ng thi_ƒë·∫•u hai tr·∫≠n s√¢n_nh√†         22   0.00\n",
      "   98               thi_ƒë·∫•u hai tr·∫≠n s√¢n_nh√† hai         22   0.00\n",
      "   99                   hai ƒë·ªôi nh√¨ b√°n_k·∫øt v√≤ng         22   0.00\n",
      "  100               ƒë·ªôi nh√¨ b√°n_k·∫øt v√≤ng c·∫∑p_ƒë·∫•u         22   0.00\n",
      "\n",
      "‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ 5-gram v√†o: d:\\data\\Search_Engine\\outputs\\fivegram_1830docs_20251009_160325.csv\n",
      "\n",
      "\n",
      "\n",
      "‚úì ƒê√£ ƒë√≥ng k·∫øt n·ªëi database\n",
      "‚úì HO√ÄN TH√ÄNH!\n",
      "\n",
      "üìÅ T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o folder: d:\\data\\Search_Engine\\outputs\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"B·∫ÆT ƒê·∫¶U X·ª¨ L√ù TEXT T·ª™ MONGODB\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # T·∫°o folder outputs\n",
    "    output_dir = os.path.join(\"d:\\\\data\\\\Search_Engine\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"‚úì Folder outputs: {output_dir}\\n\")\n",
    "    \n",
    "    client, db, collection = connect_to_database()\n",
    "    if collection is None:\n",
    "        print(\"Kh√¥ng th·ªÉ k·∫øt n·ªëi database!\")\n",
    "        return\n",
    "    \n",
    "    # X·ª≠ l√Ω m·ªôt s·ªë documents m·∫´u\n",
    "    results = process_all_documents(collection, limit=5)  \n",
    "    if results:\n",
    "        analyze_results(results)\n",
    "        save_results(results, f\"processed_vnexpress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ª´ v·ª±ng\n",
    "    processor = VietnameseTextProcessor()\n",
    "    analyze_multiple_documents(collection, processor, n=50, top_n=50)\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ª´ng lo·∫°i n-gram ri√™ng bi·ªát\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH N-GRAMS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for n in range(1, 6):\n",
    "        analyze_single_ngram(collection, processor, n=n, n_docs=1830, top_k=100)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    client.close()\n",
    "    print(\"\\n‚úì ƒê√£ ƒë√≥ng k·∫øt n·ªëi database\")\n",
    "    print(\"‚úì HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"\\nüìÅ T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o folder: {output_dir}\")\n",
    "\n",
    "print(\"‚úì main function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
    "\n",
    "# Ch·∫°y to√†n b·ªô quy tr√¨nh x·ª≠ l√Ω\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
