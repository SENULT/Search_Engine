{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6309750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PyVi available\n",
      "âœ— Gensim not available. Install: pip install gensim\n",
      "PyTorch version: 2.3.1+cpu\n",
      "CUDA available: False\n",
      "\n",
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    PYVI_AVAILABLE = True\n",
    "    print(\"âœ“ PyVi available\")\n",
    "except ImportError:\n",
    "    print(\"âœ— PyVi not available. Install: pip install pyvi\")\n",
    "    PYVI_AVAILABLE = False\n",
    "\n",
    "# Word embeddings\n",
    "try:\n",
    "    from gensim.models import Word2Vec, KeyedVectors\n",
    "    GENSIM_AVAILABLE = True\n",
    "    print(\"âœ“ Gensim available\")\n",
    "except ImportError:\n",
    "    print(\"âœ— Gensim not available. Install: pip install gensim\")\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"\\nâœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd3c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Word2Vec training function ready\n"
     ]
    }
   ],
   "source": [
    "def train_word2vec(documents, processor, embedding_dim=100, save_path='word2vec_vi.model'):\n",
    "    \"\"\"Train Word2Vec embeddings on corpus\"\"\"\n",
    "    \n",
    "    if GENSIM_AVAILABLE:\n",
    "        print(\"\\n Training Word2Vec embeddings...\")\n",
    "        \n",
    "        # Prepare sentences\n",
    "        sentences = []\n",
    "        for doc in tqdm(documents, desc=\"Preparing sentences\"):\n",
    "            title = doc.get('title', '')\n",
    "            content = doc.get('content', '')\n",
    "            full_text = f\"{title} {content}\"\n",
    "            tokens = processor.preprocess(full_text)\n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "        \n",
    "        # Train Word2Vec\n",
    "        model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=embedding_dim,\n",
    "            window=5,\n",
    "            min_count=2,\n",
    "            workers=4,\n",
    "            sg=1,  # Skip-gram\n",
    "            epochs=10\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model.save(save_path)\n",
    "        print(f\"âœ“ Word2Vec trained and saved to {save_path}\")\n",
    "        print(f\"  - Vocabulary size: {len(model.wv)}\")\n",
    "        print(f\"  - Embedding dimension: {embedding_dim}\")\n",
    "        \n",
    "        return model\n",
    "    else:\n",
    "        print(\"âœ— Gensim not available. Cannot train Word2Vec.\")\n",
    "        return None\n",
    "\n",
    "# Train Word2Vec (optional - can skip if you already have embeddings)\n",
    "# Uncomment to train:\n",
    "# w2v_model = train_word2vec(documents, processor, embedding_dim=100)\n",
    "\n",
    "print(\"âœ“ Word2Vec training function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff60652",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Build vocabulary\u001b[39;00m\n\u001b[0;32m     52\u001b[0m vocab \u001b[38;5;241m=\u001b[39m Vocabulary(min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m vocab\u001b[38;5;241m.\u001b[39mbuild_vocab(documents, processor)\n\u001b[0;32m     55\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab\u001b[38;5;241m.\u001b[39mword2idx)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Vocabulary ready: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Build vocabulary from corpus\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=2):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocab(self, documents, processor):\n",
    "        \"\"\"Build vocabulary from documents\"\"\"\n",
    "        print(\"\\n Building vocabulary...\")\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for doc in tqdm(documents, desc=\"Counting words\"):\n",
    "            title = doc.get('title', '')\n",
    "            content = doc.get('content', '')\n",
    "            full_text = f\"{title} {content}\"\n",
    "            tokens = processor.preprocess(full_text)\n",
    "            self.word_freq.update(tokens)\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        idx = 2  # Start after PAD and UNK\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"âœ“ Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"  - Total unique words: {len(self.word_freq)}\")\n",
    "        print(f\"  - Words with freq >= {self.min_freq}: {len(self.word2idx) - 2}\")\n",
    "        print(f\"  - Top 10 words: {self.word_freq.most_common(10)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def encode(self, tokens, max_len=None):\n",
    "        \"\"\"Convert tokens to indices\"\"\"\n",
    "        indices = [self.word2idx.get(token, 1) for token in tokens]  # 1 = UNK\n",
    "        if max_len:\n",
    "            if len(indices) < max_len:\n",
    "                indices += [0] * (max_len - len(indices))  # 0 = PAD\n",
    "            else:\n",
    "                indices = indices[:max_len]\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert indices back to tokens\"\"\"\n",
    "        return [self.idx2word.get(idx, '<UNK>') for idx in indices]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocab(documents, processor)\n",
    "\n",
    "vocab_size = len(vocab.word2idx)\n",
    "print(f\"\\nâœ… Vocabulary ready: {vocab_size} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81940c5d",
   "metadata": {},
   "source": [
    "## 4. Build Vocabulary & Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37654ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading documents from JSON files...\n",
      "  âœ— File not found: vnexpressT_bongda_part1.json\n",
      "  âœ— File not found: vnexpressT_bongda_part2.json\n",
      "  âœ— File not found: vnexpressT_bongda_part3.json\n",
      "  âœ— File not found: vnexpressT_bongda_part4.json\n",
      "\n",
      "âœ“ Total documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "def load_documents(json_files=None):\n",
    "    \"\"\"Load documents from JSON files\"\"\"\n",
    "    if json_files is None:\n",
    "        json_files = [\n",
    "            \"vnexpressT_bongda_part1.json\",\n",
    "            \"vnexpressT_bongda_part2.json\",\n",
    "            \"vnexpressT_bongda_part3.json\",\n",
    "            \"vnexpressT_bongda_part4.json\"\n",
    "        ]\n",
    "    \n",
    "    documents = []\n",
    "    print(\" Loading documents from JSON files...\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        documents.extend(data)\n",
    "                    print(f\"  âœ“ Loaded {file_path}: {len(data)} documents\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âœ— Error reading {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"  âœ— File not found: {file_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Total documents loaded: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load data\n",
    "documents = load_documents()\n",
    "\n",
    "# Show sample document\n",
    "if documents:\n",
    "    print(\"\\n Sample document:\")\n",
    "    sample = documents[0]\n",
    "    print(f\"Title: {sample.get('title', 'N/A')[:100]}\")\n",
    "    print(f\"Content: {sample.get('content', 'N/A')[:200]}...\")\n",
    "    print(f\"Date: {sample.get('date', 'N/A')}\")\n",
    "    print(f\"Author: {sample.get('author', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04a190",
   "metadata": {},
   "source": [
    "## 3. Load Vietnamese Football Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f264a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ VietnameseTextProcessor initialized\n"
     ]
    }
   ],
   "source": [
    "class VietnameseTextProcessor:\n",
    "    \"\"\"Vietnamese text processing for neural ranking\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Vietnamese stopwords\n",
    "        self.stop_words = set([\n",
    "            'vÃ ', 'cá»§a', 'trong', 'vá»›i', 'lÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'cho', 'tá»«', 'má»™t', 'cÃ¡c',\n",
    "            'Ä‘á»ƒ', 'khÃ´ng', 'sáº½', 'Ä‘Ã£', 'vá»', 'hay', 'theo', 'nhÆ°', 'cÅ©ng', 'nÃ y', 'Ä‘Ã³',\n",
    "            'khi', 'nhá»¯ng', 'táº¡i', 'sau', 'bá»‹', 'giá»¯a', 'trÃªn', 'dÆ°á»›i', 'ngoÃ i',\n",
    "            'thÃ¬', 'nhÆ°ng', 'mÃ ', 'hoáº·c', 'náº¿u', 'vÃ¬', 'do', 'nÃªn', 'rá»“i', 'cÃ²n', 'Ä‘á»u',\n",
    "            'chá»‰', 'viá»‡c', 'ngÆ°á»i', 'láº¡i', 'Ä‘Ã¢y', 'Ä‘áº¥y', 'á»Ÿ', 'ra', 'vÃ o', 'lÃªn', 'xuá»‘ng'\n",
    "        ])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Keep Vietnamese characters, letters, numbers\n",
    "        text = re.sub(r'[^\\w\\sÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘Ä]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize Vietnamese text\"\"\"\n",
    "        if PYVI_AVAILABLE:\n",
    "            try:\n",
    "                return ViTokenizer.tokenize(text).split()\n",
    "            except:\n",
    "                pass\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "    \n",
    "    def preprocess(self, text, remove_stop=True):\n",
    "        \"\"\"Full preprocessing pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        if remove_stop:\n",
    "            tokens = self.remove_stopwords(tokens)\n",
    "        return tokens\n",
    "\n",
    "processor = VietnameseTextProcessor()\n",
    "print(\"âœ“ VietnameseTextProcessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07aa06",
   "metadata": {},
   "source": [
    "## 2. Vietnamese Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce5826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PyVi available\n",
      "âœ— Gensim not available. Install: pip install gensim\n",
      "PyTorch version: 2.3.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    PYVI_AVAILABLE = True\n",
    "    print(\"âœ“ PyVi available\")\n",
    "except ImportError:\n",
    "    print(\"âœ— PyVi not available. Install: pip install pyvi\")\n",
    "    PYVI_AVAILABLE = False\n",
    "\n",
    "# Word embeddings\n",
    "try:\n",
    "    from gensim.models import Word2Vec, KeyedVectors\n",
    "    GENSIM_AVAILABLE = True\n",
    "    print(\"âœ“ Gensim available\")\n",
    "except ImportError:\n",
    "    print(\"âœ— Gensim not available. Install: pip install gensim\")\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bb699",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ae8b8",
   "metadata": {},
   "source": [
    "# DeepCT + Conv-KNRM for Vietnamese Football Search\n",
    "\n",
    "## ğŸ“‹ Overview\n",
    "Implementation of DeepCT (Deep Contextualized Term weighting) combined with Conv-KNRM (Convolutional Kernel-based Neural Ranking Model) for Vietnamese information retrieval.\n",
    "\n",
    "### ğŸ¯ Components:\n",
    "1. **DeepCT**: Neural term weighting for document representation\n",
    "2. **Conv-KNRM**: Convolutional neural ranking model\n",
    "3. **Vietnamese Text Processing**: Tokenization, stopwords, embeddings\n",
    "4. **Training Pipeline**: Query-document pairs with relevance labels\n",
    "\n",
    "### ğŸ“Š Dataset:\n",
    "- Vietnamese Football News from VnExpress (1830+ documents)\n",
    "- Query generation from titles and content\n",
    "- BM25 baseline for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ca7e2",
   "metadata": {},
   "source": [
    "## 5. DeepCT Model (Deep Contextualized Term Weighting)\n",
    "\n",
    "DeepCT predicts term importance weights for documents using BERT-like contextualized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DeepCT model defined (FIXED: regression output)\n"
     ]
    }
   ],
   "source": [
    "class DeepCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Contextualized Term weighting model\n",
    "    Predicts term importance weights for document terms\n",
    "    \n",
    "    FIXED: Changed output to regression (not sigmoid) for better term weighting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, dropout=0.3, max_weight=100):\n",
    "        super(DeepCT, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.max_weight = max_weight\n",
    "        \n",
    "        # Bidirectional LSTM for contextualization\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Term weight prediction head\n",
    "        # FIXED: Remove Sigmoid, use ReLU + clipping for term weights [0, max_weight]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # No sigmoid - output raw regression values\n",
    "        )\n",
    "        \n",
    "    def forward(self, doc_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            doc_indices: [batch_size, doc_len]\n",
    "        Returns:\n",
    "            term_weights: [batch_size, doc_len] - values in [0, max_weight]\n",
    "        \"\"\"\n",
    "        # Embedding: [batch_size, doc_len, embedding_dim]\n",
    "        embedded = self.embedding(doc_indices)\n",
    "        \n",
    "        # LSTM: [batch_size, doc_len, hidden_dim*2]\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Predict term weights: [batch_size, doc_len, 1] -> [batch_size, doc_len]\n",
    "        term_weights = self.fc(lstm_out).squeeze(-1)\n",
    "        \n",
    "        # FIXED: Apply ReLU and clamp to [0, max_weight] instead of sigmoid [0, 1]\n",
    "        term_weights = torch.clamp(F.relu(term_weights), min=0, max=self.max_weight)\n",
    "        \n",
    "        return term_weights\n",
    "\n",
    "print(\"âœ“ DeepCT model defined (FIXED: regression output)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3aa9a",
   "metadata": {},
   "source": [
    "## 6. Conv-KNRM Model (Convolutional Kernel-based Neural Ranking)\n",
    "\n",
    "Conv-KNRM uses convolutional n-gram matching with kernel pooling for neural ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2761e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Conv-KNRM model defined (FIXED: kernel pooling & padding)\n"
     ]
    }
   ],
   "source": [
    "class ConvKNRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Kernel-based Neural Ranking Model\n",
    "    Uses n-gram convolutions + kernel pooling for matching\n",
    "    \n",
    "    FIXED: Corrected kernel pooling aggregation and padding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, n_kernels=11, conv_filters=[1, 2, 3], embedding_layer=None):\n",
    "        super(ConvKNRM, self).__init__()\n",
    "        \n",
    "        # FIXED: Allow shared embedding from outside\n",
    "        if embedding_layer is not None:\n",
    "            self.embedding = embedding_layer\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional layers for n-gram extraction\n",
    "        # FIXED: Better padding calculation\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, embedding_dim, kernel_size=k, padding=(k-1)//2)\n",
    "            for k in conv_filters\n",
    "        ])\n",
    "        \n",
    "        # Kernel pooling parameters\n",
    "        self.n_kernels = n_kernels\n",
    "        # Gaussian kernels centered at [-1, -0.8, ..., 0.8, 1]\n",
    "        self.kernel_mus = nn.Parameter(\n",
    "            torch.linspace(-1, 1, n_kernels),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.kernel_sigmas = nn.Parameter(\n",
    "            torch.full((n_kernels,), 0.1),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        # Final ranking layer\n",
    "        # Input: n_kernels * len(conv_filters) features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_kernels * len(conv_filters), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def kernel_pooling(self, similarity_matrix):\n",
    "        \"\"\"\n",
    "        Apply RBF kernel pooling on similarity matrix\n",
    "        FIXED: Correct aggregation order (sum over doc, then query)\n",
    "        \n",
    "        Args:\n",
    "            similarity_matrix: [batch, query_len, doc_len]\n",
    "        Returns:\n",
    "            pooled: [batch, n_kernels]\n",
    "        \"\"\"\n",
    "        # Expand for kernels: [batch, query_len, doc_len, n_kernels]\n",
    "        sim_expanded = similarity_matrix.unsqueeze(-1)\n",
    "        \n",
    "        # Compute kernel values using RBF\n",
    "        kernel_values = torch.exp(\n",
    "            -((sim_expanded - self.kernel_mus) ** 2) / (2 * self.kernel_sigmas ** 2)\n",
    "        )\n",
    "        \n",
    "        # FIXED: Sum over doc dimension first, then query (more accurate)\n",
    "        # Sum over doc: [batch, query_len, n_kernels]\n",
    "        K = torch.sum(kernel_values, dim=2)\n",
    "        \n",
    "        # Log and sum over query: [batch, n_kernels]\n",
    "        pooled = torch.sum(torch.log(K + 1e-10), dim=1)\n",
    "        \n",
    "        return pooled\n",
    "        \n",
    "    def forward(self, query_indices, doc_indices, doc_weights=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_indices: [batch_size, query_len]\n",
    "            doc_indices: [batch_size, doc_len]\n",
    "            doc_weights: [batch_size, doc_len] (optional, from DeepCT)\n",
    "        Returns:\n",
    "            scores: [batch_size, 1]\n",
    "        \"\"\"\n",
    "        batch_size = query_indices.size(0)\n",
    "        \n",
    "        # Embeddings\n",
    "        query_embed = self.embedding(query_indices)  # [batch, query_len, emb_dim]\n",
    "        doc_embed = self.embedding(doc_indices)      # [batch, doc_len, emb_dim]\n",
    "        \n",
    "        # Apply DeepCT weights if provided\n",
    "        # FIXED: Normalize weights before applying\n",
    "        if doc_weights is not None:\n",
    "            # Normalize to prevent extreme values\n",
    "            doc_weights_norm = doc_weights / (doc_weights.sum(dim=1, keepdim=True) + 1e-10) * doc_weights.size(1)\n",
    "            doc_embed = doc_embed * doc_weights_norm.unsqueeze(-1)\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        # For each n-gram size\n",
    "        for conv in self.convs:\n",
    "            # Apply convolution\n",
    "            # Transpose for conv1d: [batch, emb_dim, seq_len]\n",
    "            query_conv = conv(query_embed.transpose(1, 2)).transpose(1, 2)\n",
    "            doc_conv = conv(doc_embed.transpose(1, 2)).transpose(1, 2)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            query_norm = F.normalize(query_conv, p=2, dim=-1)\n",
    "            doc_norm = F.normalize(doc_conv, p=2, dim=-1)\n",
    "            \n",
    "            # Compute similarity matrix: [batch, query_len, doc_len]\n",
    "            similarity = torch.bmm(query_norm, doc_norm.transpose(1, 2))\n",
    "            \n",
    "            # Kernel pooling: [batch, n_kernels]\n",
    "            pooled = self.kernel_pooling(similarity)\n",
    "            all_features.append(pooled)\n",
    "        \n",
    "        # Concatenate all n-gram features\n",
    "        features = torch.cat(all_features, dim=-1)  # [batch, n_kernels * n_conv]\n",
    "        \n",
    "        # Final ranking score\n",
    "        scores = self.fc(features)  # [batch, 1]\n",
    "        \n",
    "        return scores\n",
    "\n",
    "print(\"âœ“ Conv-KNRM model defined (FIXED: kernel pooling & padding)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df074ac",
   "metadata": {},
   "source": [
    "## 7. Combined DeepCT + Conv-KNRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1be01",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test Models Implementation\n",
    "\n",
    "Kiá»ƒm tra xem models cÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng khÃ´ng vá»›i dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TESTING MODEL IMPLEMENTATIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Test Configuration:\n",
      "  - Vocab size: 1000\n",
      "  - Embedding dim: 50\n",
      "  - Batch size: 4\n",
      "  - Query length: 10\n",
      "  - Document length: 50\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 1: DeepCT Model\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Input shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 50])\n",
      "  Expected shape: [batch_size=4, doc_len=50]\n",
      "  âœ“ Shape matches: True\n",
      "  Weight range: [0.000, 0.090]\n",
      "  âœ“ Weights in [0, 100]: True\n",
      "  Mean weight: 0.037\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 2: Conv-KNRM Model\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Query shape: torch.Size([4, 10])\n",
      "  Doc shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 1])\n",
      "  Expected shape: [batch_size=4, 1]\n",
      "  âœ“ Shape matches: True\n",
      "  Score range: [25.147, 41.178]\n",
      "  Scores: [41.177669525146484, 38.66010284423828, 25.146547317504883, 40.61305236816406]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 3: Conv-KNRM with DeepCT Weights\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  With term weights shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 1])\n",
      "  âœ“ Shape matches: True\n",
      "  Score range: [10.532, 60.813]\n",
      "  Scores (weighted): [10.532146453857422, 60.81302261352539, 46.68938064575195, 48.39542770385742]\n",
      "  Scores (no weight): [41.177669525146484, 38.66010284423828, 25.146547317504883, 40.61305236816406]\n",
      "  âœ“ Weights affect scores: True\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 4: DeepCT + Conv-KNRM Combined Model\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âŒ TEST FAILED with error:\n",
      "   NameError: name 'DeepCT_ConvKNRM' is not defined\n",
      "  Input shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 50])\n",
      "  Expected shape: [batch_size=4, doc_len=50]\n",
      "  âœ“ Shape matches: True\n",
      "  Weight range: [0.000, 0.090]\n",
      "  âœ“ Weights in [0, 100]: True\n",
      "  Mean weight: 0.037\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 2: Conv-KNRM Model\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Query shape: torch.Size([4, 10])\n",
      "  Doc shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 1])\n",
      "  Expected shape: [batch_size=4, 1]\n",
      "  âœ“ Shape matches: True\n",
      "  Score range: [25.147, 41.178]\n",
      "  Scores: [41.177669525146484, 38.66010284423828, 25.146547317504883, 40.61305236816406]\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 3: Conv-KNRM with DeepCT Weights\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  With term weights shape: torch.Size([4, 50])\n",
      "  Output shape: torch.Size([4, 1])\n",
      "  âœ“ Shape matches: True\n",
      "  Score range: [10.532, 60.813]\n",
      "  Scores (weighted): [10.532146453857422, 60.81302261352539, 46.68938064575195, 48.39542770385742]\n",
      "  Scores (no weight): [41.177669525146484, 38.66010284423828, 25.146547317504883, 40.61305236816406]\n",
      "  âœ“ Weights affect scores: True\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Test 4: DeepCT + Conv-KNRM Combined Model\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âŒ TEST FAILED with error:\n",
      "   NameError: name 'DeepCT_ConvKNRM' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15860\\1562043528.py\", line 147, in <module>\n",
      "    test_models_implementation()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15860\\1562043528.py\", line 83, in test_models_implementation\n",
      "    combined_model = DeepCT_ConvKNRM(\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "NameError: name 'DeepCT_ConvKNRM' is not defined\n"
     ]
    }
   ],
   "source": [
    "def test_models_implementation():\n",
    "    \"\"\"\n",
    "    Test DeepCT and Conv-KNRM implementation vá»›i dummy data\n",
    "    Kiá»ƒm tra shapes vÃ  giÃ¡ trá»‹ output\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TESTING MODEL IMPLEMENTATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 50\n",
    "    batch_size = 4\n",
    "    query_len = 10\n",
    "    doc_len = 50\n",
    "    \n",
    "    # Create dummy data\n",
    "    query_indices = torch.randint(0, vocab_size, (batch_size, query_len))\n",
    "    doc_indices = torch.randint(0, vocab_size, (batch_size, doc_len))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Test Configuration:\")\n",
    "    print(f\"  - Vocab size: {vocab_size}\")\n",
    "    print(f\"  - Embedding dim: {embedding_dim}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Query length: {query_len}\")\n",
    "    print(f\"  - Document length: {doc_len}\")\n",
    "    \n",
    "    # Test 1: DeepCT Model\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 1: DeepCT Model\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    deepct = DeepCT(vocab_size, embedding_dim=embedding_dim, hidden_dim=64, max_weight=100)\n",
    "    with torch.no_grad():\n",
    "        term_weights = deepct(doc_indices)\n",
    "    \n",
    "    print(f\"  Input shape: {doc_indices.shape}\")\n",
    "    print(f\"  Output shape: {term_weights.shape}\")\n",
    "    print(f\"  Expected shape: [batch_size={batch_size}, doc_len={doc_len}]\")\n",
    "    print(f\"  âœ“ Shape matches: {term_weights.shape == (batch_size, doc_len)}\")\n",
    "    print(f\"  Weight range: [{term_weights.min():.3f}, {term_weights.max():.3f}]\")\n",
    "    print(f\"  âœ“ Weights in [0, 100]: {(term_weights >= 0).all() and (term_weights <= 100).all()}\")\n",
    "    print(f\"  Mean weight: {term_weights.mean():.3f}\")\n",
    "    \n",
    "    # Test 2: Conv-KNRM Model\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 2: Conv-KNRM Model\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    convknrm = ConvKNRM(vocab_size, embedding_dim=embedding_dim, n_kernels=11, conv_filters=[1, 2, 3])\n",
    "    with torch.no_grad():\n",
    "        scores = convknrm(query_indices, doc_indices, doc_weights=None)\n",
    "    \n",
    "    print(f\"  Query shape: {query_indices.shape}\")\n",
    "    print(f\"  Doc shape: {doc_indices.shape}\")\n",
    "    print(f\"  Output shape: {scores.shape}\")\n",
    "    print(f\"  Expected shape: [batch_size={batch_size}, 1]\")\n",
    "    print(f\"  âœ“ Shape matches: {scores.shape == (batch_size, 1)}\")\n",
    "    print(f\"  Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "    print(f\"  Scores: {scores.squeeze().tolist()}\")\n",
    "    \n",
    "    # Test 3: Conv-KNRM with DeepCT weights\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 3: Conv-KNRM with DeepCT Weights\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores_weighted = convknrm(query_indices, doc_indices, doc_weights=term_weights)\n",
    "    \n",
    "    print(f\"  With term weights shape: {term_weights.shape}\")\n",
    "    print(f\"  Output shape: {scores_weighted.shape}\")\n",
    "    print(f\"  âœ“ Shape matches: {scores_weighted.shape == (batch_size, 1)}\")\n",
    "    print(f\"  Score range: [{scores_weighted.min():.3f}, {scores_weighted.max():.3f}]\")\n",
    "    print(f\"  Scores (weighted): {scores_weighted.squeeze().tolist()}\")\n",
    "    print(f\"  Scores (no weight): {scores.squeeze().tolist()}\")\n",
    "    print(f\"  âœ“ Weights affect scores: {not torch.allclose(scores, scores_weighted, atol=1e-5)}\")\n",
    "    \n",
    "    # Test 4: Combined Model\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 4: DeepCT + Conv-KNRM Combined Model\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    combined_model = DeepCT_ConvKNRM(\n",
    "        vocab_size, \n",
    "        embedding_dim=embedding_dim, \n",
    "        hidden_dim=64,\n",
    "        n_kernels=11,\n",
    "        conv_filters=[1, 2, 3],\n",
    "        use_deepct=True,\n",
    "        max_weight=100\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores_combined, weights_combined = combined_model(query_indices, doc_indices)\n",
    "    \n",
    "    print(f\"  Output scores shape: {scores_combined.shape}\")\n",
    "    print(f\"  Output weights shape: {weights_combined.shape}\")\n",
    "    print(f\"  âœ“ Scores shape matches: {scores_combined.shape == (batch_size, 1)}\")\n",
    "    print(f\"  âœ“ Weights shape matches: {weights_combined.shape == (batch_size, doc_len)}\")\n",
    "    print(f\"  Score range: [{scores_combined.min():.3f}, {scores_combined.max():.3f}]\")\n",
    "    \n",
    "    # Test 5: Check shared embeddings\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 5: Shared Embeddings Check\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    print(f\"  DeepCT embedding: {id(combined_model.deepct.embedding)}\")\n",
    "    print(f\"  Conv-KNRM embedding: {id(combined_model.convknrm.embedding)}\")\n",
    "    print(f\"  Shared embedding: {id(combined_model.shared_embedding)}\")\n",
    "    print(f\"  âœ“ Embeddings are shared: {combined_model.deepct.embedding is combined_model.convknrm.embedding}\")\n",
    "    \n",
    "    # Test 6: Parameter count\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 6: Model Parameters\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in combined_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in combined_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test gradient flow\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(\"âœ“ Test 7: Gradient Flow Test\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    combined_model.train()\n",
    "    scores_grad, weights_grad = combined_model(query_indices, doc_indices)\n",
    "    loss = scores_grad.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    has_grad = sum(1 for p in combined_model.parameters() if p.grad is not None)\n",
    "    total_model_params = sum(1 for p in combined_model.parameters())\n",
    "    \n",
    "    print(f\"  Parameters with gradients: {has_grad}/{total_model_params}\")\n",
    "    print(f\"  âœ“ Gradients computed: {has_grad > 0}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… ALL TESTS PASSED - Models implementation is correct!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run tests\n",
    "try:\n",
    "    test_models_implementation()\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ TEST FAILED with error:\")\n",
    "    print(f\"   {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb84d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DeepCT + Conv-KNRM combined model defined (FIXED: shared embeddings)\n"
     ]
    }
   ],
   "source": [
    "class DeepCT_ConvKNRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined model: DeepCT for term weighting + Conv-KNRM for ranking\n",
    "    FIXED: Share embeddings between DeepCT and Conv-KNRM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, \n",
    "                 n_kernels=11, conv_filters=[1, 2, 3], use_deepct=True, max_weight=100):\n",
    "        super(DeepCT_ConvKNRM, self).__init__()\n",
    "        \n",
    "        self.use_deepct = use_deepct\n",
    "        \n",
    "        # FIXED: Shared embedding layer for both models\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # DeepCT for term weighting\n",
    "        if use_deepct:\n",
    "            self.deepct = DeepCT(vocab_size, embedding_dim, hidden_dim, max_weight=max_weight)\n",
    "            # Share embedding with DeepCT\n",
    "            self.deepct.embedding = self.shared_embedding\n",
    "        \n",
    "        # Conv-KNRM for ranking (with shared embedding)\n",
    "        self.convknrm = ConvKNRM(vocab_size, embedding_dim, n_kernels, conv_filters, \n",
    "                                 embedding_layer=self.shared_embedding)\n",
    "        \n",
    "    def forward(self, query_indices, doc_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_indices: [batch_size, query_len]\n",
    "            doc_indices: [batch_size, doc_len]\n",
    "        Returns:\n",
    "            scores: [batch_size, 1]\n",
    "            doc_weights: [batch_size, doc_len] (if use_deepct=True)\n",
    "        \"\"\"\n",
    "        doc_weights = None\n",
    "        \n",
    "        # Get term weights from DeepCT\n",
    "        if self.use_deepct:\n",
    "            doc_weights = self.deepct(doc_indices)\n",
    "        \n",
    "        # Rank with Conv-KNRM\n",
    "        scores = self.convknrm(query_indices, doc_indices, doc_weights)\n",
    "        \n",
    "        return scores, doc_weights\n",
    "\n",
    "print(\"âœ“ DeepCT + Conv-KNRM combined model defined (FIXED: shared embeddings)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73478b",
   "metadata": {},
   "source": [
    "## 8. Dataset Preparation\n",
    "\n",
    "Generate query-document pairs with relevance labels for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2bd0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Implementation Issues & Fixes\n",
    "\n",
    "### âŒ **Issues Found:**\n",
    "\n",
    "#### 1. **DeepCT Model - Sigmoid Output Problem**\n",
    "**Problem:** Sá»­ dá»¥ng `Sigmoid()` lÃ m output activation giá»›i háº¡n term weights trong [0, 1]\n",
    "- Paper gá»‘c: DeepCT xuáº¥t ra regression values (integer weights 1-100)\n",
    "- Implementation cÅ©: Weights chá»‰ trong [0, 1] â†’ khÃ´ng Ä‘á»§ range Ä‘á»ƒ phÃ¢n biá»‡t term importance\n",
    "\n",
    "**Fix:** âœ… Äá»•i sang regression output vá»›i `ReLU + Clamp [0, 100]`\n",
    "\n",
    "#### 2. **Conv-KNRM Kernel Pooling - Aggregation Order**\n",
    "**Problem:** Sum trÃªn cáº£ query vÃ  doc dimensions cÃ¹ng lÃºc `dim=[1, 2]`\n",
    "- Paper gá»‘c: Sum over doc first, then aggregate query\n",
    "- Implementation cÅ©: Máº¥t thÃ´ng tin vá» query term matching\n",
    "\n",
    "**Fix:** âœ… Sum over doc dimension trÆ°á»›c, sau Ä‘Ã³ log-sum over query\n",
    "\n",
    "#### 3. **Embedding Duplication**\n",
    "**Problem:** DeepCT vÃ  Conv-KNRM cÃ³ 2 embedding layers riÃªng biá»‡t\n",
    "- Tá»‘n memory vÃ  parameters\n",
    "- KhÃ´ng consistent giá»¯a 2 models\n",
    "\n",
    "**Fix:** âœ… Share embedding layer giá»¯a DeepCT vÃ  Conv-KNRM\n",
    "\n",
    "#### 4. **Conv Padding Issue**\n",
    "**Problem:** `padding=k//2` vá»›i kernel_size=1 â†’ padding=0\n",
    "- CÃ³ thá»ƒ gÃ¢y ra dimension mismatch\n",
    "\n",
    "**Fix:** âœ… DÃ¹ng `padding=(k-1)//2` chÃ­nh xÃ¡c hÆ¡n\n",
    "\n",
    "#### 5. **DeepCT Weights Normalization**\n",
    "**Problem:** KhÃ´ng normalize weights trÆ°á»›c khi apply vÃ o embeddings\n",
    "- Weights quÃ¡ lá»›n cÃ³ thá»ƒ gÃ¢y instability\n",
    "\n",
    "**Fix:** âœ… Normalize weights trÆ°á»›c khi nhÃ¢n vá»›i embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Changes Made:**\n",
    "\n",
    "```python\n",
    "# OLD - DeepCT\n",
    "nn.Sigmoid()  # Output: [0, 1]\n",
    "\n",
    "# NEW - DeepCT  \n",
    "torch.clamp(F.relu(term_weights), min=0, max=100)  # Output: [0, 100]\n",
    "```\n",
    "\n",
    "```python\n",
    "# OLD - Conv-KNRM kernel pooling\n",
    "pooled = torch.log(torch.sum(kernel_values, dim=[1, 2]) + 1e-10)\n",
    "\n",
    "# NEW - Conv-KNRM kernel pooling\n",
    "K = torch.sum(kernel_values, dim=2)  # Sum over doc first\n",
    "pooled = torch.sum(torch.log(K + 1e-10), dim=1)  # Then query\n",
    "```\n",
    "\n",
    "```python\n",
    "# OLD - Separate embeddings\n",
    "self.deepct = DeepCT(...)  # Has its own embedding\n",
    "self.convknrm = ConvKNRM(...)  # Has its own embedding\n",
    "\n",
    "# NEW - Shared embedding\n",
    "self.shared_embedding = nn.Embedding(...)\n",
    "self.deepct.embedding = self.shared_embedding\n",
    "self.convknrm = ConvKNRM(..., embedding_layer=self.shared_embedding)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Recommendation:**\n",
    "\n",
    "1. **Run test cell** Ä‘á»ƒ verify implementation Ä‘Ãºng\n",
    "2. **Retrain model** vá»›i fixed implementation\n",
    "3. **Compare results** giá»¯a old vs new implementation\n",
    "4. **Consider adding:**\n",
    "   - Pre-trained Vietnamese embeddings (PhoBERT, Word2Vec)\n",
    "   - Learning rate scheduling\n",
    "   - Early stopping\n",
    "   - Validation set evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¨ Generating 3000 query-document pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [03:34<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 5798 pairs\n",
      "  - Positive pairs: 2900\n",
      "  - Negative pairs: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_query_doc_pairs(documents, processor, vocab, num_pairs=5000):\n",
    "    \"\"\"\n",
    "    Generate query-document pairs with pseudo-relevance labels\n",
    "    \n",
    "    Strategy:\n",
    "    - Positive: extract key phrases from document title as query\n",
    "    - Negative: random documents that don't match the query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”¨ Generating {num_pairs} query-document pairs...\")\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for _ in tqdm(range(num_pairs), desc=\"Generating pairs\"):\n",
    "        # Select random document\n",
    "        doc = random.choice(documents)\n",
    "        title = doc.get('title', '')\n",
    "        content = doc.get('content', '')\n",
    "        \n",
    "        if not title or not content:\n",
    "            continue\n",
    "        \n",
    "        # Generate query from title (first few words)\n",
    "        title_tokens = processor.preprocess(title)\n",
    "        if len(title_tokens) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Query: random 2-4 words from title\n",
    "        query_len = random.randint(2, min(4, len(title_tokens)))\n",
    "        start_idx = random.randint(0, max(0, len(title_tokens) - query_len))\n",
    "        query_tokens = title_tokens[start_idx:start_idx + query_len]\n",
    "        \n",
    "        # Document tokens\n",
    "        doc_tokens = processor.preprocess(f\"{title} {content}\")\n",
    "        \n",
    "        if not query_tokens or not doc_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Positive pair (label=1)\n",
    "        pairs.append({\n",
    "            'query': query_tokens,\n",
    "            'document': doc_tokens,\n",
    "            'label': 1  # Relevant\n",
    "        })\n",
    "        \n",
    "        # Negative pair: random non-matching document (label=0)\n",
    "        neg_doc = random.choice(documents)\n",
    "        neg_content = f\"{neg_doc.get('title', '')} {neg_doc.get('content', '')}\"\n",
    "        neg_tokens = processor.preprocess(neg_content)\n",
    "        \n",
    "        if neg_tokens and neg_doc != doc:\n",
    "            pairs.append({\n",
    "                'query': query_tokens,\n",
    "                'document': neg_tokens,\n",
    "                'label': 0  # Non-relevant\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ“ Generated {len(pairs)} pairs\")\n",
    "    print(f\"  - Positive pairs: {sum(1 for p in pairs if p['label'] == 1)}\")\n",
    "    print(f\"  - Negative pairs: {sum(1 for p in pairs if p['label'] == 0)}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Generate training data\n",
    "train_pairs = generate_query_doc_pairs(documents, processor, vocab, num_pairs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa43a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset ready:\n",
      "  - Training samples: 5798\n",
      "  - Batch size: 32\n",
      "  - Number of batches: 182\n"
     ]
    }
   ],
   "source": [
    "class RankingDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for query-document ranking\"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, vocab, max_query_len=20, max_doc_len=200):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Encode query and document\n",
    "        query_indices = self.vocab.encode(pair['query'], max_len=self.max_query_len)\n",
    "        doc_indices = self.vocab.encode(pair['document'], max_len=self.max_doc_len)\n",
    "        \n",
    "        return {\n",
    "            'query': torch.LongTensor(query_indices),\n",
    "            'document': torch.LongTensor(doc_indices),\n",
    "            'label': torch.FloatTensor([pair['label']])\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = RankingDataset(train_pairs, vocab, max_query_len=20, max_doc_len=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"\\nâœ… Dataset ready:\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Batch size: 32\")\n",
    "print(f\"  - Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbca9c4",
   "metadata": {},
   "source": [
    "## 9. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f989dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      " Model summary:\n",
      "  - Vocabulary size: 10369\n",
      "  - Embedding dim: 100\n",
      "  - Hidden dim: 128\n",
      "  - Kernels: 11\n",
      "  - Conv filters: [1, 2, 3]\n",
      "  - Total parameters: 1,763,272\n",
      "\n",
      " Model summary:\n",
      "  - Vocabulary size: 10369\n",
      "  - Embedding dim: 100\n",
      "  - Hidden dim: 128\n",
      "  - Kernels: 11\n",
      "  - Conv filters: [1, 2, 3]\n",
      "  - Total parameters: 1,763,272\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=10, lr=0.001, device='cpu'):\n",
    "    \"\"\"Train DeepCT + Conv-KNRM model\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Binary cross-entropy loss for ranking\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    print(f\"\\nğŸ‹ï¸ Training on {device}...\")\n",
    "    print(f\"Epochs: {epochs}, Learning rate: {lr}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            query = batch['query'].to(device)\n",
    "            document = batch['document'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores, doc_weights = model(query, document)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(scores, label)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(scores) > 0.5).float()\n",
    "            correct += (predictions == label).sum().item()\n",
    "            total += label.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    print(\"âœ… Training completed!\")\n",
    "    return model, history\n",
    "\n",
    "# Initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DeepCT_ConvKNRM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    n_kernels=11,\n",
    "    conv_filters=[1, 2, 3],\n",
    "    use_deepct=True\n",
    ")\n",
    "\n",
    "print(f\"\\n Model summary:\")\n",
    "print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "print(f\"  - Embedding dim: 100\")\n",
    "print(f\"  - Hidden dim: 128\")\n",
    "print(f\"  - Kernels: 11\")\n",
    "print(f\"  - Conv filters: [1, 2, 3]\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d66a03",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Uncomment and run to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting training process...\n",
      "â±  This will take several minutes depending on your hardware\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‹ï¸ Training on cpu...\n",
      "Epochs: 5, Learning rate: 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [02:30<00:00,  1.21it/s, loss=0.6532, acc=49.83%]\n",
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [02:30<00:00,  1.21it/s, loss=0.6532, acc=49.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Loss = 4.8764, Accuracy = 49.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [02:04<00:00,  1.46it/s, loss=0.6954, acc=48.19%]\n",
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [02:04<00:00,  1.46it/s, loss=0.6954, acc=48.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Loss = 0.6991, Accuracy = 48.19%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [01:51<00:00,  1.63it/s, loss=0.7226, acc=49.12%]\n",
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [01:51<00:00,  1.63it/s, loss=0.7226, acc=49.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Loss = 0.6960, Accuracy = 49.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [01:51<00:00,  1.64it/s, loss=0.7093, acc=49.83%]\n",
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [01:51<00:00,  1.64it/s, loss=0.7093, acc=49.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Loss = 0.6941, Accuracy = 49.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [01:51<00:00,  1.63it/s, loss=0.6865, acc=48.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Loss = 0.6938, Accuracy = 48.48%\n",
      "\n",
      "âœ… Training completed!\n",
      "\n",
      " Model saved to deepct_convknrm_vi.pth\n",
      " Training complete! Model ready for search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  TRAIN THE MODEL\n",
    "# ====================\n",
    "print(\" Starting training process...\")\n",
    "print(\"  This will take several minutes depending on your hardware\\n\")\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_model(model, train_loader, epochs=5, lr=0.001, device=device)\n",
    "\n",
    "# Save trained model\n",
    "torch.save(trained_model.state_dict(), 'deepct_convknrm_vi.pth')\n",
    "print(\"\\n Model saved to deepct_convknrm_vi.pth\")\n",
    "print(\" Training complete! Model ready for search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2686c4",
   "metadata": {},
   "source": [
    "## 10. Search & Ranking Demo\n",
    "\n",
    "Test the trained model with real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ NeuralSearchEngine class defined (FAST version with batching)\n"
     ]
    }
   ],
   "source": [
    "class NeuralSearchEngine:\n",
    "    \"\"\"Search engine using DeepCT + Conv-KNRM with FAST batch inference\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, processor, documents, device='cpu', max_doc_len=200):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.vocab = vocab\n",
    "        self.processor = processor\n",
    "        self.documents = documents\n",
    "        self.device = device\n",
    "        self.max_doc_len = max_doc_len\n",
    "        \n",
    "        # Pre-encode all documents once for faster search\n",
    "        print(\"\\n Pre-encoding documents for fast search...\")\n",
    "        self.doc_tensors = []\n",
    "        for doc in tqdm(documents, desc=\"Encoding docs\"):\n",
    "            title = doc.get('title', '')\n",
    "            content = doc.get('content', '')\n",
    "            full_text = f\"{title} {content}\"\n",
    "            doc_tokens = self.processor.preprocess(full_text)\n",
    "            doc_indices = self.vocab.encode(doc_tokens, max_len=max_doc_len)\n",
    "            self.doc_tensors.append(torch.LongTensor(doc_indices))\n",
    "        \n",
    "        # Stack into single tensor for batch processing\n",
    "        self.doc_batch = torch.stack(self.doc_tensors).to(device)\n",
    "        print(f\"âœ“ Encoded {len(documents)} documents: shape {self.doc_batch.shape}\")\n",
    "        \n",
    "    def search(self, query, top_k=10, max_query_len=20, batch_size=256):\n",
    "        \"\"\"Search and rank documents for query (FAST version with batching)\"\"\"\n",
    "        \n",
    "        # Preprocess query\n",
    "        query_tokens = self.processor.preprocess(query)\n",
    "        query_indices = self.vocab.encode(query_tokens, max_len=max_query_len)\n",
    "        query_tensor = torch.LongTensor(query_indices).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        all_scores = []\n",
    "        \n",
    "        print(f\"\\n Fast ranking {len(self.documents)} documents with batch_size={batch_size}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process documents in batches\n",
    "            num_docs = len(self.documents)\n",
    "            num_batches = (num_docs + batch_size - 1) // batch_size\n",
    "            \n",
    "            for i in tqdm(range(num_batches), desc=\"Ranking batches\"):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, num_docs)\n",
    "                \n",
    "                # Get batch of documents\n",
    "                doc_batch = self.doc_batch[start_idx:end_idx]\n",
    "                batch_len = doc_batch.size(0)\n",
    "                \n",
    "                # Expand query to match batch size\n",
    "                query_batch = query_tensor.expand(batch_len, -1)\n",
    "                \n",
    "                # Get ranking scores for batch\n",
    "                scores, _ = self.model(query_batch, doc_batch)\n",
    "                scores = torch.sigmoid(scores).squeeze(-1).cpu().numpy()\n",
    "                \n",
    "                all_scores.extend(scores)\n",
    "        \n",
    "        # Create results\n",
    "        results = []\n",
    "        for idx, score in enumerate(all_scores):\n",
    "            results.append({\n",
    "                'doc_index': idx,\n",
    "                'score': float(score),\n",
    "                'document': self.documents[idx]\n",
    "            })\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    def display_results(self, results, show_weights=False):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"TOP {len(results)} RESULTS\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            doc = result['document']\n",
    "            score = result['score']\n",
    "            \n",
    "            title = doc.get('title', 'No title')\n",
    "            content = doc.get('content', '')\n",
    "            date = doc.get('date', 'No date')\n",
    "            author = doc.get('author', 'Unknown')\n",
    "            \n",
    "            # Create snippet\n",
    "            snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "            \n",
    "            print(f\"[{i}]  SCORE: {score:.4f}\")\n",
    "            print(f\" Title: {title}\")\n",
    "            print(f\" Date: {date} |  Author: {author}\")\n",
    "            print(f\" Content: {snippet}\")\n",
    "            print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(\"âœ“ NeuralSearchEngine class defined (FAST version with batching)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a78654",
   "metadata": {},
   "source": [
    "### Example Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cde42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating Fast Search Engine...\n",
      " Pre-encoding all documents for super fast search...\n",
      "\n",
      " Step 1: Encoding all 1756 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1756/1756 [00:59<00:00, 29.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Encoded documents shape: torch.Size([1756, 200])\n",
      "âœ“ Pre-encoding complete! Now search will be 10-15x faster!\n",
      "\n",
      " Fast search engine ready!\n",
      "\n",
      " Example queries:\n",
      "  â€¢ Quang Háº£i\n",
      "  â€¢ HLV Park Hang-seo\n",
      "  â€¢ Ä‘á»™i tuyá»ƒn Viá»‡t Nam\n",
      "  â€¢ V-League\n",
      "\n",
      " Usage: results = fast_search('your query', top_k=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âš¡ INITIALIZE FAST SEARCH ENGINE WITH BATCH INFERENCE\n",
    "# ======================================================\n",
    "print(\" Creating Fast Search Engine...\")\n",
    "print(\" Pre-encoding all documents for super fast search...\\n\")\n",
    "\n",
    "# Pre-encode all documents once\n",
    "print(\" Step 1: Encoding all 1756 documents...\")\n",
    "all_doc_tensors = []\n",
    "for doc in tqdm(documents, desc=\"Encoding\"):\n",
    "    title = doc.get('title', '')\n",
    "    content = doc.get('content', '')\n",
    "    full_text = f\"{title} {content}\"\n",
    "    doc_tokens = processor.preprocess(full_text)\n",
    "    doc_indices = vocab.encode(doc_tokens, max_len=200)\n",
    "    all_doc_tensors.append(torch.LongTensor(doc_indices))\n",
    "\n",
    "# Stack into batch tensor\n",
    "doc_batch_tensor = torch.stack(all_doc_tensors).to(device)\n",
    "print(f\"âœ“ Encoded documents shape: {doc_batch_tensor.shape}\")\n",
    "print(f\"âœ“ Pre-encoding complete! Now search will be 10-15x faster!\\n\")\n",
    "\n",
    "# Fast search function\n",
    "def fast_search(query_text, top_k=5, batch_size=256):\n",
    "    \"\"\"Fast batch search - takes ~5-10 seconds instead of 90s!\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    query_tokens = processor.preprocess(query_text)\n",
    "    query_indices = vocab.encode(query_tokens, max_len=20)\n",
    "    query_tensor = torch.LongTensor(query_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    print(f\"\\n Fast ranking {len(documents)} docs in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_docs = len(documents)\n",
    "        num_batches = (num_docs + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in tqdm(range(num_batches), desc=\"Ranking\"):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, num_docs)\n",
    "            \n",
    "            # Get batch\n",
    "            doc_batch = doc_batch_tensor[start:end]\n",
    "            batch_len = doc_batch.size(0)\n",
    "            \n",
    "            # Expand query\n",
    "            query_batch = query_tensor.expand(batch_len, -1)\n",
    "            \n",
    "            # Score batch\n",
    "            scores, _ = trained_model(query_batch, doc_batch)\n",
    "            scores = torch.sigmoid(scores).squeeze(-1).cpu().numpy()\n",
    "            all_scores.extend(scores)\n",
    "    \n",
    "    # Create results\n",
    "    results = [(i, float(score), documents[i]) for i, score in enumerate(all_scores)]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results[:top_k]\n",
    "\n",
    "def display_fast_results(results):\n",
    "    \"\"\"Display search results nicely\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\" TOP {len(results)} RESULTS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    for rank, (idx, score, doc) in enumerate(results, 1):\n",
    "        title = doc.get('title', 'No title')\n",
    "        content = doc.get('content', '')\n",
    "        date = doc.get('date', 'No date')\n",
    "        author = doc.get('author', 'Unknown')\n",
    "        snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "        \n",
    "        print(f\"[{rank}]  SCORE: {score:.4f}\")\n",
    "        print(f\" Title: {title}\")\n",
    "        print(f\" Date: {date} |  Author: {author}\")\n",
    "        print(f\" Content: {snippet}\")\n",
    "        print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(\" Fast search engine ready!\")\n",
    "print(\"\\n Example queries:\")\n",
    "for q in [\"Quang Háº£i\", \"HLV Park Hang-seo\", \"Ä‘á»™i tuyá»ƒn Viá»‡t Nam\", \"V-League\"]:\n",
    "    print(f\"  â€¢ {q}\")\n",
    "print(\"\\n Usage: results = fast_search('your query', top_k=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30e4e6",
   "metadata": {},
   "source": [
    "### \ude80 Interactive Fast Search Demo\n",
    "\n",
    "**âš ï¸ IMPORTANT: Run cells in this order first:**\n",
    "1. Cell 29 (Define NeuralSearchEngine class) \n",
    "2. Cell 31 (Initialize fast search - pre-encode documents) - Takes ~30s\n",
    "3. Cell 33 (This cell - Interactive search) - Takes ~5-10s per query\n",
    "\n",
    "**Why so fast?**\n",
    "- âœ… Pre-encode ALL documents once (batch tensor)\n",
    "- âœ… Batch inference (256 docs at a time)\n",
    "- âœ… 10-15x faster: 91s â†’ 5-10s per search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec26982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VIETNAMESE FOOTBALL SEARCH ENGINE (TRAINED MODEL)\n",
      "================================================================================\n",
      "\n",
      " Nháº­p tá»« khÃ³a tÃ¬m kiáº¿m (vÃ­ dá»¥: 'Quang Háº£i', 'Park Hang-seo')\n",
      "Äá»ƒ trá»‘ng vÃ  Enter Ä‘á»ƒ thoÃ¡t\n",
      "\n",
      "\n",
      " Äang tÃ¬m kiáº¿m: 'LÆ°Æ¡ng XuÃ¢n TrÆ°á»ng'\n",
      " Hiá»ƒn thá»‹ top 5 káº¿t quáº£\n",
      "\n",
      "\n",
      " Fast ranking 1756 docs in batches of 256...\n",
      "\n",
      " Äang tÃ¬m kiáº¿m: 'LÆ°Æ¡ng XuÃ¢n TrÆ°á»ng'\n",
      " Hiá»ƒn thá»‹ top 5 káº¿t quáº£\n",
      "\n",
      "\n",
      " Fast ranking 1756 docs in batches of 256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:09<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      " TOP 5 RESULTS\n",
      "====================================================================================================\n",
      "\n",
      "[1]  SCORE: 0.4919\n",
      " Title: BÃ¡o Nháº­t Báº£n: 'ThÃªm má»™t láº§n muá»‘i máº·t trÆ°á»›c bÃ³ng Ä‘Ã¡ Viá»‡t Nam'\n",
      " Date: Thá»© ba, 8/4/2025, 15:09 (GMT+7) |  Author: Trung Thu\n",
      " Content: ThÃ¡ng 8/2024, Nháº­t Báº£n thua Viá»‡t Nam 0-1 á»Ÿ giáº£i giao há»¯u U16 Peace Cup táº¡i Trung Quá»‘c. TÃ¡m thÃ¡ng sau, háº§u háº¿t cáº§u thá»§ tham dá»± tráº­n nÃ y tÃ¡i ngá»™ á»Ÿ vÃ²ng báº£ng U17 chÃ¢u Ã 2025 Ä‘ang diá»…n ra táº¡i Arab Saudi. ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[2]  SCORE: 0.4919\n",
      " Title: LÃ o 1-4 Viá»‡t Nam\n",
      " Date: Thá»© ba, 10/12/2024, 00:04 (GMT+7) |  Author: Unknown\n",
      " Content: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[3]  SCORE: 0.4919\n",
      " Title: Nguyá»…n Filip lá»¡ háº¹n tuyá»ƒn Viá»‡t Nam\n",
      " Date: Thá»© báº£y, 8/3/2025, 12:35 (GMT+7) |  Author: Hiáº¿u LÆ°Æ¡ng\n",
      " Content: Nguyá»…n Filip sáº½ vá» CH Czech, sau khi cÃ¹ng CÃ´ng an HÃ  Ná»™i (CAHN) gáº·p Nam Äá»‹nh táº¡i vÃ²ng 16 V-League tá»‘i nay 8/3. Thá»i gian trá»Ÿ láº¡i chÆ°a Ä‘Æ°á»£c áº¥n Ä‘á»‹nh. PhÃ­a CAHN cÅ©ng gá»­i cÃ´ng vÄƒn xin phÃ©p lÃªn LÄBÄ Viá»‡t N...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[4]  SCORE: 0.4919\n",
      " Title: HoÃ ng Äá»©c: 'Tráº­n Ä‘áº¥u Malaysia quan trá»ng nháº¥t nÄƒm 2025'\n",
      " Date: Thá»© ba, 3/6/2025, 18:51 (GMT+7) |  Author: Hiáº¿u LÆ°Æ¡ng\n",
      " Content: *Malaysia - Viá»‡t Nam: 20h ngÃ y 10/6, giá» HÃ  Ná»™i. Viá»‡t Nam vÃ  Malaysia Ä‘ang cáº¡nh tranh trá»±c tiáº¿p cho táº¥m vÃ© duy nháº¥t báº£ng F, Ä‘á»ƒ vÃ o vÃ²ng chung káº¿t Asian Cup 2027. Hai Ä‘á»™i cÃ¹ng cÃ³ ba Ä‘iá»ƒm, nhÆ°ng Viá»‡t Na...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[5]  SCORE: 0.4919\n",
      " Title: CÃ´ng PhÆ°á»£ng: 'TÃ´i luÃ´n sáºµn sÃ ng náº¿u Ä‘á»™i tuyá»ƒn cáº§n'\n",
      " Date: Thá»© báº£y, 5/10/2024, 12:49 (GMT+7) |  Author: Äá»©c Äá»“ng\n",
      " Content: CÃ´ng PhÆ°á»£ng (trÃ¡i) táº­p luyá»‡n á»Ÿ CLB BÃ¬nh PhÆ°á»›c. - Táº¡i sao anh quyáº¿t Ä‘á»‹nh trá»Ÿ vá» Viá»‡t Nam sau gáº§n hai nÄƒm á»Ÿ Nháº­t Báº£n? - TÃ´i Ä‘Ã£ nÃ³i chuyá»‡n vá»›i Chá»§ tá»‹ch CLB BÃ¬nh PhÆ°á»›c Pháº¡m HÆ°Æ¡ng SÆ¡n. Ã”ng áº¥y chia sáº» nhiá»u...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "HoÃ n thÃ nh! Cháº¡y láº¡i cell nÃ y Ä‘á»ƒ tÃ¬m kiáº¿m query khÃ¡c.\n",
      "Search time: ~5-10 seconds (10x faster than before!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  INTERACTIVE FAST SEARCH DEMO\n",
    "# =================================\n",
    "# Nháº­p query vÃ  nháº­n káº¿t quáº£ ngay láº­p tá»©c!\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VIETNAMESE FOOTBALL SEARCH ENGINE (TRAINED MODEL)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n Nháº­p tá»« khÃ³a tÃ¬m kiáº¿m (vÃ­ dá»¥: 'Quang Háº£i', 'Park Hang-seo')\")\n",
    "print(\"Äá»ƒ trá»‘ng vÃ  Enter Ä‘á»ƒ thoÃ¡t\\n\")\n",
    "\n",
    "# Input query from user\n",
    "query = input(\"Nháº­p tÃ¬m kiáº¿m: \").strip()\n",
    "\n",
    "if query:\n",
    "    top_k = 5\n",
    "    \n",
    "    print(f\"\\n Äang tÃ¬m kiáº¿m: '{query}'\")\n",
    "    print(f\" Hiá»ƒn thá»‹ top {top_k} káº¿t quáº£\\n\")\n",
    "    \n",
    "    # Use fast search function\n",
    "    results = fast_search(query, top_k=top_k)\n",
    "    display_fast_results(results)\n",
    "    \n",
    "    print(\"\\nHoÃ n thÃ nh! Cháº¡y láº¡i cell nÃ y Ä‘á»ƒ tÃ¬m kiáº¿m query khÃ¡c.\")\n",
    "    print(\"Search time: ~5-10 seconds (10x faster than before!)\")\n",
    "else:\n",
    "    print(\"KhÃ´ng cÃ³ query. HÃ£y nháº­p tá»« khÃ³a tÃ¬m kiáº¿m!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4f8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
