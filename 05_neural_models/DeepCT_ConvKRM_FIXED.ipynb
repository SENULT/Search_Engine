{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19933bb",
   "metadata": {},
   "source": [
    "# DeepCT + Conv-KNRM for Vietnamese Football Search\n",
    "\n",
    "## üìã Overview\n",
    "Implementation of DeepCT (Deep Contextualized Term weighting) combined with Conv-KNRM (Convolutional Kernel-based Neural Ranking Model) for Vietnamese information retrieval.\n",
    "\n",
    "### üéØ Components:\n",
    "1. **DeepCT**: Neural term weighting for document representation\n",
    "2. **Conv-KNRM**: Convolutional neural ranking model\n",
    "3. **Vietnamese Text Processing**: Tokenization, stopwords, embeddings\n",
    "4. **Training Pipeline**: Query-document pairs with relevance labels\n",
    "\n",
    "### üìä Dataset:\n",
    "- Vietnamese Football News from VnExpress (1830+ documents)\n",
    "- Query generation from titles and content\n",
    "- BM25 baseline for comparison\n",
    "\n",
    "### üìù Run Order:\n",
    "**Run cells in this exact order:**\n",
    "1. Import Libraries ‚úÖ\n",
    "2. Vietnamese Text Processor ‚úÖ\n",
    "3. Load Data ‚úÖ\n",
    "4. Build Vocabulary ‚úÖ\n",
    "5. Define Models (DeepCT ‚Üí Conv-KNRM ‚Üí Combined) ‚úÖ\n",
    "6. Test Models ‚úÖ\n",
    "7. Generate Training Data ‚úÖ\n",
    "8. Train Model üèãÔ∏è\n",
    "9. Search Engine üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0eba5f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db07ab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyVi available\n",
      "‚úó Gensim not available. Install: pip install gensim\n",
      "PyTorch version: 2.3.1+cpu\n",
      "CUDA available: False\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    PYVI_AVAILABLE = True\n",
    "    print(\"‚úì PyVi available\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyVi not available. Install: pip install pyvi\")\n",
    "    PYVI_AVAILABLE = False\n",
    "\n",
    "# Word embeddings\n",
    "try:\n",
    "    from gensim.models import Word2Vec, KeyedVectors\n",
    "    GENSIM_AVAILABLE = True\n",
    "    print(\"‚úì Gensim available\")\n",
    "except ImportError:\n",
    "    print(\"‚úó Gensim not available. Install: pip install gensim\")\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2759d8",
   "metadata": {},
   "source": [
    "## 2. Vietnamese Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa4d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VietnameseTextProcessor initialized\n"
     ]
    }
   ],
   "source": [
    "class VietnameseTextProcessor:\n",
    "    \"\"\"Vietnamese text processing for neural ranking\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Vietnamese stopwords\n",
    "        self.stop_words = set([\n",
    "            'v√†', 'c·ªßa', 'trong', 'v·ªõi', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'cho', 't·ª´', 'm·ªôt', 'c√°c',\n",
    "            'ƒë·ªÉ', 'kh√¥ng', 's·∫Ω', 'ƒë√£', 'v·ªÅ', 'hay', 'theo', 'nh∆∞', 'c≈©ng', 'n√†y', 'ƒë√≥',\n",
    "            'khi', 'nh·ªØng', 't·∫°i', 'sau', 'b·ªã', 'gi·ªØa', 'tr√™n', 'd∆∞·ªõi', 'ngo√†i',\n",
    "            'th√¨', 'nh∆∞ng', 'm√†', 'ho·∫∑c', 'n·∫øu', 'v√¨', 'do', 'n√™n', 'r·ªìi', 'c√≤n', 'ƒë·ªÅu',\n",
    "            'ch·ªâ', 'vi·ªác', 'ng∆∞·ªùi', 'l·∫°i', 'ƒë√¢y', 'ƒë·∫•y', '·ªü', 'ra', 'v√†o', 'l√™n', 'xu·ªëng'\n",
    "        ])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Keep Vietnamese characters, letters, numbers\n",
    "        text = re.sub(r'[^\\w\\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize Vietnamese text\"\"\"\n",
    "        if PYVI_AVAILABLE:\n",
    "            try:\n",
    "                return ViTokenizer.tokenize(text).split()\n",
    "            except:\n",
    "                pass\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "    \n",
    "    def preprocess(self, text, remove_stop=True):\n",
    "        \"\"\"Full preprocessing pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        if remove_stop:\n",
    "            tokens = self.remove_stopwords(tokens)\n",
    "        return tokens\n",
    "\n",
    "processor = VietnameseTextProcessor()\n",
    "print(\"‚úì VietnameseTextProcessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36af624",
   "metadata": {},
   "source": [
    "## 3. Load Vietnamese Football Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcefeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading documents from JSON files...\n",
      "  ‚úì Loaded ../data/raw/vnexpressT_bongda_part1.json: 473 documents\n",
      "  ‚úì Loaded ../data/raw/vnexpressT_bongda_part2.json: 488 documents\n",
      "  ‚úì Loaded ../data/raw/vnexpressT_bongda_part3.json: 487 documents\n",
      "  ‚úì Loaded ../data/raw/vnexpressT_bongda_part4.json: 308 documents\n",
      "\n",
      "‚úì Total documents loaded: 1756\n",
      "\n",
      "üìÑ Sample document:\n",
      "Title: '·∫¢o t∆∞·ªüng b√≥ng ƒë√° Vi·ªát Nam v∆∞∆°n t·∫ßm khi gi√†nh v√© d·ª± VCK U23 ch√¢u √Å'\n",
      "Content: U23 Vi·ªát Nam v·ª´a gi√†nh v√© d·ª± V√≤ng chung k·∫øt U23 ch√¢u √Å 2026 sau tr·∫≠n th·∫Øng 1-0 tr∆∞·ªõc Yemen, ƒë√°nh d·∫•u l·∫ßn th·ª© s√°u li√™n ti·∫øp g√≥p m·∫∑t ·ªü ƒë·∫•u tr∆∞·ªùng ch√¢u l·ª•c n√†y. Ng∆∞·ªùi h√¢m m·ªô v·ª° √≤a, truy·ªÅn th√¥ng r·ªôn r√†ng ...\n",
      "Date: Th·ª© t∆∞, 10/9/2025, 16:30 (GMT+7)\n",
      "Author: Thu Sang\n"
     ]
    }
   ],
   "source": [
    "def load_documents(json_files=None):\n",
    "    \"\"\"Load documents from JSON files\"\"\"\n",
    "    if json_files is None:\n",
    "        json_files = [\n",
    "            \"../data/raw/vnexpressT_bongda_part1.json\",\n",
    "            \"../data/raw/vnexpressT_bongda_part2.json\",\n",
    "            \"../data/raw/vnexpressT_bongda_part3.json\",\n",
    "            \"../data/raw/vnexpressT_bongda_part4.json\"\n",
    "        ]\n",
    "    \n",
    "    documents = []\n",
    "    print(\"üìÇ Loading documents from JSON files...\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        documents.extend(data)\n",
    "                    print(f\"  ‚úì Loaded {file_path}: {len(data)} documents\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚úó Error reading {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó File not found: {file_path}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Total documents loaded: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load data\n",
    "documents = load_documents()\n",
    "\n",
    "# Show sample document\n",
    "if documents:\n",
    "    print(\"\\nüìÑ Sample document:\")\n",
    "    sample = documents[0]\n",
    "    print(f\"Title: {sample.get('title', 'N/A')[:100]}\")\n",
    "    print(f\"Content: {sample.get('content', 'N/A')[:200]}...\")\n",
    "    print(f\"Date: {sample.get('date', 'N/A')}\")\n",
    "    print(f\"Author: {sample.get('author', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac166c",
   "metadata": {},
   "source": [
    "## 4. Build Vocabulary & Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da874d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1756/1756 [00:54<00:00, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocabulary size: 10369\n",
      "  - Total unique words: 15607\n",
      "  - Words with freq >= 2: 10367\n",
      "  - Top 10 words: [('nam', 8293), ('ƒë·ªôi', 7750), ('tr·∫≠n', 7581), ('hai', 6792), ('vi·ªát', 6762), ('c·∫ßu_th·ªß', 6677), ('b√≥ng', 5719), ('hlv', 5612), ('league', 5192), ('nƒÉm', 5190)]\n",
      "\n",
      "‚úÖ Vocabulary ready: 10369 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Build vocabulary from corpus\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=2):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocab(self, documents, processor):\n",
    "        \"\"\"Build vocabulary from documents\"\"\"\n",
    "        print(\"\\nüìö Building vocabulary...\")\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for doc in tqdm(documents, desc=\"Counting words\"):\n",
    "            title = doc.get('title', '')\n",
    "            content = doc.get('content', '')\n",
    "            full_text = f\"{title} {content}\"\n",
    "            tokens = processor.preprocess(full_text)\n",
    "            self.word_freq.update(tokens)\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        idx = 2  # Start after PAD and UNK\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"‚úì Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"  - Total unique words: {len(self.word_freq)}\")\n",
    "        print(f\"  - Words with freq >= {self.min_freq}: {len(self.word2idx) - 2}\")\n",
    "        print(f\"  - Top 10 words: {self.word_freq.most_common(10)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def encode(self, tokens, max_len=None):\n",
    "        \"\"\"Convert tokens to indices\"\"\"\n",
    "        indices = [self.word2idx.get(token, 1) for token in tokens]  # 1 = UNK\n",
    "        if max_len:\n",
    "            if len(indices) < max_len:\n",
    "                indices += [0] * (max_len - len(indices))  # 0 = PAD\n",
    "            else:\n",
    "                indices = indices[:max_len]\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert indices back to tokens\"\"\"\n",
    "        return [self.idx2word.get(idx, '<UNK>') for idx in indices]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocab(documents, processor)\n",
    "\n",
    "vocab_size = len(vocab.word2idx)\n",
    "print(f\"\\n‚úÖ Vocabulary ready: {vocab_size} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d340c4",
   "metadata": {},
   "source": [
    "## 5. DeepCT Model (Deep Contextualized Term Weighting)\n",
    "\n",
    "DeepCT predicts term importance weights for documents using BERT-like contextualized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99b6a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ImprovedDeepCT model defined\n"
     ]
    }
   ],
   "source": [
    "class ImprovedDeepCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved DeepCT - combines query and document for scoring\n",
    "    Output: relevance score [0, 100]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        nn.init.normal_(self.embedding.weight, 0, 0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.term_weight = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, query, doc):\n",
    "        q_embed = self.embedding(query)\n",
    "        q_lstm, _ = self.lstm(q_embed)\n",
    "        q_weights = torch.sigmoid(self.term_weight(q_lstm))\n",
    "        q_weighted = q_lstm * q_weights\n",
    "        \n",
    "        q_mask = (query != 0).unsqueeze(2).float()\n",
    "        q_pooled = torch.sum(q_weighted * q_mask, dim=1) / (torch.sum(q_mask, dim=1) + 1e-8)\n",
    "        \n",
    "        d_embed = self.embedding(doc)\n",
    "        d_lstm, _ = self.lstm(d_embed)\n",
    "        d_weights = torch.sigmoid(self.term_weight(d_lstm))\n",
    "        d_weighted = d_lstm * d_weights\n",
    "        \n",
    "        d_mask = (doc != 0).unsqueeze(2).float()\n",
    "        d_pooled = torch.sum(d_weighted * d_mask, dim=1) / (torch.sum(d_mask, dim=1) + 1e-8)\n",
    "        \n",
    "        interaction = q_pooled * d_pooled\n",
    "        score = torch.sigmoid(torch.mean(q_pooled + d_pooled + interaction, dim=1, keepdim=True)) * 2\n",
    "        \n",
    "        return score\n",
    "\n",
    "print(\"‚úì ImprovedDeepCT model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b093ce",
   "metadata": {},
   "source": [
    "## 6. Conv-KNRM Model (Convolutional Kernel-based Neural Ranking)\n",
    "\n",
    "Conv-KNRM uses convolutional n-gram matching with kernel pooling for neural ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97596cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ImprovedConvKNRM model defined\n"
     ]
    }
   ],
   "source": [
    "class ImprovedConvKNRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Conv-KNRM with shared embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, n_kernels=11, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embedding_layer is not None:\n",
    "            self.embedding = embedding_layer\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "            nn.init.normal_(self.embedding.weight, 0, 0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, embed_dim, k, padding=k//2) for k in [1, 2, 3]\n",
    "        ])\n",
    "        \n",
    "        self.n_kernels = n_kernels\n",
    "        self.kernel_mus = nn.Parameter(torch.linspace(-1, 1, n_kernels), requires_grad=False)\n",
    "        self.kernel_sigmas = nn.Parameter(torch.full((n_kernels,), 0.1), requires_grad=False)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_kernels * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def kernel_pooling(self, sim_matrix):\n",
    "        sim_expanded = sim_matrix.unsqueeze(-1)\n",
    "        kernel_vals = torch.exp(-((sim_expanded - self.kernel_mus) ** 2) / (2 * self.kernel_sigmas ** 2))\n",
    "        K = torch.sum(kernel_vals, dim=2)\n",
    "        pooled = torch.sum(torch.log(K + 1e-10), dim=1)\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, query, doc):\n",
    "        q_embed = self.embedding(query)\n",
    "        d_embed = self.embedding(doc)\n",
    "        \n",
    "        all_features = []\n",
    "        for conv in self.convs:\n",
    "            q_conv = conv(q_embed.transpose(1, 2)).transpose(1, 2)\n",
    "            d_conv = conv(d_embed.transpose(1, 2)).transpose(1, 2)\n",
    "            \n",
    "            q_norm = F.normalize(q_conv, p=2, dim=-1)\n",
    "            d_norm = F.normalize(d_conv, p=2, dim=-1)\n",
    "            \n",
    "            sim = torch.bmm(q_norm, d_norm.transpose(1, 2))\n",
    "            pooled = self.kernel_pooling(sim)\n",
    "            all_features.append(pooled)\n",
    "        \n",
    "        features = torch.cat(all_features, dim=-1)\n",
    "        scores = self.fc(features)\n",
    "        return scores\n",
    "\n",
    "print(\"‚úì ImprovedConvKNRM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b3a06",
   "metadata": {},
   "source": [
    "## 7. Combined DeepCT + Conv-KNRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e32fd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DeepCT_ConvKNRM combined model defined\n"
     ]
    }
   ],
   "source": [
    "class DeepCT_ConvKNRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined model: DeepCT + Conv-KNRM with shared embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, n_kernels=11):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared embedding\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        nn.init.normal_(self.shared_embedding.weight, 0, 0.1)\n",
    "        \n",
    "        # DeepCT\n",
    "        self.deepct = ImprovedDeepCT(vocab_size, embed_dim, hidden_dim)\n",
    "        self.deepct.embedding = self.shared_embedding\n",
    "        \n",
    "        # Conv-KNRM\n",
    "        self.convknrm = ImprovedConvKNRM(vocab_size, embed_dim, n_kernels, self.shared_embedding)\n",
    "    \n",
    "    def forward(self, query, doc):\n",
    "        deepct_score = self.deepct(query, doc)\n",
    "        convknrm_score = self.convknrm(query, doc)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_score = (deepct_score + convknrm_score) / 2\n",
    "        return combined_score, deepct_score\n",
    "\n",
    "print(\"‚úì DeepCT_ConvKNRM combined model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24beed6a",
   "metadata": {},
   "source": [
    "## 8. Dataset Preparation\n",
    "\n",
    "Generate query-document pairs with relevance labels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7044cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî® Generating 3000 query-document pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [02:33<00:00, 19.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 5798 pairs\n",
      "  - Positive pairs: 2900\n",
      "  - Negative pairs: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_query_doc_pairs(documents, processor, vocab, num_pairs=5000):\n",
    "    \"\"\"\n",
    "    Generate query-document pairs with pseudo-relevance labels\n",
    "    \n",
    "    Strategy:\n",
    "    - Positive: extract key phrases from document title as query\n",
    "    - Negative: random documents that don't match the query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüî® Generating {num_pairs} query-document pairs...\")\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for _ in tqdm(range(num_pairs), desc=\"Generating pairs\"):\n",
    "        # Select random document\n",
    "        doc = random.choice(documents)\n",
    "        title = doc.get('title', '')\n",
    "        content = doc.get('content', '')\n",
    "        \n",
    "        if not title or not content:\n",
    "            continue\n",
    "        \n",
    "        # Generate query from title (first few words)\n",
    "        title_tokens = processor.preprocess(title)\n",
    "        if len(title_tokens) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Query: random 2-4 words from title\n",
    "        query_len = random.randint(2, min(4, len(title_tokens)))\n",
    "        start_idx = random.randint(0, max(0, len(title_tokens) - query_len))\n",
    "        query_tokens = title_tokens[start_idx:start_idx + query_len]\n",
    "        \n",
    "        # Document tokens\n",
    "        doc_tokens = processor.preprocess(f\"{title} {content}\")\n",
    "        \n",
    "        if not query_tokens or not doc_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Positive pair (label=1)\n",
    "        pairs.append({\n",
    "            'query': query_tokens,\n",
    "            'document': doc_tokens,\n",
    "            'label': 1  # Relevant\n",
    "        })\n",
    "        \n",
    "        # Negative pair: random non-matching document (label=0)\n",
    "        neg_doc = random.choice(documents)\n",
    "        neg_content = f\"{neg_doc.get('title', '')} {neg_doc.get('content', '')}\"\n",
    "        neg_tokens = processor.preprocess(neg_content)\n",
    "        \n",
    "        if neg_tokens and neg_doc != doc:\n",
    "            pairs.append({\n",
    "                'query': query_tokens,\n",
    "                'document': neg_tokens,\n",
    "                'label': 0  # Non-relevant\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úì Generated {len(pairs)} pairs\")\n",
    "    print(f\"  - Positive pairs: {sum(1 for p in pairs if p['label'] == 1)}\")\n",
    "    print(f\"  - Negative pairs: {sum(1 for p in pairs if p['label'] == 0)}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Generate training data\n",
    "train_pairs = generate_query_doc_pairs(documents, processor, vocab, num_pairs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336f0e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset ready:\n",
      "  - Training samples: 5798\n",
      "  - Batch size: 32\n",
      "  - Number of batches: 182\n"
     ]
    }
   ],
   "source": [
    "class RankingDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for query-document ranking\"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, vocab, max_query_len=20, max_doc_len=200):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Encode query and document\n",
    "        query_indices = self.vocab.encode(pair['query'], max_len=self.max_query_len)\n",
    "        doc_indices = self.vocab.encode(pair['document'], max_len=self.max_doc_len)\n",
    "        \n",
    "        return {\n",
    "            'query': torch.LongTensor(query_indices),\n",
    "            'document': torch.LongTensor(doc_indices),\n",
    "            'label': torch.FloatTensor([pair['label']])\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = RankingDataset(train_pairs, vocab, max_query_len=20, max_doc_len=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset ready:\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Batch size: 32\")\n",
    "print(f\"  - Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50359e86",
   "metadata": {},
   "source": [
    "## 9. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be5bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "üìä Model summary:\n",
      "  - Vocabulary size: 10369\n",
      "  - Embedding dim: 128\n",
      "  - Hidden dim: 128\n",
      "  - Kernels: 11\n",
      "  - Total parameters: 1,692,632\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, epochs=10, lr=0.001, device='cpu'):\n",
    "    \"\"\"Train DeepCT + Conv-KNRM model\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Binary cross-entropy loss for ranking\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    print(f\"\\nüèãÔ∏è Training on {device}...\")\n",
    "    print(f\"Epochs: {epochs}, Learning rate: {lr}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            query = batch['query'].to(device)\n",
    "            document = batch['document'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores, _ = model(query, document)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(scores, label)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(scores) > 0.5).float()\n",
    "            correct += (predictions == label).sum().item()\n",
    "            total += label.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    print(\"‚úÖ Training completed!\")\n",
    "    return model, history\n",
    "\n",
    "# Initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DeepCT_ConvKNRM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    n_kernels=11\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Model summary:\")\n",
    "print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "print(f\"  - Embedding dim: 128\")\n",
    "print(f\"  - Hidden dim: 128\")\n",
    "print(f\"  - Kernels: 11\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb32e4",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Train the Model\n",
    "\n",
    "Run this cell to start training (takes ~5-10 minutes on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91632cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Starting training process...\n",
      "  This will take several minutes depending on your hardware\n",
      "\n",
      "\n",
      "üèãÔ∏è Training on cpu...\n",
      "Epochs: 20, Learning rate: 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:35<00:00,  1.90it/s, loss=0.4648, acc=56.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Loss = 1.8810, Accuracy = 56.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:26<00:00,  2.11it/s, loss=0.4447, acc=81.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: Loss = 0.4346, Accuracy = 81.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:31<00:00,  2.00it/s, loss=0.0329, acc=94.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: Loss = 0.1819, Accuracy = 94.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:18<00:00,  2.33it/s, loss=0.8219, acc=97.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: Loss = 0.0982, Accuracy = 97.02%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:16<00:00,  2.36it/s, loss=0.0640, acc=98.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: Loss = 0.0646, Accuracy = 98.07%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:16<00:00,  2.37it/s, loss=0.0189, acc=98.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: Loss = 0.0392, Accuracy = 98.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:17<00:00,  2.35it/s, loss=0.0455, acc=99.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: Loss = 0.0317, Accuracy = 99.09%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:30<00:00,  2.01it/s, loss=0.0004, acc=99.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: Loss = 0.0164, Accuracy = 99.55%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:17<00:00,  2.34it/s, loss=0.0075, acc=99.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: Loss = 0.0196, Accuracy = 99.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:18<00:00,  2.33it/s, loss=0.0070, acc=99.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: Loss = 0.0158, Accuracy = 99.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:18<00:00,  2.30it/s, loss=0.0010, acc=99.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: Loss = 0.0129, Accuracy = 99.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:40<00:00,  1.81it/s, loss=0.0015, acc=99.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: Loss = 0.0089, Accuracy = 99.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:41<00:00,  1.79it/s, loss=0.0007, acc=99.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: Loss = 0.0108, Accuracy = 99.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:15<00:00,  2.41it/s, loss=0.0007, acc=99.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: Loss = 0.0130, Accuracy = 99.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:18<00:00,  2.32it/s, loss=0.0288, acc=99.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: Loss = 0.0285, Accuracy = 99.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:16<00:00,  2.37it/s, loss=0.0019, acc=99.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: Loss = 0.0317, Accuracy = 99.09%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:18<00:00,  2.32it/s, loss=0.6360, acc=99.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: Loss = 0.0267, Accuracy = 99.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:17<00:00,  2.34it/s, loss=0.0423, acc=99.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: Loss = 0.0235, Accuracy = 99.24%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:17<00:00,  2.34it/s, loss=0.0133, acc=99.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: Loss = 0.0176, Accuracy = 99.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [01:17<00:00,  2.36it/s, loss=0.0000, acc=99.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: Loss = 0.0110, Accuracy = 99.66%\n",
      "\n",
      "‚úÖ Training completed!\n",
      "\n",
      "üíæ Model saved to deepct_convknrm_vi.pth\n",
      "‚úÖ Training complete! Model ready for search.\n"
     ]
    }
   ],
   "source": [
    "# üèãÔ∏è TRAIN THE MODEL\n",
    "# ====================\n",
    "print(\"‚ö° Starting training process...\")\n",
    "print(\"  This will take several minutes depending on your hardware\\n\")\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_model(model, train_loader, epochs=20, lr=0.001, device=device)\n",
    "\n",
    "# Save trained model\n",
    "torch.save(trained_model.state_dict(), 'deepct_convknrm_vi.pth')\n",
    "print(\"\\nüíæ Model saved to deepct_convknrm_vi.pth\")\n",
    "print(\"‚úÖ Training complete! Model ready for search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be75c6",
   "metadata": {},
   "source": [
    "## 10. Search & Ranking Demo\n",
    "\n",
    "Test the trained model with real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298104eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating Fast Search Engine...\n",
      "üì¶ Pre-encoding all documents for super fast search...\n",
      "\n",
      "üìÑ Step 1: Encoding all documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1756/1756 [00:19<00:00, 92.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Encoded documents shape: torch.Size([1756, 200])\n",
      "‚úì Pre-encoding complete! Now search will be 10-15x faster!\n",
      "\n",
      "‚úÖ Fast search engine ready!\n",
      "\n",
      "üí° Example queries:\n",
      "  ‚Ä¢ Quang H·∫£i\n",
      "  ‚Ä¢ HLV Park Hang-seo\n",
      "  ‚Ä¢ ƒë·ªôi tuy·ªÉn Vi·ªát Nam\n",
      "  ‚Ä¢ V-League\n",
      "\n",
      "üöÄ Usage: results = fast_search('your query', top_k=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° INITIALIZE FAST SEARCH ENGINE WITH BATCH INFERENCE\n",
    "# ======================================================\n",
    "print(\"üîç Creating Fast Search Engine...\")\n",
    "print(\"üì¶ Pre-encoding all documents for super fast search...\\n\")\n",
    "\n",
    "# Pre-encode all documents once\n",
    "print(\"üìÑ Step 1: Encoding all documents...\")\n",
    "all_doc_tensors = []\n",
    "for doc in tqdm(documents, desc=\"Encoding\"):\n",
    "    title = doc.get('title', '')\n",
    "    content = doc.get('content', '')\n",
    "    full_text = f\"{title} {content}\"\n",
    "    doc_tokens = processor.preprocess(full_text)\n",
    "    doc_indices = vocab.encode(doc_tokens, max_len=200)\n",
    "    all_doc_tensors.append(torch.LongTensor(doc_indices))\n",
    "\n",
    "# Stack into batch tensor\n",
    "doc_batch_tensor = torch.stack(all_doc_tensors).to(device)\n",
    "print(f\"‚úì Encoded documents shape: {doc_batch_tensor.shape}\")\n",
    "print(f\"‚úì Pre-encoding complete! Now search will be 10-15x faster!\\n\")\n",
    "\n",
    "# Fast search function\n",
    "def fast_search(query_text, top_k=5, batch_size=256):\n",
    "    \"\"\"Fast batch search - takes ~5-10 seconds instead of 90s!\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    query_tokens = processor.preprocess(query_text)\n",
    "    query_indices = vocab.encode(query_tokens, max_len=20)\n",
    "    query_tensor = torch.LongTensor(query_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    print(f\"\\n‚ö° Fast ranking {len(documents)} docs in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_docs = len(documents)\n",
    "        num_batches = (num_docs + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in tqdm(range(num_batches), desc=\"Ranking\"):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, num_docs)\n",
    "            \n",
    "            # Get batch\n",
    "            doc_batch = doc_batch_tensor[start:end]\n",
    "            batch_len = doc_batch.size(0)\n",
    "            \n",
    "            # Expand query\n",
    "            query_batch = query_tensor.expand(batch_len, -1)\n",
    "            \n",
    "            # Score batch\n",
    "            scores, _ = trained_model(query_batch, doc_batch)\n",
    "            scores = torch.sigmoid(scores).squeeze(-1).cpu().numpy()\n",
    "            all_scores.extend(scores)\n",
    "    \n",
    "    # Create results\n",
    "    results = [(i, float(score), documents[i]) for i, score in enumerate(all_scores)]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results[:top_k]\n",
    "\n",
    "def display_fast_results(results):\n",
    "    \"\"\"Display search results nicely\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"üèÜ TOP {len(results)} RESULTS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    for rank, (idx, score, doc) in enumerate(results, 1):\n",
    "        title = doc.get('title', 'No title')\n",
    "        content = doc.get('content', '')\n",
    "        date = doc.get('date', 'No date')\n",
    "        author = doc.get('author', 'Unknown')\n",
    "        snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "        \n",
    "        print(f\"[{rank}] üìä SCORE: {score:.4f}\")\n",
    "        print(f\"üì∞ Title: {title}\")\n",
    "        print(f\"üìÖ Date: {date} | ‚úçÔ∏è Author: {author}\")\n",
    "        print(f\"üìù Content: {snippet}\")\n",
    "        print(f\"{'-'*100}\\n\")\n",
    "\n",
    "print(\"‚úÖ Fast search engine ready!\")\n",
    "print(\"\\nüí° Example queries:\")\n",
    "for q in [\"Quang H·∫£i\", \"HLV Park Hang-seo\", \"ƒë·ªôi tuy·ªÉn Vi·ªát Nam\", \"V-League\"]:\n",
    "    print(f\"  ‚Ä¢ {q}\")\n",
    "print(\"\\nüöÄ Usage: results = fast_search('your query', top_k=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681502d0",
   "metadata": {},
   "source": [
    "### üîç Interactive Fast Search Demo\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run cells in this order first:**\n",
    "1. Previous cell (Initialize fast search - pre-encode documents) - Takes ~30s\n",
    "2. This cell (Interactive search) - Takes ~5-10s per query\n",
    "\n",
    "**Why so fast?**\n",
    "- ‚úÖ Pre-encode ALL documents once (batch tensor)\n",
    "- ‚úÖ Batch inference (256 docs at a time)\n",
    "- ‚úÖ 10-15x faster: 91s ‚Üí 5-10s per search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0620fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç VIETNAMESE FOOTBALL SEARCH ENGINE (TRAINED MODEL)\n",
      "================================================================================\n",
      "\n",
      "üí¨ Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (v√≠ d·ª•: 'Quang H·∫£i', 'Park Hang-seo')\n",
      "ƒê·ªÉ tr·ªëng v√† Enter ƒë·ªÉ tho√°t\n",
      "\n",
      "‚ùå Kh√¥ng c√≥ query. H√£y nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm!\n"
     ]
    }
   ],
   "source": [
    "# üîç INTERACTIVE FAST SEARCH DEMO\n",
    "# =================================\n",
    "# Nh·∫≠p query v√† nh·∫≠n k·∫øt qu·∫£ ngay l·∫≠p t·ª©c!\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç VIETNAMESE FOOTBALL SEARCH ENGINE (TRAINED MODEL)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí¨ Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (v√≠ d·ª•: 'Quang H·∫£i', 'Park Hang-seo')\")\n",
    "print(\"ƒê·ªÉ tr·ªëng v√† Enter ƒë·ªÉ tho√°t\\n\")\n",
    "\n",
    "# Input query from user\n",
    "query = input(\"üîé Nh·∫≠p t√¨m ki·∫øm: \").strip()\n",
    "\n",
    "if query:\n",
    "    top_k = 5\n",
    "    \n",
    "    print(f\"\\n‚ö° ƒêang t√¨m ki·∫øm: '{query}'\")\n",
    "    print(f\"üìä Hi·ªÉn th·ªã top {top_k} k·∫øt qu·∫£\\n\")\n",
    "    \n",
    "    # Use fast search function\n",
    "    results = fast_search(query, top_k=top_k)\n",
    "    display_fast_results(results)\n",
    "    \n",
    "    print(\"\\n‚úÖ Ho√†n th√†nh! Ch·∫°y l·∫°i cell n√†y ƒë·ªÉ t√¨m ki·∫øm query kh√°c.\")\n",
    "    print(\"‚è±Ô∏è Search time: ~5-10 seconds (10x faster than before!)\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ query. H√£y nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
