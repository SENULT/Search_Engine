{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39651749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Trang 1 query 'bóng đá Việt Nam' không trả JSON, thử HTML...\n",
      "[1] 'Ảo tưởng bóng đá Việt Nam vươn tầm khi giành vé dự VCK U23 châu Á'\n",
      "[2] Liệu Malaysia có bị xử thua trận Việt Nam?\n",
      "[3] Báo Nhật Bản: 'Thêm một lần muối mặt trước bóng đá Việt Nam'\n",
      "[4] Đội tuyển Việt Nam thua đậm Nam Định\n",
      "[5] Tuyển Việt Nam tìm lại niềm vui chiến thắng\n",
      "[6] Unknown\n",
      "[7] HLV Quảng Nam: 'Vấn nạn trọng tài kéo bóng đá Việt Nam đi xuống'\n",
      "[8] HLV Kim Sang-sik đến với bóng đá Việt Nam thế nào\n",
      "[9] Thủ tướng: Bóng đá Việt Nam cần nỗ lực vô địch châu Á, dự World Cup\n",
      "[10] HLV Park Hang-seo: 'Đã tới lúc bóng đá Việt Nam vươn ra châu Á'\n",
      "Đã lưu 10 bài vào vnexpressT_bongda_part1.json\n",
      "[11] Agribank treo thưởng một tỷ đồng khi đội tuyển bóng đá Việt Nam vô địch\n",
      "[12] Agribank thưởng đội tuyển bóng đá Việt Nam 1 tỷ đồng\n",
      "[13] HLV Kim Sang-sik: 'Bóng đá Việt Nam đã trở lại'\n",
      "[14] Unknown\n",
      "[15] Unknown\n",
      "[16] Cầu thủ Trẻ TP HCM xin lỗi vì làm xấu hình ảnh bóng đá Việt Nam\n",
      "[17] Lối đá 'chuqua-chula-chuda' xói mòn tuyển bóng đá Việt Nam\n",
      "[18] Một thập kỷ bóng đá Việt Nam áp đảo Malaysia\n",
      "[19] Chuyên gia Hàn Quốc: 'Bóng đá Việt Nam đang có quá nhiều vấn đề'\n",
      "[20] Bóng đá Việt Nam như thế nào dưới thời Troussier\n",
      "Đã lưu 10 bài vào vnexpressT_bongda_part2.json\n",
      "[21] Số phận của 10 ngôi sao bóng đá Việt Nam từng xuất ngoại\n",
      "[22] 'Nhập tịch cầu thủ để đưa bóng đá Việt Nam trở lại đường đua'\n",
      "[23] Tuyển Việt Nam triệu tập đội hình lạ đấu Nam Định, CAHN\n",
      "[24] Filip Nguyễn: 'Hy vọng HLV Kim vực dậy tinh thần bóng đá Việt Nam'\n",
      "[!] Trang 2 query 'bóng đá Việt Nam' không trả JSON, thử HTML...\n",
      "[25] Tuyển Việt Nam đứt chuỗi tăng bậc FIFA\n",
      "[26] Dàn sao bóng đá Việt Nam dự đám cưới Quang Hải\n",
      "[27] HLV Hoàng Anh Tuấn không tin bóng đá Việt Nam hết thời\n",
      "[28] HLV Troussier: 'Nhiều người nghĩ tôi không phù hợp với bóng đá Việt Nam'\n",
      "[29] Unknown\n",
      "[30] Herbalife tổ chức giao lưu các đội tuyển bóng đá Quốc gia Việt Nam\n",
      "Đã lưu 10 bài vào vnexpressT_bongda_part3.json\n",
      "[31] Từ bỏ lối đá phòng ngự không giúp nâng tầm bóng đá Việt Nam\n",
      "[32] Ảo tưởng bóng đá Việt Nam ở 'mâm' trên Thái Lan\n",
      "[33] HLV Calisto mừng khi Việt Nam đã giành ba chức vô địch Đông Nam Á\n",
      "[34] Sai lầm 'chọn cầu thủ đá hay từ nhỏ' của bóng đá Việt\n",
      "[35] Unknown\n",
      "[36] Unknown\n",
      "[37] Steve Darby: 'Việt Nam cần nhìn xa hơn việc nhập tịch cầu thủ'\n",
      "[38] HLV Troussier: 'Bóng đá Việt Nam cùng đẳng cấp với Trung Quốc'\n",
      "[39] HLV Hoàng Anh Tuấn: 'ASIAD 19 là nơi kiểm chứng cho bóng đá Việt Nam'\n",
      "[40] Unknown\n",
      "Đã lưu 10 bài vào vnexpressT_bongda_part4.json\n",
      "[41] Công Phượng rời tuyển Việt Nam\n",
      "[42] Những nẻo đường nhập tịch cầu thủ ở Đông Nam Á\n",
      "[43] Việt Nam thua Malaysia 0-4 ở vòng loại Asian Cup\n",
      "[44] Thủ môn Nguyễn Đình Triệu trở lại tập luyện cùng tuyển Việt Nam\n",
      "[45] Chủ tịch VFF: 'Việt Nam không thể nhập tịch ồ ạt như Malaysia, Indonesia'\n",
      "[46] 'Bóng đá Việt Nam không thua đẳng cấp Thái Lan'\n",
      "[47] Unknown\n",
      "[48] Bốn năm của HLV Park với bóng đá Việt Nam\n",
      "[!] Trang 3 query 'bóng đá Việt Nam' không trả JSON, thử HTML...\n",
      "[49] Tuyển Malaysia giấu bài trước Việt Nam\n",
      "[50] Thanh Hóa đàm phán với cựu trợ lý tuyển Việt Nam\n",
      "Đã lưu 10 bài vào vnexpressT_bongda_part5.json\n",
      "Hoàn tất! Tổng số bài crawl: 50\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "SEARCH_URL = \"https://timkiem.vnexpress.net/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Các query muốn tìm\n",
    "QUERIES = [\"bóng đá Việt Nam\", \"V-League\", \"cầu thủ Việt Nam\", \"CLB Việt Nam\"]\n",
    "\n",
    "# Mục tiêu số lượng bài\n",
    "MAX_ARTICLES = 3000\n",
    "SAVE_PER_FILE = 500\n",
    "\n",
    "# Bộ nhớ tạm\n",
    "RESULTS = []\n",
    "CRAWLED_URLS = set()\n",
    "CRAWLED_COUNT = 0\n",
    "FILE_INDEX = 1\n",
    "\n",
    "def extract_author(soup):\n",
    "    # 1. Tác giả trong p.author\n",
    "    author_tag = soup.select_one(\"p.author\")\n",
    "    if author_tag:\n",
    "        return author_tag.get_text(strip=True)\n",
    "\n",
    "    # 2. Tác giả trong p.author_mail\n",
    "    author_tag = soup.select_one(\"p.author_mail\")\n",
    "    if author_tag:\n",
    "        return author_tag.get_text(strip=True)\n",
    "\n",
    "    # 3. Tác giả ở đoạn cuối bài\n",
    "    content_div = soup.select_one(\"article.fck_detail\")\n",
    "    if content_div:\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if paragraphs:\n",
    "            last_p = paragraphs[-1].get_text(strip=True)\n",
    "            if 2 <= len(last_p) <= 50: \n",
    "                return last_p\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "def crawl_article(url):\n",
    "    \"\"\"Crawl chi tiết 1 bài báo\"\"\"\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        title = soup.select_one(\"h1.title-detail\")\n",
    "        title = title.get_text(strip=True) if title else \"Unknown\"\n",
    "\n",
    "        date = soup.select_one(\"span.date\")\n",
    "        date = date.get_text(strip=True) if date else \"Unknown\"\n",
    "\n",
    "        author = extract_author(soup)\n",
    "\n",
    "        content_div = soup.select_one(\"article.fck_detail\")\n",
    "        paragraphs = content_div.find_all(\"p\") if content_div else []\n",
    "        content = \" \".join(p.get_text(\" \", strip=True) for p in paragraphs)\n",
    "\n",
    "        tags = [a.get_text(strip=True) for a in soup.select(\"ul.breadcrumb li a\")]\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"date\": date,\n",
    "            \"author\": author,\n",
    "            \"content\": content,\n",
    "            \"tags\": tags\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi crawl {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_results():\n",
    "    \"\"\"Lưu dữ liệu ra file JSON\"\"\"\n",
    "    global RESULTS, FILE_INDEX\n",
    "    if RESULTS:\n",
    "        filename = f\"vnexpressT_bongda_part{FILE_INDEX}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(RESULTS, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Đã lưu {len(RESULTS)} bài vào {filename}\")\n",
    "        RESULTS.clear()\n",
    "        FILE_INDEX += 1\n",
    "\n",
    "\n",
    "def crawl_search():\n",
    "    \"\"\"Crawl nhiều query cho đến khi đủ bài\"\"\"\n",
    "    global CRAWLED_COUNT\n",
    "    for query in QUERIES:\n",
    "        page = 1\n",
    "        while CRAWLED_COUNT < MAX_ARTICLES:\n",
    "            params = {\"q\": query, \"page\": page}\n",
    "            res = requests.get(SEARCH_URL, params=params, headers=HEADERS, timeout=10)\n",
    "\n",
    "            try:\n",
    "                data = res.json()\n",
    "                articles = data.get(\"data\", [])\n",
    "            except Exception:\n",
    "                print(f\"[!] Trang {page} query '{query}' không trả JSON, thử HTML...\")\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "                articles = [{\"share_url\": a[\"href\"]} for a in soup.select(\"h3.title-news a[href]\")]\n",
    "\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            for item in articles:\n",
    "                url = item.get(\"share_url\")\n",
    "                if not url or url in CRAWLED_URLS:\n",
    "                    continue\n",
    "\n",
    "                article = crawl_article(url)\n",
    "                if article:\n",
    "                    RESULTS.append(article)\n",
    "                    CRAWLED_URLS.add(url)\n",
    "                    CRAWLED_COUNT += 1\n",
    "                    print(f\"[{CRAWLED_COUNT}] {article['title']}\")\n",
    "\n",
    "                time.sleep(1)  # tránh bị chặn\n",
    "\n",
    "                if len(RESULTS) >= SAVE_PER_FILE:\n",
    "                    save_results()\n",
    "\n",
    "                if CRAWLED_COUNT >= MAX_ARTICLES:\n",
    "                    break\n",
    "\n",
    "            page += 1\n",
    "\n",
    "    # Lưu nốt phần còn lại\n",
    "    save_results()\n",
    "    print(f\"Hoàn tất! Tổng số bài crawl: {CRAWLED_COUNT}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    crawl_search()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
